"""
GeliÅŸmiÅŸ DokÃ¼man Analiz API Routes

YÃ¼klenen dosyalarda:
- Pivot tablo
- Ä°statistiksel analiz
- Trend analizi
- KarÅŸÄ±laÅŸtÄ±rma
- Rapor oluÅŸturma
- Yorum ve tavsiye
- DoÄŸal dil ile veri sorgulama
"""

import io
import time
import json as _json
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Form
from fastapi.responses import StreamingResponse, FileResponse
from sqlalchemy.ext.asyncio import AsyncSession
from pydantic import BaseModel, Field
from typing import Optional, List

from app.db.database import get_db
from app.db.models import User, Query
from app.api.routes.auth import get_current_user
from app.core.audit import log_action
from app.llm.client import ollama_client

import structlog

logger = structlog.get_logger()

# Analiz motoru
try:
    from app.core.document_analyzer import (
        parse_file_to_dataframe,
        discover_data,
        create_pivot,
        smart_pivot,
        statistical_analysis,
        trend_analysis,
        top_n_analysis,
        comparison_analysis,
        natural_language_query,
        format_analysis_for_llm,
        anomaly_detection,
        correlation_analysis,
        distribution_analysis,
        forecast_analysis,
        pareto_analysis,
        data_quality_analysis,
    )
    ANALYZER_AVAILABLE = True
except ImportError as e:
    ANALYZER_AVAILABLE = False
    logger.warning("document_analyzer_not_available", error=str(e))

# Insight Engine (v3.9.0)
try:
    from app.core.insight_engine import extract_insights, insights_to_dict
    INSIGHT_AVAILABLE = True
except ImportError:
    INSIGHT_AVAILABLE = False

# Dosya Ã§Ä±karÄ±cÄ± (documents.py'den)
try:
    from app.api.routes.documents import extract_text_from_file
    EXTRACTOR_AVAILABLE = True
except ImportError:
    EXTRACTOR_AVAILABLE = False

router = APIRouter()

# â”€â”€ Tip-spesifik sistem prompt'larÄ± (v3.9.7 â€” enhanced) â”€â”€
def _get_analysis_system_prompt(analysis_type: str) -> str:
    """Analiz tipine gÃ¶re optimize edilmiÅŸ sistem prompt'u dÃ¶ndÃ¼r"""
    base = (
        "Sen deneyimli bir veri analisti ve iÅŸ zekasÄ± uzmanÄ±sÄ±n. TÃ¼rkÃ§e yanÄ±t ver. "
        "SayÄ±sal deÄŸerleri daima belirt. Markdown tablolarÄ± aktif kullan. "
        "Her analiz bÃ¶lÃ¼mÃ¼nÃ¼ baÅŸlÄ±klarla (##, ###) yapÄ±landÄ±r. "
        "Ã–nemli sayÄ±larÄ± **kalÄ±n** yaz. BulgularÄ±nÄ± maddeler halinde sun. "
        "Veriye dayalÄ± somut Ã§Ä±karÄ±mlar yap, genel/belirsiz ifadelerden kaÃ§Ä±n."
    )

    type_prompts = {
        "full": f"""{base}
KapsamlÄ± bir tam analiz raporu Ã¼ret. Mutlaka ÅŸu bÃ¶lÃ¼mleri dahil et:
## ğŸ“‹ YÃ¶netici Ã–zeti (en kritik 3-4 bulgu, tek paragraf)
## ğŸ“Š Temel Metrikler (markdown tablo: Metrik | DeÄŸer | Yorum)
## ğŸ“ˆ DetaylÄ± Bulgular (her Ã¶nemli sÃ¼tun/metrik iÃ§in derinlemesine)
## ğŸ” KarÅŸÄ±laÅŸtÄ±rma ve Trendler
## âš ï¸ Dikkat Edilmesi Gerekenler (anomali, risk, eksik)
## âœ… Aksiyon Ã–nerileri (Ã¶ncelik sÄ±rasÄ±yla, somut adÄ±mlar)
Her bÃ¶lÃ¼mde markdown tablolarÄ± kullan. HiÃ§bir bÃ¶lÃ¼mÃ¼ atlama.""",

        "pivot": f"""{base}
Pivot tablo ve Ã§apraz analiz uzmanÄ±sÄ±n. YanÄ±tÄ±nÄ± ÅŸu yapÄ±da sun:
## ğŸ“Š Pivot Tablo Ã–zeti
- Hangi kategoriler hangi deÄŸerlere gÃ¶re Ã§aprazlanmÄ±ÅŸ
## ğŸ“‹ DetaylÄ± Pivot Tablo (markdown tablo formatÄ±nda)
## ğŸ” Ã–ne Ã‡Ä±kan Bulgular
- En yÃ¼ksek/dÃ¼ÅŸÃ¼k hÃ¼creler, oranlar, paylar
## ğŸ’¡ Stratejik Ã‡Ä±karÄ±mlar
TÃ¼m sayÄ±larÄ± yÃ¼zde ve oran olarak da ifade et.""",

        "trend": f"""{base}
Zaman serisi ve trend analizi uzmanÄ±sÄ±n. YanÄ±tÄ±nÄ± ÅŸu yapÄ±da sun:
## ğŸ“ˆ Trend Ã–zeti (genel yÃ¶n, bÃ¼yÃ¼me hÄ±zÄ±)
## ğŸ“Š DÃ¶nemsel Performans Tablosu (markdown tablo: DÃ¶nem | DeÄŸer | DeÄŸiÅŸim% | Yorum)
## ğŸ”„ Hareketli Ortalamalar ve Momentum
## ğŸ“‰ Volatilite ve Risk Profili
## ğŸ”® Gelecek DÃ¶nem Beklentileri
## âœ… Stratejik Ã–neriler
TÃ¼m trendi sayÄ±larla destekle, grafik verisi oluÅŸtur.""",

        "compare": f"""{base}
KarÅŸÄ±laÅŸtÄ±rmalÄ± analiz uzmanÄ±sÄ±n. YanÄ±tÄ±nÄ± ÅŸu yapÄ±da sun:
## ğŸ“Š KarÅŸÄ±laÅŸtÄ±rma Ã–zet Tablosu (Grup | Metrik1 | Metrik2 | ... | Genel SÄ±ralama)
## ğŸ† En Ä°yi Performans GÃ¶sterenler (neden iyi, hangi metriklerde)
## âš ï¸ En DÃ¼ÅŸÃ¼k Performans GÃ¶sterenler (neden kÃ¶tÃ¼, nerede gerileme)
## ğŸ“ˆ Ä°statistiksel AnlamlÄ±lÄ±k (p-value, etki bÃ¼yÃ¼klÃ¼ÄŸÃ¼ yorumu)
## ğŸ” Grup Ä°Ã§i TutarlÄ±lÄ±k (std sapma, CV analizi)
## âœ… Grup BazlÄ± Aksiyon Ã–nerileri
Her grubu ayrÄ± deÄŸerlendir, sÄ±ralama tablosu oluÅŸtur.""",

        "summary": f"""{base}
Veriden etkili bir yÃ¶netici Ã¶zeti Ã§Ä±kar:
## ğŸ“‹ Veri KapsamÄ± (ne, ne zaman, ne kadar â€” tek paragraf)
## ğŸ¯ Kritik Bulgular (en Ã¶nemli 3-5 sayÄ±sal bulgu, madde halinde)
## âš¡ Dikkat Ã‡ekici Noktalar (anomali, trend kÄ±rÄ±lmasÄ±, fÄ±rsat)
## ğŸ“Œ SonuÃ§ ve Ã–neri (tek paragraf, net ve aksiyona yÃ¶nelik)
KÄ±sa, Ã¶z ama bilgi dolu olsun. Maksimum 15 cÃ¼mle.""",

        "recommend": f"""{base}
Stratejik danÄ±ÅŸman gibi dÃ¼ÅŸÃ¼n. YapÄ±landÄ±rÄ±lmÄ±ÅŸ tavsiye raporu sun:
## ğŸš¨ Acil Aksiyonlar (0-1 ay) â€” En az 2 madde
## ğŸ“‹ KÄ±sa Vadeli Ä°yileÅŸtirmeler (1-3 ay) â€” En az 3 madde
## ğŸ¯ Uzun Vadeli Stratejiler (3-12 ay) â€” En az 2 madde
## ğŸ“Š Ã–ncelik Matrisi (markdown tablo: Aksiyon | Ã–ncelik | Beklenen Etki | Maliyet/Zorluk)
Her tavsiyeyi verilerle destekle. ROI/etki tahmini yap.""",

        "report": f"""{base}
Profesyonel yÃ¶netici raporu yaz. Resmi ve yapÄ±landÄ±rÄ±lmÄ±ÅŸ format:
## ğŸ“‹ YÃ¶netici Ã–zeti
## ğŸ“Š KPI Tablosu (markdown tablo: KPI | Mevcut DeÄŸer | Hedef | Durum)
## ğŸ“ˆ DetaylÄ± Analiz BulgularÄ±
### BÃ¶lÃ¼m 1: [Konuya gÃ¶re]
### BÃ¶lÃ¼m 2: [Konuya gÃ¶re]
## ğŸ” KarÅŸÄ±laÅŸtÄ±rmalÄ± DeÄŸerlendirme
## âš ï¸ Risk ve UyarÄ±lar
## âœ… Aksiyon PlanÄ± (markdown tablo: AdÄ±m | Sorumlu | SÃ¼re | Ã–ncelik)
## ğŸ“Œ SonuÃ§
Resmi dil kullan. TÃ¼m bÃ¶lÃ¼mlerde sayÄ±sal veri olsun.""",

        "anomaly": f"""{base}
Anomali tespiti uzmanÄ±sÄ±n. DetaylÄ± anomali raporu sun:
## ğŸ”´ Anomali Ã–zeti (toplam, ciddiyet daÄŸÄ±lÄ±mÄ±)
## ğŸ“Š Anomali Tablosu (SÃ¼tun | Anomali SayÄ±sÄ± | Ciddiyet | En UÃ§ DeÄŸer | Normal AralÄ±k)
## ğŸ” YÃ¶ntem BazlÄ± SonuÃ§lar (IQR, Z-Score, Modified Z-Score karÅŸÄ±laÅŸtÄ±rmasÄ±)
## âš ï¸ Kritik Anomaliler (her biri ayrÄ± aÃ§Ä±klamayla)
## ğŸ¤” OlasÄ± Nedenler (veri hatasÄ± mÄ±, gerÃ§ek sapma mÄ±?)
## âœ… Temizleme Stratejisi (sil/dÃ¼zelt/araÅŸtÄ±r Ã¶nerileri)
Her anomaliyi iÅŸ sÃ¼reÃ§leri perspektifinden yorumla.""",

        "correlation": f"""{base}
Korelasyon analizi uzmanÄ±sÄ±n. Ä°liÅŸkileri raporla:
## ğŸ“Š Korelasyon Matrisi (markdown tablo formatÄ±nda)
## ğŸ”´ GÃ¼Ã§lÃ¼ Ä°liÅŸkiler (|r| > 0.7, tablo: DeÄŸiÅŸken1 | DeÄŸiÅŸken2 | Pearson | Spearman | YÃ¶n)
## ğŸŸ¡ Orta Ä°liÅŸkiler (0.4 < |r| < 0.7)
## ğŸ” Ä°statistiksel AnlamlÄ±lÄ±k (p-value yorumu)
## ğŸ’¡ Nedensellik TartÄ±ÅŸmasÄ± (korelasyon â‰  nedensellik uyarÄ±sÄ± ile)
## âœ… Stratejik Ã‡Ä±karÄ±mlar (hangi deÄŸiÅŸkeni deÄŸiÅŸtirirsek ne olur?)
Pearson ve Spearman farklarÄ±nÄ± yorumla.""",

        "distribution": f"""{base}
DaÄŸÄ±lÄ±m analizi uzmanÄ±sÄ±n. Ä°statistiksel daÄŸÄ±lÄ±mlarÄ± raporla:
## ğŸ“Š DaÄŸÄ±lÄ±m Ã–zet Tablosu (SÃ¼tun | Ort | Medyan | Std | CV% | DaÄŸÄ±lÄ±m Tipi)
## ğŸ“ˆ Normal DaÄŸÄ±lÄ±m Testi SonuÃ§larÄ± (SÃ¼tun | Test | p-value | Normal mi?)
## ğŸ” Ã‡arpÄ±klÄ±k ve BasÄ±klÄ±k Yorumu (her sÃ¼tun iÃ§in)
## ğŸ“Š YÃ¼zdelik Dilimler (P25, P50, P75, P95, P99 tablosu)
## âš ï¸ Dikkat Ã‡ekici DaÄŸÄ±lÄ±mlar (Ã§arpÄ±k, bimodal, uÃ§ deÄŸerli)
## âœ… Analiz Stratejisi Ã–nerileri (parametrik mi non-parametrik test mi?)
Ä°statistiksel terimleri iÅŸ diline Ã§evir.""",

        "forecast": f"""{base}
Tahminleme uzmanÄ±sÄ±n. Ã‡ok modelli projeksiyon raporu sun:
## ğŸ“ˆ Tahmin Ã–zeti (en iyi model, beklenen deÄŸiÅŸim)
## ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rma Tablosu (Model | MAPE% | Tahmini DeÄŸer | Trend | GÃ¼ven)
## ğŸ”® En Ä°yi Model DetaylarÄ± (parametre, gÃ¼ven aralÄ±ÄŸÄ±)
## ğŸ“Š Senaryo Analizi (Ä°yimser | Baz | KÃ¶tÃ¼mser senaryolar tablosu)
## âš ï¸ Model KÄ±sÄ±tlamalarÄ± ve VarsayÄ±mlar
## âœ… Tahminlere DayalÄ± Aksiyon Ã–nerileri
GÃ¼ven aralÄ±klarÄ±nÄ± ve belirsizlikleri mutlaka belirt.""",

        "pareto": f"""{base}
Pareto ve ABC analizi uzmanÄ±sÄ±n. Raporunu ÅŸu yapÄ±da sun:
## ğŸ“Š Pareto KuralÄ± Sonucu (80/20 geÃ§erli mi? tablo ile gÃ¶ster)
## ğŸ“‹ ABC SÄ±nÄ±flandÄ±rma Tablosu (SÄ±nÄ±f | Ã–ÄŸe SayÄ±sÄ± | Toplam DeÄŸer | Pay% | Ã–ÄŸeler)
## ğŸ† A SÄ±nÄ±fÄ± Detay Analizi (her Ã¶ÄŸe ayrÄ±, neden deÄŸerli?)
## ğŸ” B SÄ±nÄ±fÄ± FÄ±rsat Analizi (A'ya Ã§Ä±kma potansiyeli)
## âš ï¸ C SÄ±nÄ±fÄ± DeÄŸerlendirme (optimize et veya kes)
## âœ… Kaynak DaÄŸÄ±lÄ±mÄ± Ã–nerileri (bÃ¼tÃ§e, zaman, personel yÃ¼zdeleri)
Her Ã¶neriyi katkÄ± yÃ¼zdeleriyle destekle.""",

        "quality": f"""{base}
Veri kalitesi denetÃ§isisin. Profesyonel denetim raporu sun:
## ğŸ“Š Kalite Skor KartÄ± (Boyut | Skor | Not | AÃ§Ä±klama tablosu)
## ğŸ” BÃ¼tÃ¼nlÃ¼k Analizi (eksik veri haritasÄ±, sÃ¼tun bazlÄ± tablo)
## ğŸ”„ Teksillik KontrolÃ¼ (tekrar satÄ±r analizi)
## âš¡ TutarlÄ±lÄ±k Denetimi (tip uyumsuzluklarÄ±, format sorunlarÄ±)
## âœ… GeÃ§erlilik Testi (aralÄ±k ihlalleri, mantÄ±ksal kontroller)
## ğŸ“‹ Temizlik PlanÄ± (AdÄ±m | SÃ¼tun | Ä°ÅŸlem | Ã–ncelik tablosu)
## ğŸ“Œ SonuÃ§ (veri gÃ¼venilirlik deÄŸerlendirmesi)
Her sorunu somut Ã¶rneklerle gÃ¶ster.""",

        # â”€â”€ CEO-TIER ANALÄ°Z TÄ°PLERÄ° (v3.8.0 â†’ v3.9.7 enhanced) â”€â”€
        "profitability": f"""{base}
CEO'ya hitap eden karlÄ±lÄ±k raporu sun:
## ğŸ’° KarlÄ±lÄ±k Ã–zeti (toplam gelir, maliyet, net kÃ¢r marjÄ±)
## ğŸ“Š Segment BazlÄ± KarlÄ±lÄ±k Tablosu (Segment | Gelir | Maliyet | Net KÃ¢r | Marj% | SÄ±ralama)
## ğŸ”´ Zarar Eden Segmentler (gizli maliyet analizi)
## ğŸŸ¢ En KÃ¢rlÄ± Segmentler (bÃ¼yÃ¼tme fÄ±rsatlarÄ±)
## ğŸ’¡ Fiyatlama Analizi ve FÄ±rsatlar
## ğŸ“Š Contribution Margin Tablosu
## âœ… Patron'un Aksiyon Listesi (direkt TL/â‚º etkisiyle)
Her bulguyu para birimi cinsinden ifade et.""",

        "bottleneck": f"""{base}
CEO'ya hitap eden darboÄŸaz raporu sun:
## ğŸ”´ Ana DarboÄŸaz Tespiti (nerede, neden, ne kadar etkili?)
## ğŸ“Š SÃ¼reÃ§ Performans Tablosu (AdÄ±m | SÃ¼re | Kapasite% | Hata% | Maliyet | Skor)
## â›“ï¸ Zincirleme Etki Analizi (darboÄŸazÄ±n domino etkisi)
## ğŸ“ˆ Kapasite ve Verimlilik HaritasÄ±
## ğŸ’¡ Ä°yileÅŸtirme Ã–nerileri (ROI ile: Aksiyon | Maliyet | Tasarruf | SÃ¼re)
## âœ… Ã–ncelikli Aksiyon PlanÄ±
Her darboÄŸazÄ±n finansal etkisini hesapla.""",

        "executive": f"""{base}
CEO/CFO'ya hitap eden Åirket SaÄŸlÄ±k Dashboard'u oluÅŸtur:
## ğŸ¥ Genel SaÄŸlÄ±k Skoru (0-100 puan, harf notu, durum)
## ğŸ“Š 4 Boyut Tablosu (Boyut | Skor | Not | Trend | Renk)
- ğŸ’° Finansal SaÄŸlamlÄ±k
- âš™ï¸ Operasyonel Verimlilik
- ğŸ“ˆ BÃ¼yÃ¼me Ä°vmesi
- ğŸ›¡ï¸ Risk Maruziyet
## ğŸ† En GÃ¼Ã§lÃ¼ 3 GÃ¶sterge
## âš ï¸ En ZayÄ±f 3 GÃ¶sterge (acil mÃ¼dahale gereken)
## âœ… Stratejik Ã–ncelikler (harf notuyla sÄ±ralÄ±)
Tek bakÄ±ÅŸta anlaÅŸÄ±lÄ±r dashboard formatÄ± kullan.""",

        "benchmark": f"""{base}
CEO'ya hitap eden sektÃ¶rel kÄ±yaslama raporu sun:
## ğŸ“Š KÄ±yaslama Tablosu (KPI | Åirket | SektÃ¶r Ort. | En Ä°yi | Konum | Gap)
## ğŸ† ÃœstÃ¼n OlduÄŸumuz Alanlar (neden iyi, nasÄ±l sÃ¼rdÃ¼rÃ¼lÃ¼r?)
## âš ï¸ Geride KaldÄ±ÄŸÄ±mÄ±z Alanlar (gap analizi, kapatma sÃ¼resi)
## ğŸ“ˆ Rekabet Pozisyonu DeÄŸerlendirmesi
## ğŸ¯ Hedef Belirleme (KPI | Mevcut | 3 Ay Hedef | 12 Ay Hedef)
## âœ… Gap Kapatma Aksiyon PlanÄ±
Her KPI'Ä± sektÃ¶r benchmark'Ä± ile karÅŸÄ±laÅŸtÄ±r.""",
    }

    return type_prompts.get(analysis_type, type_prompts["full"])

# â”€â”€ Aktif analiz dosyalarÄ± cache (kullanÄ±cÄ± bazlÄ±) â”€â”€
_analysis_cache: dict[int, dict] = {}
MAX_CACHE_PER_USER = 3


def _cache_analysis(user_id: int, filename: str, data: dict):
    """Analiz verisini cache'e al"""
    if user_id not in _analysis_cache:
        _analysis_cache[user_id] = {}
    
    # Eski verileri temizle
    if len(_analysis_cache[user_id]) >= MAX_CACHE_PER_USER:
        oldest = next(iter(_analysis_cache[user_id]))
        del _analysis_cache[user_id][oldest]
    
    _analysis_cache[user_id][filename] = data


def _get_cached(user_id: int, filename: str = None) -> Optional[dict]:
    """Cache'ten analiz verisini al"""
    if user_id not in _analysis_cache:
        return None
    if filename:
        return _analysis_cache[user_id].get(filename)
    # Son yÃ¼klenen dosyayÄ± dÃ¶ndÃ¼r
    if _analysis_cache[user_id]:
        last_key = list(_analysis_cache[user_id].keys())[-1]
        return _analysis_cache[user_id][last_key]
    return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REQUEST / RESPONSE MODELLERÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AnalyzeRequest(BaseModel):
    """Analiz talebi (dosya zaten yÃ¼klenmiÅŸse)"""
    analysis_type: str = Field("full", description="full, pivot, trend, compare, summary, recommend, report")
    question: Optional[str] = Field(None, description="Ek soru veya talep")
    filename: Optional[str] = Field(None, description="Cache'teki dosya adÄ± (None=son yÃ¼klenen)")

class PivotRequest(BaseModel):
    """Pivot tablo talebi"""
    rows: Optional[List[str]] = None
    columns: Optional[List[str]] = None
    values: Optional[List[str]] = None
    aggfunc: str = "sum"
    filename: Optional[str] = None

class QueryRequest(BaseModel):
    """DoÄŸal dil sorgusu"""
    question: str
    filename: Optional[str] = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENDPOINT'LER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.post("/upload-analyze")
async def upload_and_analyze(
    file: UploadFile = File(...),
    analysis_type: str = Form("full"),
    question: Optional[str] = Form(None),
    department: str = Form("Genel"),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    Dosya yÃ¼kle + otomatik kapsamlÄ± analiz.
    
    Desteklenen analiz tipleri:
    - **full**: Tam analiz (pivot + trend + istatistik + tavsiye)
    - **pivot**: Pivot tablo odaklÄ±
    - **trend**: Zaman bazlÄ± trend analizi
    - **compare**: Kategori karÅŸÄ±laÅŸtÄ±rmasÄ±
    - **summary**: HÄ±zlÄ± Ã¶zet
    - **recommend**: Tavsiye odaklÄ±
    - **report**: Profesyonel rapor formatÄ±
    """
    if not ANALYZER_AVAILABLE:
        raise HTTPException(status_code=503, detail="Analiz modÃ¼lÃ¼ kullanÄ±lamÄ±yor")
    
    start_time = time.time()
    
    try:
        file_content = await file.read()
        if not file_content:
            raise HTTPException(status_code=400, detail="Dosya boÅŸ")
        
        filename = file.filename or "unknown"
        logger.info("analyze_upload", file=filename, type=analysis_type, user=current_user.email)
        
        # 1. DataFrame'e Ã§evirmeyi dene
        df = parse_file_to_dataframe(filename, file_content)
        
        # 2. DataFrame baÅŸarÄ±lÄ±ysa tablolu analiz
        if df is not None and not df.empty:
            # Cache'e al
            discovery = discover_data(df)
            _cache_analysis(current_user.id, filename, {
                "df": df,
                "discovery": discovery,
                "filename": filename,
                "uploaded_at": time.time(),
            })
            
            # LLM iÃ§in analiz prompt'u oluÅŸtur
            analysis_prompt = format_analysis_for_llm(
                df=df,
                analysis_type=analysis_type,
                question=question,
                filename=filename,
            )
            
            # LLM'den analiz al
            system_prompt = _get_analysis_system_prompt(analysis_type)

            llm_answer = ""
            if await ollama_client.is_available():
                llm_answer = await ollama_client.generate(
                    prompt=analysis_prompt,
                    system_prompt=system_prompt,
                    temperature=0.3,
                )
            else:
                llm_answer = f"[LLM eriÅŸilemez - Ham analiz verisi]\n\n{analysis_prompt}"
            
            processing_ms = int((time.time() - start_time) * 1000)
            
            # DB kaydet
            try:
                query = Query(
                    user_id=current_user.id,
                    question=f"[Dosya Analizi: {filename}] {question or analysis_type}",
                    answer=llm_answer[:5000],
                    department=department,
                    mode="Analiz",
                    risk_level="DÃ¼ÅŸÃ¼k",
                    confidence=0.9,
                    processing_time_ms=processing_ms,
                )
                db.add(query)
                await db.commit()
            except Exception:
                pass
            
            return {
                "success": True,
                "answer": llm_answer,
                "analysis_type": analysis_type,
                "filename": filename,
                "data_info": {
                    "rows": discovery["row_count"],
                    "cols": discovery["col_count"],
                    "numeric_columns": discovery["numeric_columns"],
                    "categorical_columns": discovery["categorical_columns"],
                    "date_columns": discovery["date_columns"],
                    "has_missing": discovery["has_missing"],
                },
                "processing_time_ms": processing_ms,
                "cached": True,
            }
        
        # 3. DataFrame deÄŸilse, metin olarak analiz et
        if EXTRACTOR_AVAILABLE:
            text_content, doc_type = extract_text_from_file(filename, file_content)
        else:
            try:
                text_content = file_content.decode('utf-8')
                doc_type = "text"
            except Exception:
                raise HTTPException(status_code=400, detail="Dosya okunamadÄ±")
        
        if not text_content or not text_content.strip():
            raise HTTPException(status_code=400, detail="Dosyadan iÃ§erik Ã§Ä±karÄ±lamadÄ±")
        
        # Metin analiz prompt'u
        analysis_prompt = format_analysis_for_llm(
            text=text_content,
            analysis_type=analysis_type,
            question=question,
            filename=filename,
        )
        
        system_prompt = """Sen bir dokÃ¼man analisti ve iÅŸ zekasÄ± uzmanÄ±sÄ±n. TÃ¼rkÃ§e yanÄ±t ver.
Verilen dokÃ¼manÄ± detaylÄ± analiz et. Net bulgular, yorumlar ve Ã¶neriler sun.
Profesyonel ama anlaÅŸÄ±lÄ±r bir dil kullan. Tavsiyelerini somut yap."""
        
        llm_answer = ""
        if await ollama_client.is_available():
            llm_answer = await ollama_client.generate(
                prompt=analysis_prompt,
                system_prompt=system_prompt,
                temperature=0.3,
            )
        else:
            llm_answer = f"[LLM eriÅŸilemez]\n\n{analysis_prompt}"
        
        processing_ms = int((time.time() - start_time) * 1000)
        
        return {
            "success": True,
            "answer": llm_answer,
            "analysis_type": analysis_type,
            "filename": filename,
            "data_info": {
                "type": doc_type,
                "chars": len(text_content),
                "words": len(text_content.split()),
            },
            "processing_time_ms": processing_ms,
            "cached": False,
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error("analyze_error", error=str(e))
        raise HTTPException(status_code=500, detail=f"Analiz hatasÄ±: {str(e)}")


@router.post("/upload-analyze/stream")
async def upload_and_analyze_stream(
    file: UploadFile = File(...),
    analysis_type: str = Form("full"),
    question: Optional[str] = Form(None),
    department: str = Form("Genel"),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """Dosya yÃ¼kle + streaming analiz (SSE)"""
    if not ANALYZER_AVAILABLE:
        raise HTTPException(status_code=503, detail="Analiz modÃ¼lÃ¼ kullanÄ±lamÄ±yor")
    
    start_time = time.time()
    
    file_content = await file.read()
    if not file_content:
        raise HTTPException(status_code=400, detail="Dosya boÅŸ")
    
    filename = file.filename or "unknown"
    
    # 1. DataFrame dene
    df = parse_file_to_dataframe(filename, file_content)
    
    if df is not None and not df.empty:
        discovery = discover_data(df)
        _cache_analysis(current_user.id, filename, {
            "df": df, "discovery": discovery, "filename": filename, "uploaded_at": time.time(),
        })
        
        analysis_prompt = format_analysis_for_llm(
            df=df, analysis_type=analysis_type, question=question, filename=filename,
        )
        data_info = {
            "rows": discovery["row_count"],
            "cols": discovery["col_count"],
            "numeric_columns": discovery["numeric_columns"],
        }
    else:
        # Metin analizi
        if EXTRACTOR_AVAILABLE:
            text_content, doc_type = extract_text_from_file(filename, file_content)
        else:
            text_content = file_content.decode('utf-8', errors='ignore')
            doc_type = "text"
        
        analysis_prompt = format_analysis_for_llm(
            text=text_content, analysis_type=analysis_type, question=question, filename=filename,
        )
        data_info = {"type": doc_type, "chars": len(text_content)}
    
    system_prompt = _get_analysis_system_prompt(analysis_type)

    async def _event_generator():
        collected = []
        try:
            # Ã–nce data_info gÃ¶nder
            yield f"data: {_json.dumps({'data_info': data_info, 'filename': filename})}\n\n"
            
            async for token in ollama_client.stream(analysis_prompt, system_prompt=system_prompt):
                collected.append(token)
                yield f"data: {_json.dumps({'token': token})}\n\n"
        except Exception as exc:
            yield f"data: {_json.dumps({'error': str(exc)})}\n\n"
            return
        
        processing_ms = int((time.time() - start_time) * 1000)
        full_answer = "".join(collected)
        
        try:
            query = Query(
                user_id=current_user.id,
                question=f"[Dosya Analizi: {filename}] {question or analysis_type}",
                answer=full_answer[:5000],
                department=department,
                mode="Analiz",
                risk_level="DÃ¼ÅŸÃ¼k",
                confidence=0.9,
                processing_time_ms=processing_ms,
            )
            db.add(query)
            await db.commit()
        except Exception:
            pass
        
        yield f"data: {_json.dumps({'done': True, 'processing_time_ms': processing_ms, 'analysis_type': analysis_type})}\n\n"
    
    return StreamingResponse(
        _event_generator(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive", "X-Accel-Buffering": "no"},
    )


@router.post("/discover")
async def discover_uploaded_file(
    file: UploadFile = File(...),
    current_user: User = Depends(get_current_user),
):
    """
    DosyayÄ± yÃ¼kle, yapÄ±sÄ±nÄ± keÅŸfet ve sÃ¼tun bilgilerini dÃ¶ndÃ¼r.
    KullanÄ±cÄ± bu bilgiyle hangi analizleri yapacaÄŸÄ±nÄ± seÃ§er.
    """
    if not ANALYZER_AVAILABLE:
        raise HTTPException(status_code=503, detail="Analiz modÃ¼lÃ¼ kullanÄ±lamÄ±yor")
    
    file_content = await file.read()
    df = parse_file_to_dataframe(file.filename, file_content)
    
    if df is None or df.empty:
        # Metin bazlÄ± â€” basit bilgi dÃ¶ndÃ¼r
        try:
            text = file_content.decode('utf-8', errors='ignore')
        except Exception:
            text = ""
        
        return {
            "type": "text",
            "filename": file.filename,
            "chars": len(text),
            "words": len(text.split()),
            "lines": len(text.split('\n')),
            "is_tabular": False,
            "available_analyses": ["full", "summary", "recommend", "report"],
        }
    
    discovery = discover_data(df)
    _cache_analysis(current_user.id, file.filename, {
        "df": df, "discovery": discovery, "filename": file.filename, "uploaded_at": time.time(),
    })
    
    # Hangi analizler yapÄ±labilir?
    available = ["full", "summary", "recommend", "report", "quality"]
    if discovery["categorical_columns"] and discovery["numeric_columns"]:
        available.append("pivot")
        available.append("compare")
        available.append("pareto")
    if discovery["date_columns"]:
        available.append("trend")
        available.append("forecast")
    if len(discovery["numeric_columns"]) >= 2:
        available.append("correlation")
        available.append("distribution")
        available.append("anomaly")
    
    # Sayfalar (Excel)
    sheets = None
    if hasattr(df, 'attrs') and '_all_sheets' in df.attrs:
        sheets = df.attrs['_all_sheets']
    
    return {
        "type": "tabular",
        "filename": file.filename,
        "is_tabular": True,
        "rows": discovery["row_count"],
        "cols": discovery["col_count"],
        "columns": discovery["columns"],
        "numeric_columns": discovery["numeric_columns"],
        "categorical_columns": discovery["categorical_columns"],
        "date_columns": discovery["date_columns"],
        "has_missing": discovery["has_missing"],
        "missing_summary": discovery["missing_summary"],
        "sheets": sheets,
        "available_analyses": available,
        "sample_data": df.head(5).to_dict('records'),
    }


@router.post("/pivot")
async def create_pivot_table(
    request: PivotRequest,
    current_user: User = Depends(get_current_user),
):
    """Cache'teki veriden pivot tablo oluÅŸtur"""
    if not ANALYZER_AVAILABLE:
        raise HTTPException(status_code=503, detail="Analiz modÃ¼lÃ¼ kullanÄ±lamÄ±yor")
    
    cached = _get_cached(current_user.id, request.filename)
    if not cached or "df" not in cached:
        raise HTTPException(status_code=404, detail="Ã–nce bir dosya yÃ¼kleyin (/analyze/upload-analyze veya /analyze/discover)")
    
    df = cached["df"]
    
    if request.rows or request.values:
        result = create_pivot(
            df,
            rows=request.rows,
            columns=request.columns,
            values=request.values,
            aggfunc=request.aggfunc,
        )
    else:
        result = smart_pivot(df)
    
    if not result.get("success"):
        raise HTTPException(status_code=400, detail=result.get("error", "Pivot oluÅŸturulamadÄ±"))
    
    return {
        "success": True,
        "pivot_table": result["table_str"],
        "pivot_markdown": result.get("table_markdown"),
        "shape": result["shape"],
        "filename": cached["filename"],
    }


@router.post("/query")
async def query_data(
    request: QueryRequest,
    current_user: User = Depends(get_current_user),
):
    """DoÄŸal dil ile veri sorgula"""
    if not ANALYZER_AVAILABLE:
        raise HTTPException(status_code=503, detail="Analiz modÃ¼lÃ¼ kullanÄ±lamÄ±yor")
    
    cached = _get_cached(current_user.id, request.filename)
    if not cached or "df" not in cached:
        raise HTTPException(status_code=404, detail="Ã–nce bir dosya yÃ¼kleyin")
    
    df = cached["df"]
    result = natural_language_query(df, request.question)
    
    if result.get("success"):
        return {
            "success": True,
            "answer": result["answer"],
            "value": result.get("value"),
            "query_type": result["query_type"],
            "filename": cached["filename"],
        }
    
    # DoÄŸal dil sorgusu bulunamadÄ±ysa, LLM'e sor
    analysis_prompt = format_analysis_for_llm(
        df=df,
        analysis_type="full",
        question=request.question,
        filename=cached["filename"],
    )
    
    system_prompt = """Sen bir veri analistisin. Verilen soruyu verilere dayanarak yanÄ±tla. 
KÄ±sa ve net cevap ver. SayÄ±sal deÄŸerleri mutlaka belirt. TÃ¼rkÃ§e yanÄ±t ver."""
    
    if await ollama_client.is_available():
        answer = await ollama_client.generate(
            prompt=analysis_prompt,
            system_prompt=system_prompt,
            temperature=0.2,
        )
    else:
        answer = "LLM eriÅŸilemez, doÄŸrudan veri sorgusu denendi ama eÅŸleÅŸme bulunamadÄ±."
    
    return {
        "success": True,
        "answer": answer,
        "query_type": "llm_analysis",
        "filename": cached["filename"],
    }


@router.post("/statistics")
async def get_statistics(
    current_user: User = Depends(get_current_user),
    filename: Optional[str] = None,
):
    """Cache'teki verinin detaylÄ± istatistikleri"""
    if not ANALYZER_AVAILABLE:
        raise HTTPException(status_code=503, detail="Analiz modÃ¼lÃ¼ kullanÄ±lamÄ±yor")
    
    cached = _get_cached(current_user.id, filename)
    if not cached or "df" not in cached:
        raise HTTPException(status_code=404, detail="Ã–nce bir dosya yÃ¼kleyin")
    
    df = cached["df"]
    stats = statistical_analysis(df)
    
    # v3.9.0 â€” Otomatik insight ekleme
    auto_insights = None
    if INSIGHT_AVAILABLE:
        try:
            report = extract_insights(df, max_insights=10)
            auto_insights = insights_to_dict(report)
        except Exception as ie:
            logger.warning("insight_extraction_failed", error=str(ie))
    
    return {
        "success": True,
        "filename": cached["filename"],
        "basic_stats": stats["basic_stats"],
        "correlations": stats.get("correlations"),
        "strong_correlations": stats.get("strong_correlations", []),
        "outliers": stats.get("outliers", {}),
        "distributions": stats.get("distributions", {}),
        "auto_insights": auto_insights,
    }


@router.post("/trend")
async def get_trend(
    current_user: User = Depends(get_current_user),
    filename: Optional[str] = None,
    date_col: Optional[str] = None,
    value_col: Optional[str] = None,
):
    """Cache'teki verinin trend analizi"""
    if not ANALYZER_AVAILABLE:
        raise HTTPException(status_code=503, detail="Analiz modÃ¼lÃ¼ kullanÄ±lamÄ±yor")
    
    cached = _get_cached(current_user.id, filename)
    if not cached or "df" not in cached:
        raise HTTPException(status_code=404, detail="Ã–nce bir dosya yÃ¼kleyin")
    
    df = cached["df"]
    result = trend_analysis(df, date_col=date_col, value_col=value_col)
    
    if not result.get("success"):
        raise HTTPException(status_code=400, detail=result.get("error"))
    
    return {"success": True, "filename": cached["filename"], **result}


@router.post("/compare")
async def compare_groups(
    current_user: User = Depends(get_current_user),
    filename: Optional[str] = None,
    group_col: Optional[str] = None,
):
    """Cache'teki veride grup karÅŸÄ±laÅŸtÄ±rmasÄ±"""
    if not ANALYZER_AVAILABLE:
        raise HTTPException(status_code=503, detail="Analiz modÃ¼lÃ¼ kullanÄ±lamÄ±yor")
    
    cached = _get_cached(current_user.id, filename)
    if not cached or "df" not in cached:
        raise HTTPException(status_code=404, detail="Ã–nce bir dosya yÃ¼kleyin")
    
    df = cached["df"]
    result = comparison_analysis(df, group_col=group_col)
    
    if not result.get("success"):
        raise HTTPException(status_code=400, detail=result.get("error"))
    
    return {"success": True, "filename": cached["filename"], **result}


@router.get("/cached")
async def list_cached_files(
    current_user: User = Depends(get_current_user),
):
    """KullanÄ±cÄ±nÄ±n cache'teki dosyalarÄ±nÄ± listele"""
    if current_user.id not in _analysis_cache:
        return {"files": [], "count": 0}
    
    files = []
    for fname, data in _analysis_cache[current_user.id].items():
        info = {
            "filename": fname,
            "uploaded_at": data.get("uploaded_at"),
        }
        if "discovery" in data:
            d = data["discovery"]
            info["rows"] = d["row_count"]
            info["cols"] = d["col_count"]
            info["type"] = "tabular"
        else:
            info["type"] = "text"
        files.append(info)
    
    return {"files": files, "count": len(files)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ANALÄ°Z SONUÃ‡LARINI DIÅA AKTAR (v3.9.7)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXPORT_AVAILABLE = False
try:
    from app.core.export_service import generate_export, get_export_info
    EXPORT_AVAILABLE = True
except Exception:
    pass


class AnalysisExportRequest(BaseModel):
    """Analiz sonucu export talebi"""
    content: str = Field(..., description="Analiz sonuÃ§ metni (markdown)")
    format: str = Field("excel", description="excel, pdf, csv, word, pptx")
    title: Optional[str] = Field(None, description="Rapor baÅŸlÄ±ÄŸÄ±")
    analysis_type: Optional[str] = Field(None, description="Analiz tipi")
    filename: Optional[str] = Field(None, description="Orijinal dosya adÄ±")


@router.post("/export")
async def export_analysis(
    req: AnalysisExportRequest,
    current_user: User = Depends(get_current_user),
):
    """Analiz sonucunu Excel/PDF/CSV/Word/PPTX olarak dÄ±ÅŸa aktar"""
    if not EXPORT_AVAILABLE:
        raise HTTPException(status_code=503, detail="Export modÃ¼lÃ¼ kullanÄ±lamÄ±yor")

    fmt = req.format.lower().strip()
    if fmt not in ("excel", "pdf", "csv", "word", "pptx"):
        raise HTTPException(status_code=400, detail=f"Desteklenmeyen format: {fmt}")

    # BaÅŸlÄ±k oluÅŸtur
    type_labels = {
        "full": "Tam Analiz", "pivot": "Pivot Tablo", "trend": "Trend Analizi",
        "compare": "KarÅŸÄ±laÅŸtÄ±rma", "summary": "Ã–zet Rapor", "recommend": "Tavsiyeler",
        "report": "Profesyonel Rapor", "anomaly": "Anomali Tespiti", "correlation": "Korelasyon",
        "distribution": "DaÄŸÄ±lÄ±m Analizi", "forecast": "Tahminleme", "pareto": "Pareto ABC",
        "quality": "Veri Kalitesi", "profitability": "KarlÄ±lÄ±k Analizi",
        "bottleneck": "DarboÄŸaz Analizi", "executive": "SaÄŸlÄ±k Skoru",
        "benchmark": "KÄ±yaslama Raporu",
    }
    title = req.title or type_labels.get(req.analysis_type, "Analiz Raporu")
    if req.filename:
        title = f"{title} â€” {req.filename}"

    try:
        result = generate_export(req.content, fmt, title)
        if not result.get("success"):
            raise HTTPException(status_code=500, detail=result.get("error", "Export hatasÄ±"))

        return {
            "success": True,
            "file_id": result["file_id"],
            "filename": result["filename"],
            "format": fmt,
            "download_url": f"/export/download/{result['file_id']}",
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error("analysis_export_error", error=str(e))
        raise HTTPException(status_code=500, detail=f"Export sÄ±rasÄ±nda hata: {str(e)}")


@router.get("/export/download/{file_id}")
async def download_analysis_export(
    file_id: str,
    current_user: User = Depends(get_current_user),
):
    """Export dosyasÄ±nÄ± indir"""
    if not EXPORT_AVAILABLE:
        raise HTTPException(status_code=503, detail="Export modÃ¼lÃ¼ kullanÄ±lamÄ±yor")

    info = get_export_info(file_id)
    if not info:
        raise HTTPException(status_code=404, detail="Dosya bulunamadÄ± veya sÃ¼resi dolmuÅŸ")

    import os
    if not os.path.exists(info["path"]):
        raise HTTPException(status_code=404, detail="Dosya disk Ã¼zerinde bulunamadÄ±")

    media_types = {
        "excel": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        "pdf": "application/pdf",
        "csv": "text/csv",
        "word": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        "pptx": "application/vnd.openxmlformats-officedocument.presentationml.presentation",
    }

    return FileResponse(
        path=info["path"],
        filename=info["filename"],
        media_type=media_types.get(info["format"], "application/octet-stream"),
        headers={"Content-Disposition": f'attachment; filename="{info["filename"]}"'},
    )
