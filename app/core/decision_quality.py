"""Decision Quality Scoring â€” Karar Kalitesi Puanlama Motoru

Her AI Ã¶nerisine bÃ¼tÃ¼nleÅŸik bir kalite skoru atar.
Patron sorusu: "Bu Ã¶neriye ne kadar gÃ¼venebilirim?" â†’ Net cevap Ã¼retir.

v5.5.0 Enterprise Eklemeleri:
  â€¢ Outcome comparison â€” tahmin vs gerÃ§ekleÅŸen sonuÃ§ karÅŸÄ±laÅŸtÄ±rma
  â€¢ Regret metric â€” "keÅŸke farklÄ± karar verseydik" Ã¶lÃ§Ã¼mÃ¼
  â€¢ Counterfactual performance â€” alternatif senaryo analizi

Toplanan sinyaller:
  1. Reflection kalitesi     â€” 5 kriter puanÄ± (data_accuracy, logical_consistency, ...)
  2. Belirsizlik seviyesi    â€” Uncertainty Quantification ensemble skoru
  3. Risk seviyesi           â€” Decision Gatekeeper composite risk
  4. Tarihsel baÅŸarÄ± oranÄ±   â€” Meta Learning strateji profili
  5. Veri gÃ¼venirliÄŸi        â€” RAG kaynak kalitesi, web doÄŸrulama
  6. Governance uyumu        â€” Bias, drift, compliance
  7. KonsensÃ¼s derecesi      â€” Multi-agent debate uyuÅŸmasÄ±
  8. Nedensel gÃ¼Ã§            â€” Causal inference kanÄ±t gÃ¼cÃ¼

Ã‡Ä±ktÄ±: 0-100 arasÄ± bÃ¼tÃ¼nleÅŸik kalite skoru + gÃ¼ven bandÄ± + aÃ§Ä±klama
"""

from __future__ import annotations
import time
import json
import math
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
from collections import deque
import structlog

logger = structlog.get_logger()

_OUTCOME_DIR = Path("data/decision_outcomes")
_OUTCOME_DIR.mkdir(parents=True, exist_ok=True)


# â”€â”€â”€ Enums â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class QualityBand(Enum):
    """Kalite skor bandÄ±"""
    EXCEPTIONAL = "exceptional"      # 90-100
    HIGH = "high"                    # 75-89
    MODERATE = "moderate"            # 55-74
    LOW = "low"                      # 35-54
    INSUFFICIENT = "insufficient"    # 0-34


class SignalReliability(Enum):
    """Sinyal gÃ¼venirlik seviyesi"""
    STRONG = "strong"          # ModÃ¼l aktif, veri zengin
    MODERATE = "moderate"      # ModÃ¼l aktif ama veri sÄ±nÄ±rlÄ±
    WEAK = "weak"              # ModÃ¼l inaktif, varsayÄ±lan kullanÄ±ldÄ±
    UNAVAILABLE = "unavailable"  # ModÃ¼l yok


# â”€â”€â”€ Data Classes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class QualitySignal:
    """Tek bir kalite sinyali"""
    name: str
    source_module: str
    raw_value: float           # 0-100 normalize edilmiÅŸ
    weight: float              # 0-1
    reliability: SignalReliability
    description: str = ""
    details: Dict[str, Any] = field(default_factory=dict)


@dataclass
class QualityBreakdown:
    """Kalite skor kÄ±rÄ±lÄ±mÄ±"""
    data_reliability: float = 0.0       # Veri gÃ¼venilirliÄŸi (0-100)
    uncertainty_level: float = 0.0      # Belirsizlik seviyesi (0-100, dÃ¼ÅŸÃ¼k=iyi)
    risk_level: float = 0.0            # Risk seviyesi (0-100, dÃ¼ÅŸÃ¼k=iyi)
    historical_success: float = 0.0    # Tarihsel baÅŸarÄ± oranÄ± (0-100)
    governance_compliance: float = 0.0  # Governance uyumu (0-100)
    reasoning_depth: float = 0.0       # Muhakeme derinliÄŸi (0-100)
    consensus_degree: float = 0.0      # KonsensÃ¼s derecesi (0-100)
    evidence_strength: float = 0.0     # KanÄ±t gÃ¼cÃ¼ (0-100)


@dataclass
class QualityResult:
    """Nihai kalite skoru sonucu"""
    overall_score: float                # 0-100
    band: QualityBand
    band_label_tr: str                  # TÃ¼rkÃ§e band etiketi
    confidence_interval: Tuple[float, float]  # (low, high)
    breakdown: QualityBreakdown
    signals: List[QualitySignal]
    signal_coverage: float              # KaÃ§ sinyal aktif (0-1)
    recommendation_tr: str              # TÃ¼rkÃ§e gÃ¼ven aÃ§Ä±klamasÄ±
    executive_line: str                 # Tek satÄ±r yÃ¶netici Ã¶zeti
    timestamp: float = field(default_factory=time.time)

    def to_dict(self) -> dict:
        return {
            "overall_score": round(self.overall_score, 1),
            "band": self.band.value,
            "band_label": self.band_label_tr,
            "confidence_interval": {
                "low": round(self.confidence_interval[0], 1),
                "high": round(self.confidence_interval[1], 1),
            },
            "breakdown": {
                "data_reliability": round(self.breakdown.data_reliability, 1),
                "uncertainty_level": round(self.breakdown.uncertainty_level, 1),
                "risk_level": round(self.breakdown.risk_level, 1),
                "historical_success": round(self.breakdown.historical_success, 1),
                "governance_compliance": round(self.breakdown.governance_compliance, 1),
                "reasoning_depth": round(self.breakdown.reasoning_depth, 1),
                "consensus_degree": round(self.breakdown.consensus_degree, 1),
                "evidence_strength": round(self.breakdown.evidence_strength, 1),
            },
            "signal_coverage": round(self.signal_coverage, 2),
            "recommendation": self.recommendation_tr,
            "executive_line": self.executive_line,
            "signal_count": len(self.signals),
        }


# â”€â”€â”€ Sinyal AÄŸÄ±rlÄ±klarÄ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Åirket iÃ§i karar destek odaklÄ± aÄŸÄ±rlÄ±klar
SIGNAL_WEIGHTS = {
    "reflection_quality": 0.20,       # Temel kalite
    "uncertainty": 0.15,              # Belirsizlik
    "risk": 0.15,                     # Risk seviyesi
    "historical_success": 0.12,       # GeÃ§miÅŸ baÅŸarÄ±
    "data_reliability": 0.12,         # Veri kalitesi
    "governance": 0.10,               # Uyum
    "consensus": 0.08,                # KonsensÃ¼s
    "evidence_strength": 0.08,        # KanÄ±t gÃ¼cÃ¼
}

BAND_LABELS_TR = {
    QualityBand.EXCEPTIONAL: "Ã‡ok YÃ¼ksek GÃ¼venilirlik",
    QualityBand.HIGH: "YÃ¼ksek GÃ¼venilirlik",
    QualityBand.MODERATE: "Orta GÃ¼venilirlik",
    QualityBand.LOW: "DÃ¼ÅŸÃ¼k GÃ¼venilirlik",
    QualityBand.INSUFFICIENT: "Yetersiz GÃ¼venilirlik",
}


# â”€â”€â”€ Signal Collectors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class SignalCollector:
    """Pipeline Ã§Ä±ktÄ±larÄ±ndan kalite sinyalleri toplar"""

    @staticmethod
    def from_reflection(reflection_data: Optional[dict]) -> QualitySignal:
        """Reflection modÃ¼lÃ¼nden kalite sinyali"""
        if not reflection_data or not isinstance(reflection_data, dict):
            return QualitySignal(
                name="reflection_quality",
                source_module="reflection",
                raw_value=50.0,
                weight=SIGNAL_WEIGHTS["reflection_quality"],
                reliability=SignalReliability.UNAVAILABLE,
                description="Reflection verisi mevcut deÄŸil",
            )

        confidence = reflection_data.get("confidence", 50)
        criteria = reflection_data.get("criteria_scores", {})
        issues = reflection_data.get("issues", [])

        # Kriter bazlÄ± ortalama
        if criteria:
            criteria_avg = sum(criteria.values()) / len(criteria)
        else:
            criteria_avg = confidence

        # SayÄ±sal doÄŸrulama cezasÄ±
        num_val = reflection_data.get("numerical_validation", {})
        num_penalty = 0
        if isinstance(num_val, dict):
            mismatches = num_val.get("mismatches", 0)
            num_penalty = min(mismatches * 5, 20)

        score = max(0, min(100, (confidence * 0.5 + criteria_avg * 0.5) - num_penalty - len(issues) * 3))

        return QualitySignal(
            name="reflection_quality",
            source_module="reflection",
            raw_value=score,
            weight=SIGNAL_WEIGHTS["reflection_quality"],
            reliability=SignalReliability.STRONG if criteria else SignalReliability.MODERATE,
            description=f"GÃ¼ven: {confidence}, Kriter ort: {criteria_avg:.0f}, Sorun: {len(issues)}",
            details={"confidence": confidence, "criteria_avg": criteria_avg, "issues_count": len(issues)},
        )

    @staticmethod
    def from_uncertainty(uncertainty_data: Optional[dict]) -> QualitySignal:
        """Uncertainty Quantification'dan sinyal â€” dÃ¼ÅŸÃ¼k belirsizlik = yÃ¼ksek kalite"""
        if not uncertainty_data or not isinstance(uncertainty_data, dict):
            return QualitySignal(
                name="uncertainty",
                source_module="uncertainty_quantification",
                raw_value=50.0,
                weight=SIGNAL_WEIGHTS["uncertainty"],
                reliability=SignalReliability.UNAVAILABLE,
                description="Belirsizlik verisi mevcut deÄŸil",
            )

        ensemble = uncertainty_data.get("ensemble_confidence", 50)
        margin = uncertainty_data.get("margin_of_error", 15)
        agreement = uncertainty_data.get("source_agreement", 0.5)

        # DÃ¼ÅŸÃ¼k belirsizlik = yÃ¼ksek kalite
        # ensemble zaten 0-100 gÃ¼ven, doÄŸrudan kullan
        # margin yÃ¼ksekse cezala
        score = max(0, min(100, ensemble - margin * 0.5 + agreement * 10))

        return QualitySignal(
            name="uncertainty",
            source_module="uncertainty_quantification",
            raw_value=score,
            weight=SIGNAL_WEIGHTS["uncertainty"],
            reliability=SignalReliability.STRONG,
            description=f"Ensemble: %{ensemble:.0f} Â± {margin:.0f}, Kaynak uyumu: {agreement:.2f}",
            details={"ensemble": ensemble, "margin": margin, "agreement": agreement},
        )

    @staticmethod
    def from_risk(gate_data: Optional[dict]) -> QualitySignal:
        """Decision Gatekeeper'dan risk sinyali â€” dÃ¼ÅŸÃ¼k risk = yÃ¼ksek kalite"""
        if not gate_data or not isinstance(gate_data, dict):
            return QualitySignal(
                name="risk",
                source_module="decision_gatekeeper",
                raw_value=65.0,
                weight=SIGNAL_WEIGHTS["risk"],
                reliability=SignalReliability.UNAVAILABLE,
                description="Risk kapÄ±sÄ± verisi mevcut deÄŸil",
            )

        composite = gate_data.get("composite_risk_score", 0.3)
        verdict = gate_data.get("verdict", "PASS")
        risk_level = gate_data.get("risk_level", "unknown")
        signal_count = gate_data.get("signal_count", 0)

        # Risk skoru ters Ã§evir (dÃ¼ÅŸÃ¼k risk = yÃ¼ksek kalite)
        risk_quality = max(0, min(100, (1 - composite) * 100))

        # Verdict bazlÄ± bonus/ceza
        verdict_mod = {
            "PASS": 5, "PASS_WITH_WARNING": 0,
            "BLOCK": -20, "ESCALATE": -30,
        }
        risk_quality += verdict_mod.get(verdict, 0)
        risk_quality = max(0, min(100, risk_quality))

        return QualitySignal(
            name="risk",
            source_module="decision_gatekeeper",
            raw_value=risk_quality,
            weight=SIGNAL_WEIGHTS["risk"],
            reliability=SignalReliability.STRONG if signal_count > 2 else SignalReliability.MODERATE,
            description=f"Composite risk: {composite:.2f}, Karar: {verdict}",
            details={"composite": composite, "verdict": verdict, "risk_level": risk_level},
        )

    @staticmethod
    def from_meta_learning(meta_data: Optional[dict]) -> QualitySignal:
        """Meta Learning'den tarihsel baÅŸarÄ± sinyali"""
        if not meta_data or not isinstance(meta_data, dict):
            return QualitySignal(
                name="historical_success",
                source_module="meta_learning",
                raw_value=60.0,
                weight=SIGNAL_WEIGHTS["historical_success"],
                reliability=SignalReliability.UNAVAILABLE,
                description="Meta Ã¶ÄŸrenme verisi mevcut deÄŸil",
            )

        quality_trend = meta_data.get("quality_trend", {})
        strategy_success = meta_data.get("strategy_success_rate", 0.6)
        domain_perf = meta_data.get("domain_performance", {})

        avg_quality = quality_trend.get("avg_confidence", 60)
        trend_slope = quality_trend.get("slope", 0)

        # Trend yÃ¶nÃ¼ bonus: pozitif slope = iyileÅŸme
        trend_bonus = min(10, max(-10, trend_slope * 100))

        score = max(0, min(100, avg_quality + trend_bonus + strategy_success * 10))

        return QualitySignal(
            name="historical_success",
            source_module="meta_learning",
            raw_value=score,
            weight=SIGNAL_WEIGHTS["historical_success"],
            reliability=SignalReliability.STRONG if isinstance(quality_trend, dict) and quality_trend else SignalReliability.WEAK,
            description=f"Ort kalite: {avg_quality:.0f}, Trend: {trend_slope:+.3f}",
            details={"avg_quality": avg_quality, "trend_slope": trend_slope},
        )

    @staticmethod
    def from_data_sources(
        rag_used: bool = False,
        web_searched: bool = False,
        sources: Optional[list] = None,
        source_citation_valid: Optional[bool] = None,
    ) -> QualitySignal:
        """Veri kaynaklarÄ±nÄ±n gÃ¼venilirliÄŸi"""
        score = 40.0  # Temel (LLM bilgisi)

        if rag_used:
            score += 20  # RAG veri tabanÄ±ndan bilgi
        if web_searched:
            score += 10  # Web aramasÄ± yapÄ±ldÄ±
        if sources and len(sources) > 0:
            score += min(15, len(sources) * 3)  # Kaynak sayÄ±sÄ±
        if source_citation_valid is True:
            score += 15  # Kaynak doÄŸrulamasÄ± geÃ§ti
        elif source_citation_valid is False:
            score -= 10  # Kaynak doÄŸrulamasÄ± baÅŸarÄ±sÄ±z

        score = max(0, min(100, score))

        reliability = SignalReliability.STRONG if rag_used else (
            SignalReliability.MODERATE if web_searched else SignalReliability.WEAK
        )

        return QualitySignal(
            name="data_reliability",
            source_module="data_sources",
            raw_value=score,
            weight=SIGNAL_WEIGHTS["data_reliability"],
            reliability=reliability,
            description=f"RAG: {'âœ“' if rag_used else 'âœ—'}, Web: {'âœ“' if web_searched else 'âœ—'}, Kaynak: {len(sources or [])}",
            details={"rag": rag_used, "web": web_searched, "source_count": len(sources or [])},
        )

    @staticmethod
    def from_governance(governance_data: Optional[dict]) -> QualitySignal:
        """AI Governance'dan uyum sinyali"""
        if not governance_data or not isinstance(governance_data, dict):
            return QualitySignal(
                name="governance",
                source_module="governance",
                raw_value=70.0,
                weight=SIGNAL_WEIGHTS["governance"],
                reliability=SignalReliability.UNAVAILABLE,
                description="Governance verisi mevcut deÄŸil",
            )

        compliance = governance_data.get("compliance_score", 0.7)
        bias_score = governance_data.get("bias_score", 0)
        drift = governance_data.get("drift_detected", False)
        alert = governance_data.get("alert_triggered", False)

        # Compliance zaten 0-1 arasÄ±
        score = compliance * 100

        # Bias cezasÄ±
        if bias_score > 0.3:
            score -= bias_score * 20

        # Drift cezasÄ±
        if drift:
            score -= 15

        # Alert cezasÄ±
        if alert:
            score -= 10

        score = max(0, min(100, score))

        return QualitySignal(
            name="governance",
            source_module="governance",
            raw_value=score,
            weight=SIGNAL_WEIGHTS["governance"],
            reliability=SignalReliability.STRONG,
            description=f"Uyum: %{compliance*100:.0f}, Bias: {bias_score:.2f}, Drift: {'Var' if drift else 'Yok'}",
            details={"compliance": compliance, "bias": bias_score, "drift": drift, "alert": alert},
        )

    @staticmethod
    def from_debate(debate_data: Optional[dict]) -> QualitySignal:
        """Multi-Agent Debate'den konsensÃ¼s sinyali"""
        if not debate_data or not isinstance(debate_data, dict):
            return QualitySignal(
                name="consensus",
                source_module="multi_agent_debate",
                raw_value=60.0,
                weight=SIGNAL_WEIGHTS["consensus"],
                reliability=SignalReliability.UNAVAILABLE,
                description="TartÄ±ÅŸma verisi mevcut deÄŸil",
            )

        consensus_score = debate_data.get("consensus_score", 0.5)
        agreement_ratio = debate_data.get("agreement_ratio", 0.5)
        perspectives = debate_data.get("perspectives_count", 0)

        score = max(0, min(100, consensus_score * 50 + agreement_ratio * 50))

        return QualitySignal(
            name="consensus",
            source_module="multi_agent_debate",
            raw_value=score,
            weight=SIGNAL_WEIGHTS["consensus"],
            reliability=SignalReliability.STRONG if perspectives >= 3 else SignalReliability.MODERATE,
            description=f"KonsensÃ¼s: {consensus_score:.2f}, UzlaÅŸma: %{agreement_ratio*100:.0f}, Perspektif: {perspectives}",
            details={"consensus": consensus_score, "agreement": agreement_ratio, "perspectives": perspectives},
        )

    @staticmethod
    def from_causal(causal_data: Optional[dict]) -> QualitySignal:
        """Causal Inference'dan kanÄ±t gÃ¼cÃ¼ sinyali"""
        if not causal_data or not isinstance(causal_data, dict):
            return QualitySignal(
                name="evidence_strength",
                source_module="causal_inference",
                raw_value=50.0,
                weight=SIGNAL_WEIGHTS["evidence_strength"],
                reliability=SignalReliability.UNAVAILABLE,
                description="Nedensel analiz verisi mevcut deÄŸil",
            )

        root_causes = causal_data.get("root_causes_found", 0)
        evidence_score = causal_data.get("evidence_score", 0.5)
        confidence = causal_data.get("confidence", 0.5)

        score = max(0, min(100, evidence_score * 50 + confidence * 30 + min(root_causes * 5, 20)))

        return QualitySignal(
            name="evidence_strength",
            source_module="causal_inference",
            raw_value=score,
            weight=SIGNAL_WEIGHTS["evidence_strength"],
            reliability=SignalReliability.STRONG if root_causes > 0 else SignalReliability.WEAK,
            description=f"KanÄ±t: {evidence_score:.2f}, GÃ¼ven: {confidence:.2f}, KÃ¶k neden: {root_causes}",
            details={"evidence": evidence_score, "confidence": confidence, "root_causes": root_causes},
        )


# â”€â”€â”€ Quality Scorer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class QualityScorer:
    """Sinyalleri birleÅŸtirip nihai kalite skoru Ã¼retir"""

    @staticmethod
    def _get_band(score: float) -> QualityBand:
        if score >= 90:
            return QualityBand.EXCEPTIONAL
        elif score >= 75:
            return QualityBand.HIGH
        elif score >= 55:
            return QualityBand.MODERATE
        elif score >= 35:
            return QualityBand.LOW
        return QualityBand.INSUFFICIENT

    @staticmethod
    def _confidence_interval(signals: List[QualitySignal], overall: float) -> Tuple[float, float]:
        """Sinyallerin daÄŸÄ±lÄ±mÄ±na gÃ¶re gÃ¼ven aralÄ±ÄŸÄ±"""
        if not signals:
            return (max(0, overall - 15), min(100, overall + 15))

        values = [s.raw_value for s in signals if s.reliability != SignalReliability.UNAVAILABLE]
        if len(values) < 2:
            return (max(0, overall - 12), min(100, overall + 12))

        mean = sum(values) / len(values)
        variance = sum((v - mean) ** 2 for v in values) / len(values)
        std = math.sqrt(variance)

        # KaynaÄŸÄ± eksik sinyaller aralÄ±ÄŸÄ± geniÅŸletir
        unavailable_count = sum(1 for s in signals if s.reliability == SignalReliability.UNAVAILABLE)
        expansion = unavailable_count * 2.5

        margin = min(20, std * 0.8 + expansion)
        return (max(0, overall - margin), min(100, overall + margin))

    @staticmethod
    def _recommendation(band: QualityBand, breakdown: QualityBreakdown) -> str:
        """TÃ¼rkÃ§e gÃ¼ven tavsiyesi"""
        recs = {
            QualityBand.EXCEPTIONAL: "Bu Ã¶neri Ã§ok yÃ¼ksek gÃ¼venilirlikle sunulmaktadÄ±r. Veri, analiz ve risk deÄŸerlendirmesi gÃ¼Ã§lÃ¼dÃ¼r.",
            QualityBand.HIGH: "Bu Ã¶neri yÃ¼ksek gÃ¼venilirlikle sunulmaktadÄ±r. BirkaÃ§ kÃ¼Ã§Ã¼k belirsizlik noktasÄ± mevcut olsa da genel deÄŸerlendirme saÄŸlamdÄ±r.",
            QualityBand.MODERATE: "Bu Ã¶neriye orta dÃ¼zeyde gÃ¼venle yaklaÅŸÄ±lmalÄ±dÄ±r. BazÄ± veri eksiklikleri veya belirsizlikler mevcuttur.",
            QualityBand.LOW: "Bu Ã¶neriye dikkatle yaklaÅŸÄ±lmalÄ±dÄ±r. Veri gÃ¼venilirliÄŸi dÃ¼ÅŸÃ¼k veya risk seviyesi yÃ¼ksektir. Ek doÄŸrulama Ã¶nerilir.",
            QualityBand.INSUFFICIENT: "Bu Ã¶neri yetersiz veri veya yÃ¼ksek belirsizlik nedeniyle dÃ¼ÅŸÃ¼k gÃ¼venilirlikle sunulmaktadÄ±r. Karar vermeden Ã¶nce ek analiz yapÄ±lmasÄ± ÅŸiddetle tavsiye edilir.",
        }

        base = recs[band]

        # Spesifik uyarÄ±lar
        warnings = []
        if breakdown.risk_level > 70:
            warnings.append("âš ï¸ Risk seviyesi yÃ¼ksek")
        if breakdown.uncertainty_level < 40:
            warnings.append("âš ï¸ Belirsizlik seviyesi yÃ¼ksek")
        if breakdown.data_reliability < 50:
            warnings.append("âš ï¸ Veri gÃ¼venilirliÄŸi dÃ¼ÅŸÃ¼k")
        if breakdown.historical_success < 40:
            warnings.append("âš ï¸ Benzer Ã¶nerilerde tarihsel baÅŸarÄ± dÃ¼ÅŸÃ¼k")

        if warnings:
            base += "\n" + " | ".join(warnings)

        return base

    @staticmethod
    def _executive_line(score: float, band: QualityBand) -> str:
        """Tek satÄ±r yÃ¶netici Ã¶zeti"""
        band_emoji = {
            QualityBand.EXCEPTIONAL: "ğŸŸ¢",
            QualityBand.HIGH: "ğŸŸ¢",
            QualityBand.MODERATE: "ğŸŸ¡",
            QualityBand.LOW: "ğŸŸ ",
            QualityBand.INSUFFICIENT: "ğŸ”´",
        }
        emoji = band_emoji[band]
        label = BAND_LABELS_TR[band]
        return f"{emoji} Ã–neri Kalite Skoru: {score:.0f}/100 â€” {label}"

    def score(self, signals: List[QualitySignal]) -> QualityResult:
        """TÃ¼m sinyallerden nihai skor Ã¼ret"""
        if not signals:
            bd = QualityBreakdown()
            band = QualityBand.INSUFFICIENT
            return QualityResult(
                overall_score=0,
                band=band,
                band_label_tr=BAND_LABELS_TR[band],
                confidence_interval=(0, 30),
                breakdown=bd,
                signals=[],
                signal_coverage=0,
                recommendation_tr=self._recommendation(band, bd),
                executive_line=self._executive_line(0, band),
            )

        # AÄŸÄ±rlÄ±klÄ± ortalama
        total_weight = sum(s.weight for s in signals)
        if total_weight == 0:
            total_weight = 1

        weighted_sum = sum(s.raw_value * s.weight for s in signals)
        overall = weighted_sum / total_weight

        # Signal coverage â€” kaÃ§ sinyal "gerÃ§ek" veri iÃ§eriyor
        active_signals = sum(1 for s in signals if s.reliability != SignalReliability.UNAVAILABLE)
        coverage = active_signals / len(signals) if signals else 0

        # DÃ¼ÅŸÃ¼k coverage cezasÄ±
        if coverage < 0.5:
            overall *= 0.85 + coverage * 0.3  # %50'den az sinyal â†’ skor dÃ¼ÅŸer

        overall = max(0, min(100, overall))

        # Breakdown
        signal_map = {s.name: s.raw_value for s in signals}
        breakdown = QualityBreakdown(
            data_reliability=signal_map.get("data_reliability", 0),
            uncertainty_level=signal_map.get("uncertainty", 0),
            risk_level=signal_map.get("risk", 0),
            historical_success=signal_map.get("historical_success", 0),
            governance_compliance=signal_map.get("governance", 0),
            reasoning_depth=signal_map.get("reflection_quality", 0),
            consensus_degree=signal_map.get("consensus", 0),
            evidence_strength=signal_map.get("evidence_strength", 0),
        )

        band = self._get_band(overall)
        ci = self._confidence_interval(signals, overall)

        return QualityResult(
            overall_score=overall,
            band=band,
            band_label_tr=BAND_LABELS_TR[band],
            confidence_interval=ci,
            breakdown=breakdown,
            signals=signals,
            signal_coverage=coverage,
            recommendation_tr=self._recommendation(band, breakdown),
            executive_line=self._executive_line(overall, band),
        )


# â”€â”€â”€ Tracker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class QualityTracker:
    """Kalite skoru geÃ§miÅŸini takip eder"""

    MAX_HISTORY = 500

    def __init__(self):
        self._history: List[dict] = []
        self._band_counts: Dict[str, int] = {b.value: 0 for b in QualityBand}
        self._total = 0
        self._score_sum = 0.0

    def record(self, result: QualityResult, question: str = "", department: str = ""):
        entry = {
            "score": result.overall_score,
            "band": result.band.value,
            "coverage": result.signal_coverage,
            "question_preview": question[:80] if question else "",
            "department": department,
            "timestamp": result.timestamp,
        }
        self._history.append(entry)
        if len(self._history) > self.MAX_HISTORY:
            self._history = self._history[-self.MAX_HISTORY:]

        self._band_counts[result.band.value] = self._band_counts.get(result.band.value, 0) + 1
        self._total += 1
        self._score_sum += result.overall_score

    def get_stats(self) -> dict:
        avg = self._score_sum / self._total if self._total else 0
        recent = self._history[-20:] if self._history else []
        recent_avg = sum(r["score"] for r in recent) / len(recent) if recent else 0

        # Trend
        if len(recent) >= 5:
            first_half = recent[:len(recent)//2]
            second_half = recent[len(recent)//2:]
            first_avg = sum(r["score"] for r in first_half) / len(first_half)
            second_avg = sum(r["score"] for r in second_half) / len(second_half)
            trend = "improving" if second_avg > first_avg + 2 else ("declining" if second_avg < first_avg - 2 else "stable")
        else:
            trend = "insufficient_data"

        return {
            "total_evaluations": self._total,
            "average_score": round(avg, 1),
            "recent_average": round(recent_avg, 1),
            "trend": trend,
            "band_distribution": dict(self._band_counts),
        }

    def get_dashboard(self) -> dict:
        stats = self.get_stats()
        return {
            **stats,
            "recent_history": self._history[-10:],
        }


# â”€â”€â”€ Main Orchestrator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_scorer = QualityScorer()
_tracker = QualityTracker()
_collector = SignalCollector()


def evaluate_decision_quality(
    reflection_data: Optional[dict] = None,
    uncertainty_data: Optional[dict] = None,
    gate_data: Optional[dict] = None,
    meta_data: Optional[dict] = None,
    governance_data: Optional[dict] = None,
    debate_data: Optional[dict] = None,
    causal_data: Optional[dict] = None,
    rag_used: bool = False,
    web_searched: bool = False,
    sources: Optional[list] = None,
    source_citation_valid: Optional[bool] = None,
    question: str = "",
    department: str = "",
) -> QualityResult:
    """
    TÃ¼m pipeline Ã§Ä±ktÄ±larÄ±ndan bÃ¼tÃ¼nleÅŸik karar kalite skoru Ã¼retir.

    Args:
        reflection_data: Reflection modÃ¼lÃ¼ Ã§Ä±ktÄ±sÄ±
        uncertainty_data: Uncertainty Quantification Ã§Ä±ktÄ±sÄ±
        gate_data: Decision Gatekeeper Ã§Ä±ktÄ±sÄ±
        meta_data: Meta Learning istatistikleri
        governance_data: Governance deÄŸerlendirmesi
        debate_data: Multi-Agent Debate sonucu
        causal_data: Causal Inference sonucu
        rag_used: RAG kullanÄ±ldÄ± mÄ±
        web_searched: Web aramasÄ± yapÄ±ldÄ± mÄ±
        sources: Kaynak listesi
        source_citation_valid: Kaynak doÄŸrulamasÄ± sonucu
        question: Orijinal soru
        department: Departman

    Returns:
        QualityResult: BÃ¼tÃ¼nleÅŸik kalite skoru ve aÃ§Ä±klamasÄ±
    """
    signals = [
        _collector.from_reflection(reflection_data),
        _collector.from_uncertainty(uncertainty_data),
        _collector.from_risk(gate_data),
        _collector.from_meta_learning(meta_data),
        _collector.from_data_sources(rag_used, web_searched, sources, source_citation_valid),
        _collector.from_governance(governance_data),
        _collector.from_debate(debate_data),
        _collector.from_causal(causal_data),
    ]

    result = _scorer.score(signals)
    _tracker.record(result, question, department)

    return result


# â”€â”€â”€ Formatters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def format_quality_score(result: QualityResult) -> str:
    """Kalite skorunu Markdown formatÄ±nda gÃ¶ster"""
    lines = [
        f"\n### ğŸ“Š Karar Kalite DeÄŸerlendirmesi",
        f"",
        result.executive_line,
        f"",
        f"| Boyut | Puan |",
        f"|-------|------|",
        f"| Veri GÃ¼venilirliÄŸi | {result.breakdown.data_reliability:.0f}/100 |",
        f"| Belirsizlik KontrolÃ¼ | {result.breakdown.uncertainty_level:.0f}/100 |",
        f"| Risk DeÄŸerlendirmesi | {result.breakdown.risk_level:.0f}/100 |",
        f"| Tarihsel BaÅŸarÄ± | {result.breakdown.historical_success:.0f}/100 |",
        f"| Governance Uyumu | {result.breakdown.governance_compliance:.0f}/100 |",
        f"| Muhakeme DerinliÄŸi | {result.breakdown.reasoning_depth:.0f}/100 |",
        f"| KonsensÃ¼s Derecesi | {result.breakdown.consensus_degree:.0f}/100 |",
        f"| KanÄ±t GÃ¼cÃ¼ | {result.breakdown.evidence_strength:.0f}/100 |",
        f"",
        f"**GÃ¼ven AralÄ±ÄŸÄ±:** {result.confidence_interval[0]:.0f} â€” {result.confidence_interval[1]:.0f}",
        f"**Sinyal Kapsama:** %{result.signal_coverage*100:.0f}",
        f"",
        f"> {result.recommendation_tr}",
    ]
    return "\n".join(lines)


def format_quality_badge(result: QualityResult) -> str:
    """KÄ±sa badge â€” yanÄ±t altÄ±na eklenmek iÃ§in"""
    return result.executive_line


# â”€â”€â”€ Dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_dashboard() -> dict:
    base = {
        "module": "decision_quality",
        "module_name": "Karar Kalite Skoru",
        **_tracker.get_dashboard(),
    }
    # v5.5.0: Outcome & Regret istatistikleri
    base["outcome_tracking"] = _outcome_tracker.get_regret_stats()
    return base


def get_statistics() -> dict:
    return _tracker.get_stats()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  v5.5.0 Enterprise: Outcome Comparison & Regret Metric
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_OUTCOME_LOG = _OUTCOME_DIR / "outcome_log.jsonl"


class OutcomeTracker:
    """
    Tahmin vs gerÃ§ekleÅŸen sonuÃ§ karÅŸÄ±laÅŸtÄ±rma.

    AkÄ±ÅŸ:
      1. Decision yapÄ±lÄ±r â†’ record_prediction(decision_id, predicted_outcome, confidence)
      2. SonuÃ§ gerÃ§ekleÅŸir â†’ record_actual_outcome(decision_id, actual_outcome)
      3. KarÅŸÄ±laÅŸtÄ±rma â†’ get_outcome_comparison(decision_id)
    """

    def __init__(self):
        self._predictions: Dict[str, dict] = {}  # decision_id â†’ prediction
        self._outcomes: Dict[str, dict] = {}      # decision_id â†’ actual
        self._regret_history: deque = deque(maxlen=500)
        self._accuracy_history: deque[float] = deque(maxlen=500)

    def record_prediction(
        self,
        decision_id: str,
        predicted_outcome: str,
        confidence: float,
        quality_score: float = 0.0,
        alternatives: Optional[List[Dict]] = None,
        context: Optional[Dict] = None,
    ):
        """Karar tahmini kaydet.

        Args:
            decision_id: Benzersiz karar kimliÄŸi.
            predicted_outcome: Tahmin edilen sonuÃ§ (metin).
            confidence: Tahmin gÃ¼veni (0-100).
            quality_score: Karar kalite skoru.
            alternatives: DeÄŸerlendirilen alternatif seÃ§enekler.
            context: Karar baÄŸlamÄ±.
        """
        self._predictions[decision_id] = {
            "decision_id": decision_id,
            "predicted_outcome": predicted_outcome,
            "confidence": confidence,
            "quality_score": quality_score,
            "alternatives": alternatives or [],
            "context": context or {},
            "timestamp": time.time(),
        }

    def record_actual_outcome(
        self,
        decision_id: str,
        actual_outcome: str,
        success: bool = True,
        impact_score: float = 0.0,
        feedback: str = "",
    ):
        """GerÃ§ekleÅŸen sonucu kaydet ve regret hesapla.

        Args:
            decision_id: Karar kimliÄŸi.
            actual_outcome: GerÃ§ekleÅŸen sonuÃ§.
            success: SonuÃ§ baÅŸarÄ±lÄ± mÄ±?
            impact_score: Etki puanÄ± (0-100, opsiyonel).
            feedback: KullanÄ±cÄ± geri bildirimi.
        """
        prediction = self._predictions.get(decision_id)
        if not prediction:
            # Tahmin kaydedilmemiÅŸ ama sonucu yine de kaydet
            prediction = {"confidence": 50, "quality_score": 50, "alternatives": []}

        self._outcomes[decision_id] = {
            "decision_id": decision_id,
            "actual_outcome": actual_outcome,
            "success": success,
            "impact_score": impact_score,
            "feedback": feedback,
            "timestamp": time.time(),
        }

        # Accuracy kaydÄ±
        accuracy = 1.0 if success else 0.0
        self._accuracy_history.append(accuracy)

        # Regret hesaplama
        regret = self._calculate_regret(prediction, success, impact_score)
        self._regret_history.append(regret)

        # Diske yaz
        self._persist_outcome(decision_id, prediction, self._outcomes[decision_id], regret)

    def _calculate_regret(self, prediction: dict, success: bool, impact: float) -> dict:
        """
        Regret metriÄŸi hesapla.

        Regret = (1 - success) * confidence_at_decision * alternative_gap
        YÃ¼ksek gÃ¼venle yanlÄ±ÅŸ karar â†’ yÃ¼ksek regret
        DÃ¼ÅŸÃ¼k gÃ¼venle yanlÄ±ÅŸ karar â†’ dÃ¼ÅŸÃ¼k regret (beklenen)
        """
        confidence = prediction.get("confidence", 50) / 100
        alternatives = prediction.get("alternatives", [])

        if success:
            regret_score = 0.0
            regret_type = "none"
        else:
            # Temel regret: yanlÄ±ÅŸ karar
            regret_score = confidence * 0.5

            # Alternatif varsa: en iyi alternatifle aradaki fark
            if alternatives:
                best_alt_score = max(
                    (a.get("expected_score", 50) for a in alternatives), default=50
                )
                alt_gap = max(0, best_alt_score - prediction.get("quality_score", 50)) / 100
                regret_score += alt_gap * 0.3

            # Etki puanÄ± (yÃ¼ksek etki = yÃ¼ksek regret)
            if impact > 0:
                regret_score += (impact / 100) * 0.2

            regret_score = min(1.0, regret_score)

            if regret_score > 0.7:
                regret_type = "high"
            elif regret_score > 0.3:
                regret_type = "moderate"
            else:
                regret_type = "low"

        return {
            "regret_score": round(regret_score, 3),
            "regret_type": regret_type,
            "confidence_at_decision": confidence,
            "had_alternatives": len(alternatives) > 0,
            "timestamp": time.time(),
        }

    def _persist_outcome(self, decision_id: str, prediction: dict, outcome: dict, regret: dict):
        """Outcome log'a yaz."""
        entry = {
            "decision_id": decision_id,
            "prediction": {
                "outcome": prediction.get("predicted_outcome", ""),
                "confidence": prediction.get("confidence", 0),
                "quality_score": prediction.get("quality_score", 0),
            },
            "actual": {
                "outcome": outcome.get("actual_outcome", ""),
                "success": outcome.get("success", False),
                "impact_score": outcome.get("impact_score", 0),
            },
            "regret": regret,
            "timestamp": time.time(),
        }
        try:
            with open(_OUTCOME_LOG, "a", encoding="utf-8") as f:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.error("outcome_persist_failed", error=str(e))

    def get_outcome_comparison(self, decision_id: str) -> Optional[dict]:
        """Tahmin vs gerÃ§ek sonuÃ§ karÅŸÄ±laÅŸtÄ±rmasÄ±."""
        prediction = self._predictions.get(decision_id)
        outcome = self._outcomes.get(decision_id)

        if not prediction or not outcome:
            return None

        return {
            "decision_id": decision_id,
            "predicted": prediction.get("predicted_outcome", ""),
            "actual": outcome.get("actual_outcome", ""),
            "confidence_at_prediction": prediction.get("confidence", 0),
            "success": outcome.get("success", False),
            "impact_score": outcome.get("impact_score", 0),
            "feedback": outcome.get("feedback", ""),
        }

    def get_regret_stats(self) -> dict:
        """Regret istatistikleri."""
        regrets = list(self._regret_history)
        if not regrets:
            return {"total_decisions": 0, "avg_regret": 0, "trend": "insufficient_data"}

        scores = [r["regret_score"] for r in regrets]
        avg = sum(scores) / len(scores)

        # YÃ¼ksek regret sayÄ±sÄ±
        high_regret = sum(1 for r in regrets if r["regret_type"] == "high")

        # Trend
        if len(scores) >= 10:
            first = scores[:len(scores)//2]
            second = scores[len(scores)//2:]
            f_avg = sum(first) / len(first)
            s_avg = sum(second) / len(second)
            trend = "improving" if s_avg < f_avg - 0.05 else ("worsening" if s_avg > f_avg + 0.05 else "stable")
        else:
            trend = "insufficient_data"

        # Accuracy
        acc_list = list(self._accuracy_history)
        accuracy = sum(acc_list) / len(acc_list) * 100 if acc_list else 0

        return {
            "total_decisions": len(regrets),
            "avg_regret": round(avg, 3),
            "high_regret_count": high_regret,
            "overall_accuracy_pct": round(accuracy, 1),
            "trend": trend,
        }

    def get_counterfactual_analysis(self, decision_id: str) -> Optional[dict]:
        """Counterfactual â€” 'alternatif seÃ§seydik ne olurdu?' analizi.

        Tahmin edilen alternatiflerin beklenen skorlarÄ± ile
        gerÃ§ekleÅŸen sonucun karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±.
        """
        prediction = self._predictions.get(decision_id)
        outcome = self._outcomes.get(decision_id)

        if not prediction:
            return None

        alternatives = prediction.get("alternatives", [])
        actual_success = outcome.get("success", False) if outcome else None
        actual_impact = outcome.get("impact_score", 0) if outcome else 0

        counterfactuals = []
        for alt in alternatives:
            cf = {
                "alternative": alt.get("label", "Bilinmeyen"),
                "expected_score": alt.get("expected_score", 50),
                "would_have_been_better": not actual_success and alt.get("expected_score", 50) > prediction.get("quality_score", 50),
                "score_gap": alt.get("expected_score", 50) - prediction.get("quality_score", 50),
            }
            counterfactuals.append(cf)

        best_alternative = max(counterfactuals, key=lambda x: x["expected_score"]) if counterfactuals else None

        return {
            "decision_id": decision_id,
            "chosen_confidence": prediction.get("confidence", 0),
            "chosen_quality": prediction.get("quality_score", 0),
            "actual_success": actual_success,
            "actual_impact": actual_impact,
            "alternative_count": len(counterfactuals),
            "counterfactuals": counterfactuals,
            "best_alternative": best_alternative,
            "regret_applicable": best_alternative is not None and best_alternative.get("would_have_been_better", False),
        }


# Singleton tracker instance
_outcome_tracker = OutcomeTracker()


def record_prediction(decision_id: str, predicted_outcome: str, confidence: float,
                      quality_score: float = 0.0, alternatives: Optional[List[Dict]] = None,
                      context: Optional[Dict] = None):
    """Karar tahminini kaydet (engine.py'den Ã§aÄŸrÄ±lÄ±r)."""
    _outcome_tracker.record_prediction(
        decision_id, predicted_outcome, confidence, quality_score, alternatives, context
    )


def record_actual_outcome(decision_id: str, actual_outcome: str, success: bool = True,
                          impact_score: float = 0.0, feedback: str = ""):
    """GerÃ§ekleÅŸen sonucu kaydet (feedback endpoint'inden Ã§aÄŸrÄ±lÄ±r)."""
    _outcome_tracker.record_actual_outcome(
        decision_id, actual_outcome, success, impact_score, feedback
    )


def get_outcome_comparison(decision_id: str) -> Optional[dict]:
    """Tahmin-sonuÃ§ karÅŸÄ±laÅŸtÄ±rmasÄ±."""
    return _outcome_tracker.get_outcome_comparison(decision_id)


def get_regret_stats() -> dict:
    """Regret istatistikleri."""
    return _outcome_tracker.get_regret_stats()


def get_counterfactual(decision_id: str) -> Optional[dict]:
    """Counterfactual analizi."""
    return _outcome_tracker.get_counterfactual_analysis(decision_id)
