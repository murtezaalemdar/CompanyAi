"""
GeliÅŸmiÅŸ DokÃ¼man Analiz Motoru (Document Intelligence)

YÃ¼klenen dokÃ¼manlardan:
- Pivot tablo oluÅŸturma
- Ä°statistiksel analiz
- Trend/karÅŸÄ±laÅŸtÄ±rma raporu
- Yorum ve tavsiyeler
- Otomatik veri keÅŸfi
- DoÄŸal dil ile veri sorgulama

Desteklenen girdiler:
- Excel (.xlsx, .xls) â†’ Tam tablolu analiz
- CSV (.csv) â†’ Tablolu analiz
- JSON (.json) â†’ YapÄ±sal analiz
- PDF/DOCX/TXT â†’ Metin tabanlÄ± analiz
- RAG'daki mevcut dokÃ¼manlar â†’ Semantik analiz
"""

import io
import json
import re
from typing import Optional, Any
from datetime import datetime

import structlog
import pandas as pd
import numpy as np

logger = structlog.get_logger()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. VERÄ° PARSE & KEÅžÄ°F
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_file_to_dataframe(filename: str, file_content: bytes) -> Optional[pd.DataFrame]:
    """
    DosyayÄ± pandas DataFrame'e Ã§evir.
    Excel, CSV, JSON ve TSV destekler.
    """
    filename_lower = filename.lower()
    
    try:
        if filename_lower.endswith(('.xlsx', '.xls')):
            # Excel â€” tÃ¼m sayfalarÄ± oku, en bÃ¼yÃ¼k olanÄ± kullan
            xls = pd.ExcelFile(io.BytesIO(file_content))
            sheets = {}
            for sheet_name in xls.sheet_names:
                df = pd.read_excel(xls, sheet_name=sheet_name)
                sheets[sheet_name] = df
            
            if not sheets:
                return None
            
            # En Ã§ok satÄ±rÄ± olan sayfayÄ± dÃ¶ndÃ¼r
            main_sheet = max(sheets.values(), key=lambda x: len(x))
            
            # TÃ¼m sayfalarÄ± metadata olarak sakla
            main_sheet.attrs['_all_sheets'] = {
                name: {"rows": len(df), "cols": len(df.columns)} 
                for name, df in sheets.items()
            }
            main_sheet.attrs['_sheets_data'] = sheets
            
            return main_sheet
        
        elif filename_lower.endswith('.csv'):
            # CSV â€” farklÄ± delimiter'larÄ± dene
            for sep in [',', ';', '\t', '|']:
                try:
                    df = pd.read_csv(io.BytesIO(file_content), sep=sep, encoding='utf-8')
                    if len(df.columns) > 1:
                        return df
                except Exception:
                    continue
            # Son deneme: otomatik
            return pd.read_csv(io.BytesIO(file_content), encoding='utf-8')
        
        elif filename_lower.endswith('.json'):
            text = file_content.decode('utf-8')
            data = json.loads(text)
            
            if isinstance(data, list):
                return pd.DataFrame(data)
            elif isinstance(data, dict):
                # Ä°Ã§ iÃ§e dict'i dÃ¼zleÅŸtirmeye Ã§alÄ±ÅŸ
                if any(isinstance(v, list) for v in data.values()):
                    for key, val in data.items():
                        if isinstance(val, list) and val:
                            return pd.DataFrame(val)
                return pd.DataFrame([data])
            
        elif filename_lower.endswith('.tsv'):
            return pd.read_csv(io.BytesIO(file_content), sep='\t', encoding='utf-8')
            
    except Exception as e:
        logger.warning("parse_to_df_failed", file=filename, error=str(e))
    
    return None


def discover_data(df: pd.DataFrame) -> dict:
    """
    DataFrame'i otomatik keÅŸfet â€” sÃ¼tun tipleri, istatistikler, iliÅŸkiler.
    """
    info = {
        "row_count": len(df),
        "col_count": len(df.columns),
        "columns": [],
        "numeric_columns": [],
        "categorical_columns": [],
        "date_columns": [],
        "text_columns": [],
        "has_missing": False,
        "missing_summary": {},
        "memory_mb": round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2),
    }
    
    for col in df.columns:
        col_info = {
            "name": col,
            "dtype": str(df[col].dtype),
            "non_null": int(df[col].notna().sum()),
            "null_count": int(df[col].isna().sum()),
            "null_pct": round(df[col].isna().mean() * 100, 1),
            "unique_count": int(df[col].nunique()),
        }
        
        if df[col].isna().any():
            info["has_missing"] = True
            info["missing_summary"][col] = col_info["null_count"]
        
        # Tarih tespiti
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            info["date_columns"].append(col)
            col_info["type"] = "date"
            col_info["min"] = str(df[col].min())
            col_info["max"] = str(df[col].max())
        
        # SayÄ±sal tespiti
        elif pd.api.types.is_numeric_dtype(df[col]):
            info["numeric_columns"].append(col)
            col_info["type"] = "numeric"
            col_info["min"] = float(df[col].min()) if df[col].notna().any() else None
            col_info["max"] = float(df[col].max()) if df[col].notna().any() else None
            col_info["mean"] = round(float(df[col].mean()), 2) if df[col].notna().any() else None
            col_info["median"] = round(float(df[col].median()), 2) if df[col].notna().any() else None
            col_info["std"] = round(float(df[col].std()), 2) if df[col].notna().any() else None
            col_info["sum"] = float(df[col].sum()) if df[col].notna().any() else None
        
        # Kategorik tespiti
        elif df[col].nunique() <= max(20, len(df) * 0.05):
            info["categorical_columns"].append(col)
            col_info["type"] = "categorical"
            col_info["top_values"] = df[col].value_counts().head(10).to_dict()
        
        # Metin
        else:
            info["text_columns"].append(col)
            col_info["type"] = "text"
            col_info["avg_length"] = round(df[col].astype(str).str.len().mean(), 0)
        
        # Tarih string tespiti (sÃ¼tun string ama tarih gibi gÃ¶rÃ¼nÃ¼yor)
        if col_info.get("type") not in ("date", "numeric") and df[col].dtype == "object":
            sample = df[col].dropna().head(20).astype(str)
            date_patterns = [
                r'\d{4}[-/]\d{1,2}[-/]\d{1,2}',
                r'\d{1,2}[-/.]\d{1,2}[-/.]\d{4}',
            ]
            date_matches = sum(
                1 for s in sample 
                if any(re.search(p, s) for p in date_patterns)
            )
            if date_matches > len(sample) * 0.5:
                col_info["possible_date"] = True
                info["date_columns"].append(col)
        
        info["columns"].append(col_info)
    
    return info


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. PÄ°VOT TABLO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_pivot(
    df: pd.DataFrame,
    rows: list[str] = None,
    columns: list[str] = None,
    values: list[str] = None,
    aggfunc: str = "sum",
    fill_value: Any = 0,
) -> dict:
    """
    Pivot tablo oluÅŸtur.
    
    Parametreler:
        rows: SatÄ±r bazÄ±nda gruplama sÃ¼tunlarÄ±
        columns: SÃ¼tun bazÄ±nda gruplama
        values: Hesaplanacak deÄŸer sÃ¼tunlarÄ±
        aggfunc: sum, mean, count, min, max, std
        fill_value: BoÅŸ hÃ¼cre deÄŸeri
    """
    agg_map = {
        "sum": "sum", "toplam": "sum",
        "mean": "mean", "ortalama": "mean",
        "count": "count", "sayÄ±": "count", "adet": "count",
        "min": "min", "minimum": "min", "en dÃ¼ÅŸÃ¼k": "min",
        "max": "max", "maximum": "max", "en yÃ¼ksek": "max",
        "std": "std", "standart sapma": "std",
        "median": "median", "medyan": "median",
    }
    
    func = agg_map.get(aggfunc.lower(), "sum")
    
    try:
        if rows and values:
            pivot = pd.pivot_table(
                df,
                index=rows,
                columns=columns,
                values=values,
                aggfunc=func,
                fill_value=fill_value,
                margins=True,
                margins_name="TOPLAM"
            )
        elif rows:
            # Sadece gruplama
            pivot = df.groupby(rows).agg(func, numeric_only=True)
            pivot.loc["TOPLAM"] = pivot.sum(numeric_only=True)
        else:
            # TÃ¼m sayÄ±sal sÃ¼tunlarÄ±n Ã¶zeti
            pivot = df.describe()
        
        return {
            "success": True,
            "table": pivot.to_dict(),
            "table_str": pivot.to_string(),
            "table_markdown": pivot.to_markdown() if hasattr(pivot, 'to_markdown') else pivot.to_string(),
            "shape": list(pivot.shape),
        }
        
    except Exception as e:
        return {"success": False, "error": str(e)}


def smart_pivot(df: pd.DataFrame, question: str = None) -> dict:
    """
    Soruya gÃ¶re otomatik pivot oluÅŸtur.
    Soru verilmezse en mantÄ±klÄ± pivot'u otomatik belirle.
    """
    discovery = discover_data(df)
    
    cat_cols = discovery["categorical_columns"]
    num_cols = discovery["numeric_columns"]
    
    if not num_cols:
        return {"success": False, "error": "SayÄ±sal sÃ¼tun bulunamadÄ±, pivot oluÅŸturulamaz"}
    
    # Otomatik seÃ§im
    if cat_cols and num_cols:
        best_row = cat_cols[0]
        best_value = num_cols[:3]  # Ä°lk 3 sayÄ±sal sÃ¼tun
        
        # EÄŸer 2+ kategorik varsa, ikincisini sÃ¼tun olarak kullan
        best_col = cat_cols[1] if len(cat_cols) > 1 else None
        
        return create_pivot(
            df,
            rows=[best_row],
            columns=[best_col] if best_col else None,
            values=best_value,
            aggfunc="sum"
        )
    
    # Sadece sayÄ±sal varsa, describe (istatistiksel Ã¶zet)
    return create_pivot(df)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. Ä°STATÄ°STÄ°KSEL ANALÄ°Z
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def statistical_analysis(df: pd.DataFrame) -> dict:
    """KapsamlÄ± istatistiksel analiz"""
    result = {
        "basic_stats": {},
        "correlations": None,
        "distributions": {},
        "outliers": {},
        "trends": {},
    }
    
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # Temel istatistikler
    if num_cols:
        desc = df[num_cols].describe()
        result["basic_stats"] = desc.to_dict()
        
        # Korelasyon matrisi
        if len(num_cols) > 1:
            corr = df[num_cols].corr()
            result["correlations"] = corr.to_dict()
            
            # GÃ¼Ã§lÃ¼ korelasyonlar
            strong_corrs = []
            for i in range(len(corr.columns)):
                for j in range(i+1, len(corr.columns)):
                    val = corr.iloc[i, j]
                    if abs(val) > 0.5:
                        strong_corrs.append({
                            "col1": corr.columns[i],
                            "col2": corr.columns[j],
                            "correlation": round(val, 3),
                            "strength": "GÃ¼Ã§lÃ¼" if abs(val) > 0.7 else "Orta",
                            "direction": "Pozitif" if val > 0 else "Negatif",
                        })
            result["strong_correlations"] = strong_corrs
        
        # AykÄ±rÄ± deÄŸer tespiti (IQR yÃ¶ntemi)
        for col in num_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - 1.5 * IQR
            upper = Q3 + 1.5 * IQR
            outliers = df[(df[col] < lower) | (df[col] > upper)][col]
            if len(outliers) > 0:
                result["outliers"][col] = {
                    "count": len(outliers),
                    "percentage": round(len(outliers) / len(df) * 100, 1),
                    "lower_bound": round(lower, 2),
                    "upper_bound": round(upper, 2),
                    "min_outlier": round(float(outliers.min()), 2),
                    "max_outlier": round(float(outliers.max()), 2),
                }
        
        # DaÄŸÄ±lÄ±m bilgisi
        for col in num_cols:
            try:
                skew = float(df[col].skew())
                kurt = float(df[col].kurtosis())
                result["distributions"][col] = {
                    "skewness": round(skew, 3),
                    "kurtosis": round(kurt, 3),
                    "distribution_type": (
                        "Normal daÄŸÄ±lÄ±m" if abs(skew) < 0.5 and abs(kurt) < 1
                        else "SaÄŸa Ã§arpÄ±k" if skew > 0.5
                        else "Sola Ã§arpÄ±k" if skew < -0.5
                        else "Sivri" if kurt > 1
                        else "BasÄ±k"
                    ),
                }
            except Exception:
                pass
    
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. TREND ANALÄ°ZÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def trend_analysis(df: pd.DataFrame, date_col: str = None, value_col: str = None) -> dict:
    """Zaman serisi trend analizi"""
    
    # Tarih sÃ¼tununu otomatik bul
    if not date_col:
        for col in df.columns:
            try:
                pd.to_datetime(df[col])
                date_col = col
                break
            except Exception:
                continue
    
    if not date_col:
        return {"success": False, "error": "Tarih sÃ¼tunu bulunamadÄ±"}
    
    # DeÄŸer sÃ¼tununu otomatik bul
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if not value_col:
        value_col = num_cols[0] if num_cols else None
    
    if not value_col:
        return {"success": False, "error": "SayÄ±sal deÄŸer sÃ¼tunu bulunamadÄ±"}
    
    try:
        df_sorted = df.copy()
        df_sorted[date_col] = pd.to_datetime(df_sorted[date_col])
        df_sorted = df_sorted.sort_values(date_col)
        
        trends = {}
        for col in ([value_col] if isinstance(value_col, str) else num_cols[:5]):
            if col not in df_sorted.columns or not pd.api.types.is_numeric_dtype(df_sorted[col]):
                continue
            
            vals = df_sorted[col].dropna()
            if len(vals) < 3:
                continue
            
            first_half = vals[:len(vals)//2].mean()
            second_half = vals[len(vals)//2:].mean()
            
            change_pct = ((second_half - first_half) / first_half * 100) if first_half != 0 else 0
            
            # Basit regresyon eÄŸimi
            x = np.arange(len(vals))
            slope = np.polyfit(x, vals.values, 1)[0] if len(vals) > 1 else 0
            
            trends[col] = {
                "direction": "ArtÄ±ÅŸ" if change_pct > 5 else "Azalma" if change_pct < -5 else "Stabil",
                "change_pct": round(change_pct, 1),
                "first_half_avg": round(float(first_half), 2),
                "second_half_avg": round(float(second_half), 2),
                "slope": round(float(slope), 4),
                "min_value": round(float(vals.min()), 2),
                "max_value": round(float(vals.max()), 2),
                "latest_value": round(float(vals.iloc[-1]), 2),
            }
        
        return {
            "success": True,
            "date_column": date_col,
            "date_range": f"{df_sorted[date_col].min()} - {df_sorted[date_col].max()}",
            "data_points": len(df_sorted),
            "trends": trends,
        }
    
    except Exception as e:
        return {"success": False, "error": str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. TOP-N / SIRALAMA ANALÄ°ZÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def top_n_analysis(df: pd.DataFrame, n: int = 10) -> dict:
    """Her sayÄ±sal sÃ¼tun iÃ§in top-N ve bottom-N"""
    results = {}
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = [c for c in df.columns if c not in num_cols]
    
    label_col = cat_cols[0] if cat_cols else None
    
    for col in num_cols[:5]:  # En fazla 5 sÃ¼tun
        sorted_df = df.nlargest(n, col)
        if label_col:
            top = sorted_df[[label_col, col]].to_dict('records')
        else:
            top = sorted_df[[col]].to_dict('records')
        
        sorted_df_bottom = df.nsmallest(n, col)
        if label_col:
            bottom = sorted_df_bottom[[label_col, col]].to_dict('records')
        else:
            bottom = sorted_df_bottom[[col]].to_dict('records')
        
        results[col] = {"top": top, "bottom": bottom}
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. KARÅžILAÅžTIRMA ANALÄ°ZÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def comparison_analysis(df: pd.DataFrame, group_col: str = None) -> dict:
    """Kategorik gruplara gÃ¶re karÅŸÄ±laÅŸtÄ±rma"""
    
    if not group_col:
        # Otomatik kategorik sÃ¼tun seÃ§
        cat_cols = [
            c for c in df.columns 
            if df[c].dtype == 'object' and df[c].nunique() <= 20
        ]
        if not cat_cols:
            return {"success": False, "error": "Gruplama iÃ§in uygun kategorik sÃ¼tun bulunamadÄ±"}
        group_col = cat_cols[0]
    
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if not num_cols:
        return {"success": False, "error": "SayÄ±sal sÃ¼tun bulunamadÄ±"}
    
    grouped = df.groupby(group_col)[num_cols].agg(['mean', 'sum', 'count', 'min', 'max'])
    
    result = {
        "success": True,
        "group_column": group_col,
        "groups": list(df[group_col].unique()),
        "group_count": df[group_col].nunique(),
        "summary": {},
    }
    
    for col in num_cols[:5]:
        group_means = df.groupby(group_col)[col].mean().sort_values(ascending=False)
        group_sums = df.groupby(group_col)[col].sum().sort_values(ascending=False)
        group_counts = df.groupby(group_col)[col].count()
        
        result["summary"][col] = {
            "best_group": str(group_means.index[0]) if len(group_means) > 0 else None,
            "worst_group": str(group_means.index[-1]) if len(group_means) > 0 else None,
            "means": {str(k): round(v, 2) for k, v in group_means.items()},
            "sums": {str(k): round(v, 2) for k, v in group_sums.items()},
            "counts": {str(k): int(v) for k, v in group_counts.items()},
        }
    
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7. RAPOR + YORUM + TAVSÄ°YE OLUÅžTURMA (LLM Ä°Ã‡Ä°N PROMPT)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_analysis_prompt(
    df: pd.DataFrame,
    analysis_type: str = "full",
    question: str = None,
    filename: str = None,
) -> str:
    """
    LLM'e gÃ¶nderilecek detaylÄ± analiz prompt'u oluÅŸtur.
    
    analysis_type:
        - "full": Tam analiz (keÅŸif + istatistik + pivot + trend + tavsiye)
        - "pivot": Sadece pivot tablo
        - "trend": Trend analizi
        - "compare": KarÅŸÄ±laÅŸtÄ±rma
        - "summary": HÄ±zlÄ± Ã¶zet
        - "recommend": Tavsiye odaklÄ±
        - "report": Resmi rapor formatÄ±nda
    """
    
    discovery = discover_data(df)
    
    # Temel veri bilgisi
    prompt = f"""## ðŸ“Š DokÃ¼man Analizi: {filename or 'YÃ¼klenen Veri'}

### Veri Ã–zeti:
- **SatÄ±r sayÄ±sÄ±**: {discovery['row_count']}
- **SÃ¼tun sayÄ±sÄ±**: {discovery['col_count']}
- **SayÄ±sal sÃ¼tunlar**: {', '.join(discovery['numeric_columns']) or 'Yok'}
- **Kategorik sÃ¼tunlar**: {', '.join(discovery['categorical_columns']) or 'Yok'}
- **Tarih sÃ¼tunlarÄ±**: {', '.join(discovery['date_columns']) or 'Yok'}
"""
    
    # SÃ¼tun detaylarÄ±
    prompt += "\n### SÃ¼tun Bilgileri:\n"
    for col_info in discovery["columns"]:
        line = f"- **{col_info['name']}** ({col_info['type']}): "
        if col_info["type"] == "numeric":
            line += f"Min={col_info.get('min')}, Max={col_info.get('max')}, Ort={col_info.get('mean')}, Toplam={col_info.get('sum')}"
        elif col_info["type"] == "categorical":
            top_vals = col_info.get("top_values", {})
            top_3 = list(top_vals.items())[:3]
            line += f"{col_info['unique_count']} benzersiz deÄŸer. En sÄ±k: {', '.join(f'{k}({v})' for k, v in top_3)}"
        elif col_info["type"] == "date":
            line += f"AralÄ±k: {col_info.get('min')} â†’ {col_info.get('max')}"
        else:
            line += f"Ort uzunluk: {col_info.get('avg_length', 'N/A')} karakter"
        
        if col_info["null_count"] > 0:
            line += f" [âš ï¸ %{col_info['null_pct']} eksik]"
        prompt += line + "\n"
    
    # Ä°statistiksel Analiz
    stats = statistical_analysis(df)
    
    if stats.get("strong_correlations"):
        prompt += "\n### Korelasyonlar (GÃ¼Ã§lÃ¼ Ä°liÅŸkiler):\n"
        for corr in stats["strong_correlations"]:
            prompt += f"- **{corr['col1']}** â†” **{corr['col2']}**: {corr['correlation']} ({corr['strength']} {corr['direction']})\n"
    
    if stats.get("outliers"):
        prompt += "\n### AykÄ±rÄ± DeÄŸerler:\n"
        for col, info in stats["outliers"].items():
            prompt += f"- **{col}**: {info['count']} aykÄ±rÄ± deÄŸer (%{info['percentage']}), normal aralÄ±k: {info['lower_bound']} - {info['upper_bound']}\n"
    
    # Pivot Tablo
    if analysis_type in ("full", "pivot") and discovery["categorical_columns"] and discovery["numeric_columns"]:
        pivot_result = smart_pivot(df)
        if pivot_result.get("success"):
            prompt += f"\n### Pivot Tablo:\n```\n{pivot_result['table_str'][:2000]}\n```\n"
    
    # Trend Analizi
    if analysis_type in ("full", "trend") and discovery["date_columns"]:
        trend = trend_analysis(df)
        if trend.get("success"):
            prompt += f"\n### Trend Analizi ({trend['date_range']}):\n"
            for col, t_info in trend.get("trends", {}).items():
                prompt += f"- **{col}**: {t_info['direction']} (%{t_info['change_pct']}), Son deÄŸer: {t_info['latest_value']}\n"
    
    # KarÅŸÄ±laÅŸtÄ±rma
    if analysis_type in ("full", "compare") and discovery["categorical_columns"]:
        comp = comparison_analysis(df)
        if comp.get("success"):
            prompt += f"\n### Grup KarÅŸÄ±laÅŸtÄ±rmasÄ± ({comp['group_column']}):\n"
            for col, cinfo in comp.get("summary", {}).items():
                prompt += f"- **{col}**: En iyi={cinfo['best_group']}, En dÃ¼ÅŸÃ¼k={cinfo['worst_group']}\n"
    
    # Top-N
    if analysis_type in ("full", "report") and discovery["numeric_columns"]:
        top_n = top_n_analysis(df, n=5)
        if top_n:
            prompt += "\n### En YÃ¼ksek / En DÃ¼ÅŸÃ¼k DeÄŸerler:\n"
            for col, data in list(top_n.items())[:3]:
                prompt += f"**{col} â€” Top 5:**\n"
                for item in data["top"][:5]:
                    vals = [f"{k}: {v}" for k, v in item.items()]
                    prompt += f"  - {', '.join(vals)}\n"
    
    # Veri Ã¶rneÄŸi
    sample_rows = min(5, len(df))
    prompt += f"\n### Veri Ã–rneÄŸi (Ä°lk {sample_rows} SatÄ±r):\n"
    prompt += f"```\n{df.head(sample_rows).to_string()}\n```\n"
    
    # Analiz talimatÄ±
    if analysis_type == "pivot":
        prompt += "\n**GÃ–REV**: YukarÄ±daki verilere gÃ¶re detaylÄ± pivot tablo analizi yap. Hangi kategorilerin Ã¶ne Ã§Ä±ktÄ±ÄŸÄ±nÄ±, karÅŸÄ±laÅŸtÄ±rmalarÄ± ve Ã¶nemli bulgularÄ± raporla."
    elif analysis_type == "trend":
        prompt += "\n**GÃ–REV**: Trend analizini yorumla. ArtÄ±ÅŸ/azalma nedenlerini, mevsimsel etkileri ve gelecek projeksiyonlarÄ±nÄ± belirt."
    elif analysis_type == "compare":
        prompt += "\n**GÃ–REV**: GruplarÄ± detaylÄ± karÅŸÄ±laÅŸtÄ±r. En iyi/en kÃ¶tÃ¼ performans gÃ¶sterenleri belirle ve nedenleri hakkÄ±nda yorum yap."
    elif analysis_type == "recommend":
        prompt += "\n**GÃ–REV**: Bu verilere dayanarak somut, uygulanabilir TAVSÄ°YELER sun. Her tavsiyeyi verilerle destekle. Risk analizi de yap."
    elif analysis_type == "report":
        prompt += "\n**GÃ–REV**: Bu verilerle profesyonel bir RAPOR oluÅŸtur. YÃ¶netici Ã–zeti, Bulgular, DetaylÄ± Analiz, Riskler, Ã–neriler bÃ¶lÃ¼mlerini iÃ§ersin."
    elif analysis_type == "summary":
        prompt += "\n**GÃ–REV**: Bu veriyi 5-6 cÃ¼mlede Ã¶zetle. En Ã§arpÄ±cÄ± bulgularÄ± vurgula."
    else:  # full
        prompt += """
**GÃ–REV**: Bu veri setini kapsamlÄ± analiz et ve aÅŸaÄŸÄ±daki baÅŸlÄ±klarda yanÄ±t ver:

1. **ðŸ“‹ Veri Ã–zeti**: Veri setinin genel yapÄ±sÄ±nÄ± ve kalitesini deÄŸerlendir
2. **ðŸ“Š Temel Bulgular**: En Ã¶nemli sayÄ±sal bulgular (en yÃ¼ksek, en dÃ¼ÅŸÃ¼k, ortalamalar)
3. **ðŸ“ˆ Trend & DeÄŸiÅŸim**: Zaman bazlÄ± veya kategorik deÄŸiÅŸimler
4. **ðŸ” Dikkat Ã‡ekici Noktalar**: AykÄ±rÄ± deÄŸerler, beklenmeyen paternler, eksik veriler
5. **ðŸ’¡ Yorumlar**: Verilerin ne anlama geldiÄŸi hakkÄ±nda profesyonel yorumlar
6. **âœ… Tavsiyeler**: Somut, uygulanabilir Ã¶neriler (en az 3-5 madde)
7. **âš ï¸ Riskler**: Dikkat edilmesi gereken riskler ve uyarÄ±lar
"""
    
    # KullanÄ±cÄ± sorusu varsa ekle
    if question:
        prompt += f"\n**KullanÄ±cÄ±nÄ±n sorusu/talebi**: {question}\nBu soruyu da mutlaka cevapla.\n"
    
    return prompt


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 8. METÄ°N TABANLI DÃ–KÃœMAN ANALÄ°ZÄ° (PDF/DOCX/TXT)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_text_analysis_prompt(
    text: str,
    analysis_type: str = "full",
    question: str = None,
    filename: str = None,
) -> str:
    """
    Metin tabanlÄ± dokÃ¼manlar iÃ§in analiz prompt'u.
    PDF, DOCX, TXT gibi yapÄ±landÄ±rÄ±lmamÄ±ÅŸ veriler iÃ§in.
    """
    # Metin istatistikleri
    word_count = len(text.split())
    line_count = len(text.split('\n'))
    char_count = len(text)
    
    # Anahtar kelimeler (en sÄ±k geÃ§en kelimeler)
    words = re.findall(r'\b[a-zA-ZÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄžIÄ°Ã–ÅžÃœ]{4,}\b', text.lower())
    word_freq = {}
    stop_words = {'iÃ§in', 'olan', 'olarak', 'veya', 'gibi', 'kadar', 'daha', 'ancak', 'fakat', 'bile'}
    for w in words:
        if w not in stop_words:
            word_freq[w] = word_freq.get(w, 0) + 1
    
    top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:15]
    
    # SayÄ±sal deÄŸerler
    numbers = re.findall(r'\b\d+[.,]?\d*\b', text)
    
    # Metnin kÄ±saltÄ±lmÄ±ÅŸ hali (Ã§ok uzunsa)
    max_text = 8000
    display_text = text[:max_text] + f"\n\n[... {char_count - max_text} karakter daha ...]" if len(text) > max_text else text
    
    prompt = f"""## ðŸ“„ DokÃ¼man Analizi: {filename or 'YÃ¼klenen DokÃ¼man'}

### DokÃ¼man Bilgileri:
- **Kelime sayÄ±sÄ±**: {word_count:,}
- **SatÄ±r sayÄ±sÄ±**: {line_count:,}
- **Karakter sayÄ±sÄ±**: {char_count:,}
- **Ä°Ã§erdiÄŸi sayÄ±sal deÄŸerler**: {len(numbers)} adet
- **Anahtar kelimeler**: {', '.join(f'{w}({c})' for w, c in top_keywords[:10])}

### DokÃ¼man Ä°Ã§eriÄŸi:
```
{display_text}
```

"""
    
    if analysis_type == "summary":
        prompt += "**GÃ–REV**: Bu dokÃ¼manÄ± 5-10 cÃ¼mlede Ã¶zetle. Ana konularÄ± ve en Ã¶nemli bilgileri vurgula."
    elif analysis_type == "recommend":
        prompt += "**GÃ–REV**: Bu dokÃ¼mandaki bilgilere dayanarak somut tavsiyeler sun. Her tavsiyeyi dokÃ¼mandaki verilerle destekle."
    elif analysis_type == "report":
        prompt += """**GÃ–REV**: Bu dokÃ¼man hakkÄ±nda kapsamlÄ± bir rapor oluÅŸtur:
1. YÃ¶netici Ã–zeti
2. Ana Bulgular
3. DetaylÄ± DeÄŸerlendirme
4. Ã–neriler ve Aksiyon Maddeleri
5. Riskler ve UyarÄ±lar"""
    else:
        prompt += """**GÃ–REV**: Bu dokÃ¼manÄ± detaylÄ± analiz et:
1. **ðŸ“‹ Ã–zet**: DokÃ¼manÄ±n ana konusu ve amacÄ±
2. **ðŸ” Temel Bulgular**: Ä°Ã§indeki en Ã¶nemli bilgiler
3. **ðŸ’¡ Yorumlar**: Profesyonel deÄŸerlendirme
4. **âœ… Tavsiyeler**: Somut Ã¶neriler
5. **âš ï¸ Dikkat Edilecekler**: Riskler ve uyarÄ±lar
"""
    
    if question:
        prompt += f"\n**KullanÄ±cÄ±nÄ±n sorusu/talebi**: {question}\nBu soruyu da mutlaka cevapla.\n"
    
    return prompt


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 9. DOÄžAL DÄ°L Ä°LE VERÄ° SORGULAMA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def natural_language_query(df: pd.DataFrame, question: str) -> dict:
    """
    DoÄŸal dil sorusunu pandas iÅŸlemine Ã§evir.
    Basit sorgularÄ± otomatik Ã§alÄ±ÅŸtÄ±r.
    """
    q = question.lower()
    result = {"success": False, "answer": None, "query_type": None}
    
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    try:
        # Toplam/Sum
        if re.search(r'toplam|sum|genel\s*toplam', q):
            for col in num_cols:
                if col.lower() in q:
                    result = {
                        "success": True,
                        "answer": f"{col} toplamÄ±: {df[col].sum():,.2f}",
                        "value": float(df[col].sum()),
                        "query_type": "sum"
                    }
                    return result
            # TÃ¼m toplamlar
            sums = {col: round(float(df[col].sum()), 2) for col in num_cols}
            result = {"success": True, "answer": str(sums), "value": sums, "query_type": "sum_all"}
            return result
        
        # Ortalama
        if re.search(r'ortalama|mean|average', q):
            for col in num_cols:
                if col.lower() in q:
                    result = {
                        "success": True,
                        "answer": f"{col} ortalamasÄ±: {df[col].mean():,.2f}",
                        "value": float(df[col].mean()),
                        "query_type": "mean"
                    }
                    return result
            means = {col: round(float(df[col].mean()), 2) for col in num_cols}
            result = {"success": True, "answer": str(means), "value": means, "query_type": "mean_all"}
            return result
        
        # En yÃ¼ksek/max
        if re.search(r'en (yÃ¼ksek|fazla|bÃ¼yÃ¼k|Ã§ok)|max|maksimum', q):
            for col in num_cols:
                if col.lower() in q:
                    idx = df[col].idxmax()
                    row = df.loc[idx]
                    result = {
                        "success": True,
                        "answer": f"{col} en yÃ¼ksek: {row[col]:,.2f}\nSatÄ±r: {row.to_dict()}",
                        "value": float(row[col]),
                        "row": row.to_dict(),
                        "query_type": "max"
                    }
                    return result
        
        # En dÃ¼ÅŸÃ¼k/min
        if re.search(r'en (dÃ¼ÅŸÃ¼k|az|kÃ¼Ã§Ã¼k)|min|minimum', q):
            for col in num_cols:
                if col.lower() in q:
                    idx = df[col].idxmin()
                    row = df.loc[idx]
                    result = {
                        "success": True,
                        "answer": f"{col} en dÃ¼ÅŸÃ¼k: {row[col]:,.2f}\nSatÄ±r: {row.to_dict()}",
                        "value": float(row[col]),
                        "row": row.to_dict(),
                        "query_type": "min"
                    }
                    return result
        
        # SatÄ±r sayÄ±sÄ±
        if re.search(r'kaÃ§\s*(tane|adet|satÄ±r)|satÄ±r\s*sayÄ±sÄ±|count', q):
            result = {
                "success": True,
                "answer": f"Toplam {len(df)} satÄ±r var.",
                "value": len(df),
                "query_type": "count"
            }
            return result
        
        # Filtre (belirli bir deÄŸer arama)
        for col in df.columns:
            col_lower = col.lower()
            if col_lower in q:
                # "X olan satÄ±rlar" gibi
                for val in df[col].unique():
                    val_str = str(val).lower()
                    if val_str in q and len(val_str) > 2:
                        filtered = df[df[col] == val]
                        result = {
                            "success": True,
                            "answer": f"{col}={val} olan {len(filtered)} satÄ±r bulundu.\n{filtered.to_string()[:1000]}",
                            "value": len(filtered),
                            "query_type": "filter"
                        }
                        return result
        
    except Exception as e:
        result["error"] = str(e)
    
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 10. ANALÄ°Z SONUÃ‡LARINI FORMATLA (JSON â†’ LLM-Ready)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def format_analysis_for_llm(
    df: pd.DataFrame = None,
    text: str = None,
    analysis_type: str = "full",
    question: str = None,
    filename: str = None,
) -> str:
    """
    Dosya tipine gÃ¶re uygun analiz prompt'u dÃ¶ndÃ¼r.
    DataFrame varsa tablolu analiz, yoksa metin analizi.
    """
    if df is not None and not df.empty:
        return generate_analysis_prompt(df, analysis_type, question, filename)
    elif text:
        return generate_text_analysis_prompt(text, analysis_type, question, filename)
    else:
        return "Analiz edilecek veri bulunamadÄ±."
