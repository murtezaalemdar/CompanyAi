"""
GeliÅŸmiÅŸ DokÃ¼man Analiz Motoru (Document Intelligence)

YÃ¼klenen dokÃ¼manlardan:
- Pivot tablo oluÅŸturma
- Ä°statistiksel analiz
- Trend/karÅŸÄ±laÅŸtÄ±rma raporu
- Yorum ve tavsiyeler
- Otomatik veri keÅŸfi
- DoÄŸal dil ile veri sorgulama

Desteklenen girdiler:
- Excel (.xlsx, .xls) â†’ Tam tablolu analiz
- CSV (.csv) â†’ Tablolu analiz
- JSON (.json) â†’ YapÄ±sal analiz
- PDF/DOCX/TXT â†’ Metin tabanlÄ± analiz
- RAG'daki mevcut dokÃ¼manlar â†’ Semantik analiz
"""

import io
import json
import re
from typing import Optional, Any
from datetime import datetime

import structlog
import pandas as pd
import numpy as np

logger = structlog.get_logger()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. VERÄ° PARSE & KEÅÄ°F
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_file_to_dataframe(filename: str, file_content: bytes) -> Optional[pd.DataFrame]:
    """
    DosyayÄ± pandas DataFrame'e Ã§evir.
    Excel, CSV, JSON ve TSV destekler.
    """
    filename_lower = filename.lower()
    
    try:
        if filename_lower.endswith(('.xlsx', '.xls')):
            # Excel â€” tÃ¼m sayfalarÄ± oku, en bÃ¼yÃ¼k olanÄ± kullan
            xls = pd.ExcelFile(io.BytesIO(file_content))
            sheets = {}
            for sheet_name in xls.sheet_names:
                df = pd.read_excel(xls, sheet_name=sheet_name)
                sheets[sheet_name] = df
            
            if not sheets:
                return None
            
            # En Ã§ok satÄ±rÄ± olan sayfayÄ± dÃ¶ndÃ¼r
            main_sheet = max(sheets.values(), key=lambda x: len(x))
            
            # TÃ¼m sayfalarÄ± metadata olarak sakla (sadece istatistik, DataFrame referansÄ± KOYMUYORUZ)
            main_sheet.attrs['_all_sheets'] = {
                name: {"rows": len(df), "cols": len(df.columns)} 
                for name, df in sheets.items()
            }
            # NOT: _sheets_data attrs'a konmaz â€” pandas deepcopy recursion bug'Ä±na yol aÃ§ar
            
            return main_sheet
        
        elif filename_lower.endswith('.csv'):
            # CSV â€” farklÄ± delimiter'larÄ± dene
            for sep in [',', ';', '\t', '|']:
                try:
                    df = pd.read_csv(io.BytesIO(file_content), sep=sep, encoding='utf-8')
                    if len(df.columns) > 1:
                        return df
                except Exception:
                    continue
            # Son deneme: otomatik
            return pd.read_csv(io.BytesIO(file_content), encoding='utf-8')
        
        elif filename_lower.endswith('.json'):
            text = file_content.decode('utf-8')
            data = json.loads(text)
            
            if isinstance(data, list):
                return pd.DataFrame(data)
            elif isinstance(data, dict):
                # Ä°Ã§ iÃ§e dict'i dÃ¼zleÅŸtirmeye Ã§alÄ±ÅŸ
                if any(isinstance(v, list) for v in data.values()):
                    for key, val in data.items():
                        if isinstance(val, list) and val:
                            return pd.DataFrame(val)
                return pd.DataFrame([data])
            
        elif filename_lower.endswith('.tsv'):
            return pd.read_csv(io.BytesIO(file_content), sep='\t', encoding='utf-8')
            
    except Exception as e:
        logger.warning("parse_to_df_failed", file=filename, error=str(e))
    
    return None


def discover_data(df: pd.DataFrame) -> dict:
    """
    DataFrame'i otomatik keÅŸfet â€” sÃ¼tun tipleri, istatistikler, iliÅŸkiler.
    """
    # Ã–NEMLÄ° BUG FIX (commit 6a1d0b6): pandas 2.3.x deepcopy recursion bug
    # pandas 2.3.x'te DataFrame.__finalize__() deepcopy(other.attrs) Ã§aÄŸÄ±rÄ±yor.
    # EÄŸer attrs iÃ§inde baÅŸka DataFrame nesneleri varsa (Ã¶r: _sheets_data)
    # sonsuz dÃ¶ngÃ¼ye girer â†’ RecursionError. Bu satÄ±r attrs'u TEMÄ°ZLER.
    # parse_file_to_dataframe() sheets bilgisini artÄ±k attrs'a koymaz.
    df.attrs = {}
    
    info = {
        "row_count": len(df),
        "col_count": len(df.columns),
        "columns": [],
        "numeric_columns": [],
        "categorical_columns": [],
        "date_columns": [],
        "text_columns": [],
        "has_missing": False,
        "missing_summary": {},
        "memory_mb": round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2),
    }
    
    for col in df.columns:
        col_info = {
            "name": col,
            "dtype": str(df[col].dtype),
            "non_null": int(df[col].notna().sum()),
            "null_count": int(df[col].isna().sum()),
            "null_pct": round(df[col].isna().mean() * 100, 1),
            "unique_count": int(df[col].nunique()),
        }
        
        if df[col].isna().any():
            info["has_missing"] = True
            info["missing_summary"][col] = col_info["null_count"]
        
        # Tarih tespiti
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            info["date_columns"].append(col)
            col_info["type"] = "date"
            col_info["min"] = str(df[col].min())
            col_info["max"] = str(df[col].max())
        
        # SayÄ±sal tespiti
        elif pd.api.types.is_numeric_dtype(df[col]):
            info["numeric_columns"].append(col)
            col_info["type"] = "numeric"
            col_info["min"] = float(df[col].min()) if df[col].notna().any() else None
            col_info["max"] = float(df[col].max()) if df[col].notna().any() else None
            col_info["mean"] = round(float(df[col].mean()), 2) if df[col].notna().any() else None
            col_info["median"] = round(float(df[col].median()), 2) if df[col].notna().any() else None
            col_info["std"] = round(float(df[col].std()), 2) if df[col].notna().any() else None
            col_info["sum"] = float(df[col].sum()) if df[col].notna().any() else None
        
        # Kategorik tespiti
        elif df[col].nunique() <= max(20, len(df) * 0.05):
            info["categorical_columns"].append(col)
            col_info["type"] = "categorical"
            col_info["top_values"] = df[col].value_counts().head(10).to_dict()
        
        # Metin
        else:
            info["text_columns"].append(col)
            col_info["type"] = "text"
            col_info["avg_length"] = round(df[col].astype(str).str.len().mean(), 0)
        
        # Tarih string tespiti (sÃ¼tun string ama tarih gibi gÃ¶rÃ¼nÃ¼yor)
        if col_info.get("type") not in ("date", "numeric") and df[col].dtype == "object":
            sample = df[col].dropna().head(20).astype(str)
            date_patterns = [
                r'\d{4}[-/]\d{1,2}[-/]\d{1,2}',
                r'\d{1,2}[-/.]\d{1,2}[-/.]\d{4}',
            ]
            date_matches = sum(
                1 for s in sample 
                if any(re.search(p, s) for p in date_patterns)
            )
            if date_matches > len(sample) * 0.5:
                col_info["possible_date"] = True
                info["date_columns"].append(col)
        
        info["columns"].append(col_info)
    
    return info


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. PÄ°VOT TABLO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_pivot(
    df: pd.DataFrame,
    rows: list[str] = None,
    columns: list[str] = None,
    values: list[str] = None,
    aggfunc: str = "sum",
    fill_value: Any = 0,
) -> dict:
    """
    Pivot tablo oluÅŸtur.
    
    Parametreler:
        rows: SatÄ±r bazÄ±nda gruplama sÃ¼tunlarÄ±
        columns: SÃ¼tun bazÄ±nda gruplama
        values: Hesaplanacak deÄŸer sÃ¼tunlarÄ±
        aggfunc: sum, mean, count, min, max, std
        fill_value: BoÅŸ hÃ¼cre deÄŸeri
    """
    agg_map = {
        "sum": "sum", "toplam": "sum",
        "mean": "mean", "ortalama": "mean",
        "count": "count", "sayÄ±": "count", "adet": "count",
        "min": "min", "minimum": "min", "en dÃ¼ÅŸÃ¼k": "min",
        "max": "max", "maximum": "max", "en yÃ¼ksek": "max",
        "std": "std", "standart sapma": "std",
        "median": "median", "medyan": "median",
    }
    
    func = agg_map.get(aggfunc.lower(), "sum")
    
    try:
        if rows and values:
            pivot = pd.pivot_table(
                df,
                index=rows,
                columns=columns,
                values=values,
                aggfunc=func,
                fill_value=fill_value,
                margins=True,
                margins_name="TOPLAM"
            )
        elif rows:
            # Sadece gruplama
            pivot = df.groupby(rows).agg(func, numeric_only=True)
            pivot.loc["TOPLAM"] = pivot.sum(numeric_only=True)
        else:
            # TÃ¼m sayÄ±sal sÃ¼tunlarÄ±n Ã¶zeti
            pivot = df.describe()
        
        return {
            "success": True,
            "table": pivot.to_dict(),
            "table_str": pivot.to_string(),
            "table_markdown": pivot.to_markdown() if hasattr(pivot, 'to_markdown') else pivot.to_string(),
            "shape": list(pivot.shape),
        }
        
    except Exception as e:
        return {"success": False, "error": str(e)}


def smart_pivot(df: pd.DataFrame, question: str = None) -> dict:
    """
    Soruya gÃ¶re otomatik pivot oluÅŸtur.
    Soru verilmezse en mantÄ±klÄ± pivot'u otomatik belirle.
    """
    discovery = discover_data(df)
    
    cat_cols = discovery["categorical_columns"]
    num_cols = discovery["numeric_columns"]
    
    if not num_cols:
        return {"success": False, "error": "SayÄ±sal sÃ¼tun bulunamadÄ±, pivot oluÅŸturulamaz"}
    
    # Otomatik seÃ§im
    if cat_cols and num_cols:
        best_row = cat_cols[0]
        best_value = num_cols[:3]  # Ä°lk 3 sayÄ±sal sÃ¼tun
        
        # EÄŸer 2+ kategorik varsa, ikincisini sÃ¼tun olarak kullan
        best_col = cat_cols[1] if len(cat_cols) > 1 else None
        
        return create_pivot(
            df,
            rows=[best_row],
            columns=[best_col] if best_col else None,
            values=best_value,
            aggfunc="sum"
        )
    
    # Sadece sayÄ±sal varsa, describe (istatistiksel Ã¶zet)
    return create_pivot(df)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. Ä°STATÄ°STÄ°KSEL ANALÄ°Z
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def statistical_analysis(df: pd.DataFrame) -> dict:
    """KapsamlÄ± istatistiksel analiz"""
    result = {
        "basic_stats": {},
        "correlations": None,
        "distributions": {},
        "outliers": {},
        "trends": {},
    }
    
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # Temel istatistikler
    if num_cols:
        desc = df[num_cols].describe()
        result["basic_stats"] = desc.to_dict()
        
        # Korelasyon matrisi
        if len(num_cols) > 1:
            corr = df[num_cols].corr()
            result["correlations"] = corr.to_dict()
            
            # GÃ¼Ã§lÃ¼ korelasyonlar
            strong_corrs = []
            for i in range(len(corr.columns)):
                for j in range(i+1, len(corr.columns)):
                    val = corr.iloc[i, j]
                    if abs(val) > 0.5:
                        strong_corrs.append({
                            "col1": corr.columns[i],
                            "col2": corr.columns[j],
                            "correlation": round(val, 3),
                            "strength": "GÃ¼Ã§lÃ¼" if abs(val) > 0.7 else "Orta",
                            "direction": "Pozitif" if val > 0 else "Negatif",
                        })
            result["strong_correlations"] = strong_corrs
        
        # AykÄ±rÄ± deÄŸer tespiti (IQR yÃ¶ntemi)
        for col in num_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - 1.5 * IQR
            upper = Q3 + 1.5 * IQR
            outliers = df[(df[col] < lower) | (df[col] > upper)][col]
            if len(outliers) > 0:
                result["outliers"][col] = {
                    "count": len(outliers),
                    "percentage": round(len(outliers) / len(df) * 100, 1),
                    "lower_bound": round(lower, 2),
                    "upper_bound": round(upper, 2),
                    "min_outlier": round(float(outliers.min()), 2),
                    "max_outlier": round(float(outliers.max()), 2),
                }
        
        # DaÄŸÄ±lÄ±m bilgisi
        for col in num_cols:
            try:
                skew = float(df[col].skew())
                kurt = float(df[col].kurtosis())
                result["distributions"][col] = {
                    "skewness": round(skew, 3),
                    "kurtosis": round(kurt, 3),
                    "distribution_type": (
                        "Normal daÄŸÄ±lÄ±m" if abs(skew) < 0.5 and abs(kurt) < 1
                        else "SaÄŸa Ã§arpÄ±k" if skew > 0.5
                        else "Sola Ã§arpÄ±k" if skew < -0.5
                        else "Sivri" if kurt > 1
                        else "BasÄ±k"
                    ),
                }
            except Exception:
                pass
    
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. TREND ANALÄ°ZÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def trend_analysis(df: pd.DataFrame, date_col: str = None, value_col: str = None) -> dict:
    """GeliÅŸmiÅŸ zaman serisi trend analizi â€” hareketli ortalama, volatilite, bÃ¼yÃ¼me oranlarÄ±"""
    
    # Tarih sÃ¼tununu otomatik bul
    if not date_col:
        for col in df.columns:
            try:
                pd.to_datetime(df[col])
                date_col = col
                break
            except Exception:
                continue
    
    if not date_col:
        return {"success": False, "error": "Tarih sÃ¼tunu bulunamadÄ±"}
    
    # DeÄŸer sÃ¼tununu otomatik bul
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if not value_col:
        value_col = num_cols[0] if num_cols else None
    
    if not value_col:
        return {"success": False, "error": "SayÄ±sal deÄŸer sÃ¼tunu bulunamadÄ±"}
    
    try:
        df_sorted = df.copy()
        df_sorted[date_col] = pd.to_datetime(df_sorted[date_col])
        df_sorted = df_sorted.sort_values(date_col)
        
        trends = {}
        for col in ([value_col] if isinstance(value_col, str) else num_cols[:5]):
            if col not in df_sorted.columns or not pd.api.types.is_numeric_dtype(df_sorted[col]):
                continue
            
            vals = df_sorted[col].dropna()
            if len(vals) < 3:
                continue
            
            first_half = vals[:len(vals)//2].mean()
            second_half = vals[len(vals)//2:].mean()
            
            change_pct = ((second_half - first_half) / first_half * 100) if first_half != 0 else 0
            
            # Basit regresyon eÄŸimi
            x = np.arange(len(vals))
            slope = np.polyfit(x, vals.values, 1)[0] if len(vals) > 1 else 0
            
            # Hareketli ortalamalar
            moving_avgs = {}
            for window in [3, 7, 14, 30]:
                if len(vals) >= window * 2:
                    ma = vals.rolling(window=window).mean().dropna()
                    moving_avgs[f"MA{window}"] = {
                        "current": round(float(ma.iloc[-1]), 2),
                        "previous": round(float(ma.iloc[-2]), 2) if len(ma) > 1 else None,
                        "trend": "YÃ¼kseliÅŸ" if len(ma) > 1 and ma.iloc[-1] > ma.iloc[-2] else "DÃ¼ÅŸÃ¼ÅŸ",
                    }
            
            # Volatilite (standart sapma / ortalama)
            volatility = float(vals.std() / vals.mean() * 100) if vals.mean() != 0 else 0
            
            # DÃ¶nemsel bÃ¼yÃ¼me oranlarÄ±
            growth_rates = {}
            n = len(vals)
            quartiles = [("Q1â†’Q2", 0, n//4, n//4, n//2), ("Q2â†’Q3", n//4, n//2, n//2, 3*n//4), ("Q3â†’Q4", n//2, 3*n//4, 3*n//4, n)]
            for label, s1, e1, s2, e2 in quartiles:
                if e1 > s1 and e2 > s2:
                    avg1 = vals.iloc[s1:e1].mean()
                    avg2 = vals.iloc[s2:e2].mean()
                    if avg1 != 0:
                        growth_rates[label] = round(((avg2 - avg1) / avg1) * 100, 1)
            
            # Son deÄŸer vs uzun vadeli ortalama karÅŸÄ±laÅŸtÄ±rmasÄ±
            long_avg = float(vals.mean())
            latest = float(vals.iloc[-1])
            position_vs_avg = round(((latest - long_avg) / long_avg) * 100, 1) if long_avg != 0 else 0
            
            trends[col] = {
                "direction": "ArtÄ±ÅŸ" if change_pct > 5 else "Azalma" if change_pct < -5 else "Stabil",
                "change_pct": round(change_pct, 1),
                "first_half_avg": round(float(first_half), 2),
                "second_half_avg": round(float(second_half), 2),
                "slope": round(float(slope), 4),
                "min_value": round(float(vals.min()), 2),
                "max_value": round(float(vals.max()), 2),
                "latest_value": round(float(vals.iloc[-1]), 2),
                "moving_averages": moving_avgs,
                "volatility_pct": round(volatility, 1),
                "growth_rates": growth_rates,
                "position_vs_avg": position_vs_avg,
                "momentum": "GÃ¼Ã§lÃ¼ YÃ¼kseliÅŸ" if change_pct > 20 else "YÃ¼kseliÅŸ" if change_pct > 5 else "GÃ¼Ã§lÃ¼ DÃ¼ÅŸÃ¼ÅŸ" if change_pct < -20 else "DÃ¼ÅŸÃ¼ÅŸ" if change_pct < -5 else "Yatay",
            }
        
        return {
            "success": True,
            "date_column": date_col,
            "date_range": f"{df_sorted[date_col].min()} - {df_sorted[date_col].max()}",
            "data_points": len(df_sorted),
            "trends": trends,
        }
    
    except Exception as e:
        return {"success": False, "error": str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. TOP-N / SIRALAMA ANALÄ°ZÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def top_n_analysis(df: pd.DataFrame, n: int = 10) -> dict:
    """Her sayÄ±sal sÃ¼tun iÃ§in top-N ve bottom-N"""
    results = {}
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = [c for c in df.columns if c not in num_cols]
    
    label_col = cat_cols[0] if cat_cols else None
    
    for col in num_cols[:5]:  # En fazla 5 sÃ¼tun
        sorted_df = df.nlargest(n, col)
        if label_col:
            top = sorted_df[[label_col, col]].to_dict('records')
        else:
            top = sorted_df[[col]].to_dict('records')
        
        sorted_df_bottom = df.nsmallest(n, col)
        if label_col:
            bottom = sorted_df_bottom[[label_col, col]].to_dict('records')
        else:
            bottom = sorted_df_bottom[[col]].to_dict('records')
        
        results[col] = {"top": top, "bottom": bottom}
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. KARÅILAÅTIRMA ANALÄ°ZÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def comparison_analysis(df: pd.DataFrame, group_col: str = None) -> dict:
    """GeliÅŸmiÅŸ kategorik gruplara gÃ¶re karÅŸÄ±laÅŸtÄ±rma â€” medyan, std, fark yÃ¼zdesi"""
    
    if not group_col:
        # Otomatik kategorik sÃ¼tun seÃ§
        cat_cols = [
            c for c in df.columns 
            if df[c].dtype == 'object' and df[c].nunique() <= 20
        ]
        if not cat_cols:
            return {"success": False, "error": "Gruplama iÃ§in uygun kategorik sÃ¼tun bulunamadÄ±"}
        group_col = cat_cols[0]
    
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if not num_cols:
        return {"success": False, "error": "SayÄ±sal sÃ¼tun bulunamadÄ±"}
    
    grouped = df.groupby(group_col)[num_cols].agg(['mean', 'sum', 'count', 'min', 'max'])
    
    result = {
        "success": True,
        "group_column": group_col,
        "groups": list(df[group_col].unique()),
        "group_count": df[group_col].nunique(),
        "summary": {},
    }
    
    for col in num_cols[:5]:
        group_means = df.groupby(group_col)[col].mean().sort_values(ascending=False)
        group_medians = df.groupby(group_col)[col].median().sort_values(ascending=False)
        group_sums = df.groupby(group_col)[col].sum().sort_values(ascending=False)
        group_counts = df.groupby(group_col)[col].count()
        group_stds = df.groupby(group_col)[col].std()
        
        # Gruplar arasÄ± fark yÃ¼zdesi (en iyi vs en kÃ¶tÃ¼)
        best_val = group_means.iloc[0] if len(group_means) > 0 else 0
        worst_val = group_means.iloc[-1] if len(group_means) > 0 else 0
        gap_pct = round(((best_val - worst_val) / worst_val) * 100, 1) if worst_val != 0 else 0
        
        # Genel ortalamadan sapma
        overall_mean = df[col].mean()
        deviations = {}
        for grp in group_means.index:
            dev = round(((group_means[grp] - overall_mean) / overall_mean) * 100, 1) if overall_mean != 0 else 0
            deviations[str(grp)] = dev
        
        result["summary"][col] = {
            "best_group": str(group_means.index[0]) if len(group_means) > 0 else None,
            "worst_group": str(group_means.index[-1]) if len(group_means) > 0 else None,
            "gap_pct": gap_pct,
            "means": {str(k): round(v, 2) for k, v in group_means.items()},
            "medians": {str(k): round(v, 2) for k, v in group_medians.items()},
            "sums": {str(k): round(v, 2) for k, v in group_sums.items()},
            "counts": {str(k): int(v) for k, v in group_counts.items()},
            "std_devs": {str(k): round(v, 2) for k, v in group_stds.items() if not pd.isna(v)},
            "deviation_from_avg": deviations,
            "overall_mean": round(overall_mean, 2),
        }
    
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6b. ANOMALÄ° TESPÄ°TÄ° (IQR + Z-Score)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def anomaly_detection(df: pd.DataFrame) -> dict:
    """IQR ve Z-Score ile aykÄ±rÄ± deÄŸer tespiti"""
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if not num_cols:
        return {"success": False, "error": "SayÄ±sal sÃ¼tun bulunamadÄ±"}
    
    anomalies = {}
    total_anomaly_count = 0
    
    for col in num_cols[:8]:
        vals = df[col].dropna()
        if len(vals) < 10:
            continue
        
        # IQR yÃ¶ntemi
        Q1 = vals.quantile(0.25)
        Q3 = vals.quantile(0.75)
        IQR = Q3 - Q1
        lower_iqr = Q1 - 1.5 * IQR
        upper_iqr = Q3 + 1.5 * IQR
        iqr_outliers = vals[(vals < lower_iqr) | (vals > upper_iqr)]
        
        # Z-Score yÃ¶ntemi
        mean_val = vals.mean()
        std_val = vals.std()
        if std_val > 0:
            z_scores = np.abs((vals - mean_val) / std_val)
            z_outliers = vals[z_scores > 2.5]
        else:
            z_outliers = pd.Series(dtype=float)
        
        # Ciddi anomaliler (her iki yÃ¶ntemde de tespit edilen)
        severe = set(iqr_outliers.index) & set(z_outliers.index)
        
        col_anomaly_count = len(iqr_outliers)
        total_anomaly_count += col_anomaly_count
        
        if col_anomaly_count > 0:
            anomalies[col] = {
                "iqr_count": len(iqr_outliers),
                "zscore_count": len(z_outliers),
                "severe_count": len(severe),
                "anomaly_pct": round(len(iqr_outliers) / len(vals) * 100, 1),
                "normal_range": f"{round(float(lower_iqr), 2)} â€” {round(float(upper_iqr), 2)}",
                "mean": round(float(mean_val), 2),
                "std": round(float(std_val), 2),
                "top_anomalies": sorted([round(float(v), 2) for v in iqr_outliers.values], reverse=True)[:5],
                "severity": "Kritik" if len(severe) > 0 else "UyarÄ±" if len(iqr_outliers) > len(vals) * 0.05 else "Bilgi",
            }
    
    return {
        "success": True,
        "total_anomalies": total_anomaly_count,
        "columns_with_anomalies": len(anomalies),
        "total_columns_checked": len(num_cols[:8]),
        "anomaly_details": anomalies,
        "overall_health": "Ä°yi" if total_anomaly_count < 5 else "Dikkat" if total_anomaly_count < 20 else "Sorunlu",
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6c. KORELASYON ANALÄ°ZÄ° (DetaylÄ±)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def correlation_analysis(df: pd.DataFrame) -> dict:
    """DetaylÄ± korelasyon matrisi ve iliÅŸki Ã¶nerileri"""
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if len(num_cols) < 2:
        return {"success": False, "error": "En az 2 sayÄ±sal sÃ¼tun gerekli"}
    
    corr_matrix = df[num_cols[:10]].corr()
    
    # TÃ¼m iliÅŸkileri sÄ±nÄ±fla
    relationships = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            val = corr_matrix.iloc[i, j]
            if pd.isna(val):
                continue
            abs_val = abs(val)
            strength = (
                "Ã‡ok GÃ¼Ã§lÃ¼" if abs_val > 0.9 else
                "GÃ¼Ã§lÃ¼" if abs_val > 0.7 else
                "Orta" if abs_val > 0.5 else
                "ZayÄ±f" if abs_val > 0.3 else
                "Ã‡ok ZayÄ±f"
            )
            relationships.append({
                "col1": corr_matrix.columns[i],
                "col2": corr_matrix.columns[j],
                "correlation": round(val, 3),
                "strength": strength,
                "direction": "Pozitif" if val > 0 else "Negatif",
                "actionable": abs_val > 0.5,
            })
    
    # Ã–nemlilere gÃ¶re sÄ±rala
    relationships.sort(key=lambda x: abs(x["correlation"]), reverse=True)
    
    # Her sÃ¼tunun en gÃ¼Ã§lÃ¼ iliÅŸkisi
    best_pairs = {}
    for col in num_cols[:10]:
        col_rels = [r for r in relationships if r["col1"] == col or r["col2"] == col]
        if col_rels:
            best = col_rels[0]
            partner = best["col2"] if best["col1"] == col else best["col1"]
            best_pairs[col] = {"partner": partner, "correlation": best["correlation"], "strength": best["strength"]}
    
    return {
        "success": True,
        "matrix": {str(k): {str(k2): round(v2, 3) for k2, v2 in v.items()} for k, v in corr_matrix.to_dict().items()},
        "relationships": relationships[:20],
        "strong_count": sum(1 for r in relationships if abs(r["correlation"]) > 0.7),
        "moderate_count": sum(1 for r in relationships if 0.5 < abs(r["correlation"]) <= 0.7),
        "best_pairs": best_pairs,
        "total_pairs": len(relationships),
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6d. DAÄILIM ANALÄ°ZÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def distribution_analysis(df: pd.DataFrame) -> dict:
    """Veri daÄŸÄ±lÄ±m profili â€” Ã§arpÄ±klÄ±k, basÄ±klÄ±k, yÃ¼zdelikler"""
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if not num_cols:
        return {"success": False, "error": "SayÄ±sal sÃ¼tun bulunamadÄ±"}
    
    distributions = {}
    for col in num_cols[:8]:
        vals = df[col].dropna()
        if len(vals) < 5:
            continue
        
        try:
            skew = float(vals.skew())
            kurt = float(vals.kurtosis())
            
            # DaÄŸÄ±lÄ±m tipi belirleme
            if abs(skew) < 0.5 and abs(kurt) < 1:
                dist_type = "Normal (Simetrik)"
            elif skew > 1:
                dist_type = "GÃ¼Ã§lÃ¼ SaÄŸa Ã‡arpÄ±k"
            elif skew > 0.5:
                dist_type = "Hafif SaÄŸa Ã‡arpÄ±k"
            elif skew < -1:
                dist_type = "GÃ¼Ã§lÃ¼ Sola Ã‡arpÄ±k"
            elif skew < -0.5:
                dist_type = "Hafif Sola Ã‡arpÄ±k"
            elif kurt > 3:
                dist_type = "Sivri (Leptokurtik)"
            elif kurt < -1:
                dist_type = "BasÄ±k (Platykurtik)"
            else:
                dist_type = "Normal CivarÄ±"
            
            # YÃ¼zdelik deÄŸerler
            percentiles = {
                "P5": round(float(vals.quantile(0.05)), 2),
                "P10": round(float(vals.quantile(0.10)), 2),
                "P25": round(float(vals.quantile(0.25)), 2),
                "P50": round(float(vals.quantile(0.50)), 2),
                "P75": round(float(vals.quantile(0.75)), 2),
                "P90": round(float(vals.quantile(0.90)), 2),
                "P95": round(float(vals.quantile(0.95)), 2),
            }
            
            # Histogram benzeri bant analizi
            bands = {}
            min_val, max_val = float(vals.min()), float(vals.max())
            if max_val > min_val:
                band_width = (max_val - min_val) / 5
                for i in range(5):
                    low = min_val + i * band_width
                    high = low + band_width
                    count = int(((vals >= low) & (vals < high if i < 4 else vals <= high)).sum())
                    bands[f"{round(low, 1)}-{round(high, 1)}"] = count
            
            distributions[col] = {
                "distribution_type": dist_type,
                "skewness": round(skew, 3),
                "kurtosis": round(kurt, 3),
                "mean": round(float(vals.mean()), 2),
                "median": round(float(vals.median()), 2),
                "mode": round(float(vals.mode().iloc[0]), 2) if len(vals.mode()) > 0 else None,
                "std": round(float(vals.std()), 2),
                "cv_pct": round(float(vals.std() / vals.mean() * 100), 1) if vals.mean() != 0 else 0,
                "percentiles": percentiles,
                "range": round(max_val - min_val, 2),
                "iqr": round(float(vals.quantile(0.75) - vals.quantile(0.25)), 2),
                "bands": bands,
            }
        except Exception:
            continue
    
    return {"success": True, "distributions": distributions, "columns_analyzed": len(distributions)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6e. TAHMÄ°NLEME (Basit Projeksiyon)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def forecast_analysis(df: pd.DataFrame, date_col: str = None, value_col: str = None, periods: int = 5) -> dict:
    """Hareketli ortalama ve lineer regresyon tabanlÄ± basit tahminleme"""
    
    if not date_col:
        for col in df.columns:
            try:
                pd.to_datetime(df[col])
                date_col = col
                break
            except Exception:
                continue
    
    if not date_col:
        return {"success": False, "error": "Tarih sÃ¼tunu bulunamadÄ±"}
    
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if not value_col:
        value_col = num_cols[0] if num_cols else None
    
    if not value_col:
        return {"success": False, "error": "SayÄ±sal deÄŸer sÃ¼tunu bulunamadÄ±"}
    
    try:
        df_sorted = df.copy()
        df_sorted[date_col] = pd.to_datetime(df_sorted[date_col])
        df_sorted = df_sorted.sort_values(date_col)
        
        vals = df_sorted[value_col].dropna()
        if len(vals) < 5:
            return {"success": False, "error": "Tahmin iÃ§in en az 5 veri noktasÄ± gerekli"}
        
        x = np.arange(len(vals))
        
        # Lineer regresyon
        coeffs = np.polyfit(x, vals.values, 1)
        slope, intercept = coeffs
        
        # RÂ² hesapla
        y_pred = slope * x + intercept
        ss_res = np.sum((vals.values - y_pred) ** 2)
        ss_tot = np.sum((vals.values - vals.mean()) ** 2)
        r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
        
        # Gelecek tahminleri
        future_x = np.arange(len(vals), len(vals) + periods)
        linear_forecast = [round(float(slope * xi + intercept), 2) for xi in future_x]
        
        # Hareketli ortalama tabanlÄ± tahmin
        window = min(5, len(vals) // 2)
        ma_last = float(vals.rolling(window=window).mean().iloc[-1]) if window > 0 else float(vals.mean())
        ma_trend = float(slope)  # Trendi ekle
        ma_forecast = [round(ma_last + ma_trend * (i + 1), 2) for i in range(periods)]
        
        # GÃ¼ven seviyesi
        confidence = "YÃ¼ksek" if r_squared > 0.7 else "Orta" if r_squared > 0.4 else "DÃ¼ÅŸÃ¼k"
        
        return {
            "success": True,
            "value_column": value_col,
            "data_points": len(vals),
            "forecast_periods": periods,
            "linear_forecast": linear_forecast,
            "ma_forecast": ma_forecast,
            "trend_slope": round(float(slope), 4),
            "r_squared": round(r_squared, 3),
            "confidence": confidence,
            "current_value": round(float(vals.iloc[-1]), 2),
            "predicted_change_pct": round(((linear_forecast[-1] - float(vals.iloc[-1])) / float(vals.iloc[-1])) * 100, 1) if vals.iloc[-1] != 0 else 0,
            "method": "Lineer Regresyon + Hareketli Ortalama",
        }
    
    except Exception as e:
        return {"success": False, "error": str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6f. PARETO / ABC ANALÄ°ZÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def pareto_analysis(df: pd.DataFrame, value_col: str = None, label_col: str = None) -> dict:
    """80/20 kuralÄ± ve ABC sÄ±nÄ±flandÄ±rmasÄ±"""
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = [c for c in df.columns if df[c].dtype == 'object']
    
    if not value_col:
        value_col = num_cols[0] if num_cols else None
    if not label_col:
        label_col = cat_cols[0] if cat_cols else None
    
    if not value_col:
        return {"success": False, "error": "SayÄ±sal sÃ¼tun bulunamadÄ±"}
    
    try:
        # Label yoksa index kullan
        if label_col:
            grouped = df.groupby(label_col)[value_col].sum().sort_values(ascending=False)
        else:
            grouped = df[value_col].sort_values(ascending=False)
        
        total = grouped.sum()
        if total == 0:
            return {"success": False, "error": "Toplam deÄŸer 0"}
        
        cumulative = grouped.cumsum()
        cumulative_pct = (cumulative / total * 100).round(1)
        
        # ABC SÄ±nÄ±flandÄ±rmasÄ±
        a_items = []  # %80'e kadar
        b_items = []  # %80-95
        c_items = []  # %95-100
        
        for idx, pct in cumulative_pct.items():
            item = {
                "label": str(idx),
                "value": round(float(grouped[idx]), 2),
                "pct": round(float(grouped[idx] / total * 100), 1),
                "cumulative_pct": float(pct),
            }
            if pct <= 80:
                a_items.append(item)
            elif pct <= 95:
                b_items.append(item)
            else:
                c_items.append(item)
        
        # Tam A sÄ±nÄ±rÄ±nÄ± kontrol et (son A Ã¶ÄŸesi %80'i geÃ§ebilir)
        if not a_items and b_items:
            a_items.append(b_items.pop(0))
        
        # 80/20 kuralÄ± kontrolÃ¼
        top_20_pct_count = max(1, int(len(grouped) * 0.2))
        top_20_value = grouped.head(top_20_pct_count).sum()
        top_20_contribution = round(float(top_20_value / total * 100), 1)
        
        return {
            "success": True,
            "value_column": value_col,
            "label_column": label_col,
            "total_items": len(grouped),
            "total_value": round(float(total), 2),
            "pareto_rule": {
                "top_20_pct_items": top_20_pct_count,
                "top_20_contribution_pct": top_20_contribution,
                "is_pareto": top_20_contribution >= 65,  # Kabaca 80/20'ye yakÄ±n
            },
            "abc": {
                "A": {"count": len(a_items), "items": a_items[:10], "description": "YÃ¼ksek deÄŸer (%80 katkÄ±)"},
                "B": {"count": len(b_items), "items": b_items[:10], "description": "Orta deÄŸer (%80-95 katkÄ±)"},
                "C": {"count": len(c_items), "items": c_items[:10], "description": "DÃ¼ÅŸÃ¼k deÄŸer (%95-100 katkÄ±)"},
            },
        }
    
    except Exception as e:
        return {"success": False, "error": str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6g. VERÄ° KALÄ°TESÄ° DENETÄ°MÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def data_quality_analysis(df: pd.DataFrame) -> dict:
    """KapsamlÄ± veri kalitesi deÄŸerlendirmesi"""
    total_cells = df.shape[0] * df.shape[1]
    
    # 1. Eksik veri analizi
    missing = {}
    total_missing = 0
    for col in df.columns:
        null_count = int(df[col].isnull().sum())
        if null_count > 0:
            missing[col] = {
                "count": null_count,
                "pct": round(null_count / len(df) * 100, 1),
                "severity": "Kritik" if null_count / len(df) > 0.3 else "UyarÄ±" if null_count / len(df) > 0.1 else "DÃ¼ÅŸÃ¼k",
            }
            total_missing += null_count
    
    # 2. Tekrarlayan satÄ±rlar
    duplicates = int(df.duplicated().sum())
    dup_pct = round(duplicates / len(df) * 100, 1) if len(df) > 0 else 0
    
    # 3. SÃ¼tun tip tutarlÄ±lÄ±ÄŸÄ±
    type_issues = {}
    for col in df.columns:
        if df[col].dtype == 'object':
            # SayÄ± gibi gÃ¶rÃ¼nen metin var mÄ±?
            numeric_like = df[col].dropna().apply(lambda x: str(x).replace(',', '.').replace(' ', '').replace('-', '')).str.match(r'^\d+\.?\d*$')
            numeric_count = int(numeric_like.sum()) if len(numeric_like) > 0 else 0
            total_non_null = int(df[col].notna().sum())
            if total_non_null > 0 and numeric_count / total_non_null > 0.7:
                type_issues[col] = {
                    "issue": "SayÄ±sal veri metin olarak saklanmÄ±ÅŸ",
                    "numeric_ratio": round(numeric_count / total_non_null * 100, 1),
                }
    
    # 4. BoÅŸ/whitespace satÄ±rlar
    whitespace_issues = {}
    for col in df.columns:
        if df[col].dtype == 'object':
            ws_count = int(df[col].dropna().apply(lambda x: str(x).strip() == '').sum())
            if ws_count > 0:
                whitespace_issues[col] = ws_count
    
    # 5. Genel kalite skoru
    completeness = round((1 - total_missing / total_cells) * 100, 1) if total_cells > 0 else 100
    uniqueness = round((1 - duplicates / len(df)) * 100, 1) if len(df) > 0 else 100
    consistency = round((1 - len(type_issues) / len(df.columns)) * 100, 1) if len(df.columns) > 0 else 100
    quality_score = round((completeness * 0.4 + uniqueness * 0.3 + consistency * 0.3), 1)
    
    return {
        "success": True,
        "rows": len(df),
        "columns": len(df.columns),
        "total_cells": total_cells,
        "quality_score": quality_score,
        "quality_grade": "A" if quality_score >= 90 else "B" if quality_score >= 75 else "C" if quality_score >= 60 else "D" if quality_score >= 40 else "F",
        "completeness": {
            "score": completeness,
            "total_missing": total_missing,
            "columns_with_missing": len(missing),
            "details": missing,
        },
        "uniqueness": {
            "score": uniqueness,
            "duplicate_rows": duplicates,
            "duplicate_pct": dup_pct,
        },
        "consistency": {
            "score": consistency,
            "type_issues": type_issues,
            "whitespace_issues": whitespace_issues,
        },
        "recommendations": _quality_recommendations(completeness, uniqueness, consistency, missing, type_issues),
    }


def _quality_recommendations(completeness, uniqueness, consistency, missing, type_issues) -> list:
    """Veri kalitesi tavsiyelerini oluÅŸtur"""
    recs = []
    if completeness < 90:
        worst_cols = sorted(missing.items(), key=lambda x: x[1]["count"], reverse=True)[:3]
        cols_str = ", ".join(f"{c} (%{v['pct']})" for c, v in worst_cols)
        recs.append(f"Eksik veri temizliÄŸi: {cols_str} sÃ¼tunlarÄ±ndaki boÅŸluklarÄ± doldurun veya Ã§Ä±karÄ±n")
    if uniqueness < 95:
        recs.append(f"Tekrarlayan satÄ±rlarÄ± kaldÄ±rÄ±n (toplam tekrar oranÄ±: %{round(100-uniqueness, 1)})")
    if consistency < 90:
        for col, info in type_issues.items():
            recs.append(f"'{col}' sÃ¼tununu sayÄ±sal tipe dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n (%{info['numeric_ratio']} sayÄ±sal)")
    if completeness >= 90 and uniqueness >= 95 and consistency >= 90:
        recs.append("Veri kalitesi genel olarak iyi durumda âœ“")
    return recs


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7. RAPOR + YORUM + TAVSÄ°YE OLUÅTURMA (LLM Ä°Ã‡Ä°N PROMPT)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_analysis_prompt(
    df: pd.DataFrame,
    analysis_type: str = "full",
    question: str = None,
    filename: str = None,
) -> str:
    """
    LLM'e gÃ¶nderilecek detaylÄ± analiz prompt'u oluÅŸtur.
    
    analysis_type:
        - "full": Tam analiz (keÅŸif + istatistik + pivot + trend + tavsiye)
        - "pivot": Sadece pivot tablo
        - "trend": Trend analizi
        - "compare": KarÅŸÄ±laÅŸtÄ±rma
        - "summary": HÄ±zlÄ± Ã¶zet
        - "recommend": Tavsiye odaklÄ±
        - "report": Resmi rapor formatÄ±nda
        - "anomaly": Anomali / aykÄ±rÄ± deÄŸer tespiti
        - "correlation": Korelasyon analizi
        - "distribution": DaÄŸÄ±lÄ±m analizi
        - "forecast": Tahminleme / projeksiyon
        - "pareto": Pareto ABC analizi
        - "quality": Veri kalitesi denetimi
    """
    
    discovery = discover_data(df)
    
    # Temel veri bilgisi â€” tÃ¼m tipler iÃ§in ortak
    prompt = f"""## ğŸ“Š DokÃ¼man Analizi: {filename or 'YÃ¼klenen Veri'}

### Veri Ã–zeti:
- **SatÄ±r sayÄ±sÄ±**: {discovery['row_count']}
- **SÃ¼tun sayÄ±sÄ±**: {discovery['col_count']}
- **SayÄ±sal sÃ¼tunlar**: {', '.join(discovery['numeric_columns']) or 'Yok'}
- **Kategorik sÃ¼tunlar**: {', '.join(discovery['categorical_columns']) or 'Yok'}
- **Tarih sÃ¼tunlarÄ±**: {', '.join(discovery['date_columns']) or 'Yok'}
"""
    
    # SÃ¼tun detaylarÄ±
    prompt += "\n### SÃ¼tun Bilgileri:\n"
    for col_info in discovery["columns"]:
        line = f"- **{col_info['name']}** ({col_info['type']}): "
        if col_info["type"] == "numeric":
            line += f"Min={col_info.get('min')}, Max={col_info.get('max')}, Ort={col_info.get('mean')}, Toplam={col_info.get('sum')}"
        elif col_info["type"] == "categorical":
            top_vals = col_info.get("top_values", {})
            top_3 = list(top_vals.items())[:3]
            line += f"{col_info['unique_count']} benzersiz deÄŸer. En sÄ±k: {', '.join(f'{k}({v})' for k, v in top_3)}"
        elif col_info["type"] == "date":
            line += f"AralÄ±k: {col_info.get('min')} â†’ {col_info.get('max')}"
        else:
            line += f"Ort uzunluk: {col_info.get('avg_length', 'N/A')} karakter"
        
        if col_info["null_count"] > 0:
            line += f" [âš ï¸ %{col_info['null_pct']} eksik]"
        prompt += line + "\n"
    
    # â”€â”€ TÄ°P-SPESÄ°FÄ°K VERÄ° EKLEMELERÄ° â”€â”€
    
    # Ä°statistiksel Analiz (tÃ¼m tipler iÃ§in temel)
    stats = statistical_analysis(df)
    
    if stats.get("strong_correlations") and analysis_type in ("full", "correlation", "report", "recommend"):
        prompt += "\n### Korelasyonlar (GÃ¼Ã§lÃ¼ Ä°liÅŸkiler):\n"
        for corr in stats["strong_correlations"]:
            prompt += f"- **{corr['col1']}** â†” **{corr['col2']}**: {corr['correlation']} ({corr['strength']} {corr['direction']})\n"
    
    if stats.get("outliers") and analysis_type in ("full", "anomaly", "quality", "report"):
        prompt += "\n### AykÄ±rÄ± DeÄŸerler:\n"
        for col, info in stats["outliers"].items():
            prompt += f"- **{col}**: {info['count']} aykÄ±rÄ± deÄŸer (%{info['percentage']}), normal aralÄ±k: {info['lower_bound']} - {info['upper_bound']}\n"
    
    # Pivot Tablo
    if analysis_type in ("full", "pivot") and discovery["categorical_columns"] and discovery["numeric_columns"]:
        pivot_result = smart_pivot(df)
        if pivot_result.get("success"):
            prompt += f"\n### Pivot Tablo:\n```\n{pivot_result['table_str'][:2000]}\n```\n"
    
    # Trend Analizi
    if analysis_type in ("full", "trend", "forecast") and discovery["date_columns"]:
        trend = trend_analysis(df)
        if trend.get("success"):
            prompt += f"\n### Trend Analizi ({trend['date_range']}):\n"
            for col, t_info in trend.get("trends", {}).items():
                prompt += f"- **{col}**: {t_info['direction']} (%{t_info['change_pct']}), Momentum: {t_info.get('momentum', '-')}, Volatilite: %{t_info.get('volatility_pct', 0)}, Son: {t_info['latest_value']}\n"
                if t_info.get("moving_averages"):
                    for ma_name, ma_info in t_info["moving_averages"].items():
                        prompt += f"  - {ma_name}: {ma_info['current']} ({ma_info['trend']})\n"
                if t_info.get("growth_rates"):
                    for period, rate in t_info["growth_rates"].items():
                        prompt += f"  - {period}: %{rate} bÃ¼yÃ¼me\n"
    
    # KarÅŸÄ±laÅŸtÄ±rma
    if analysis_type in ("full", "compare") and discovery["categorical_columns"]:
        comp = comparison_analysis(df)
        if comp.get("success"):
            prompt += f"\n### Grup KarÅŸÄ±laÅŸtÄ±rmasÄ± ({comp['group_column']}, {comp['group_count']} grup):\n"
            for col, cinfo in comp.get("summary", {}).items():
                prompt += f"- **{col}**: En iyi={cinfo['best_group']}, En dÃ¼ÅŸÃ¼k={cinfo['worst_group']}, Fark=%{cinfo.get('gap_pct', 0)}\n"
                if cinfo.get("medians"):
                    prompt += f"  Medyanlar: {', '.join(f'{k}={v}' for k, v in list(cinfo['medians'].items())[:5])}\n"
    
    # Top-N
    if analysis_type in ("full", "report", "pareto") and discovery["numeric_columns"]:
        top_n = top_n_analysis(df, n=5)
        if top_n:
            prompt += "\n### En YÃ¼ksek / En DÃ¼ÅŸÃ¼k DeÄŸerler:\n"
            for col, data in list(top_n.items())[:3]:
                prompt += f"**{col} â€” Top 5:**\n"
                for item in data["top"][:5]:
                    vals_str = [f"{k}: {v}" for k, v in item.items()]
                    prompt += f"  - {', '.join(vals_str)}\n"
    
    # â”€â”€ YENÄ° TÄ°PLER Ä°Ã‡Ä°N EK VERÄ° â”€â”€
    
    # Anomali Tespiti
    if analysis_type == "anomaly":
        anom = anomaly_detection(df)
        if anom.get("success"):
            prompt += f"\n### Anomali Tespiti (Genel SaÄŸlÄ±k: {anom['overall_health']}):\n"
            prompt += f"- Toplam anomali: {anom['total_anomalies']}, Etkilenen sÃ¼tun: {anom['columns_with_anomalies']}/{anom['total_columns_checked']}\n"
            for col, det in anom.get("anomaly_details", {}).items():
                prompt += f"- **{col}** [{det['severity']}]: {det['iqr_count']} IQR, {det['zscore_count']} Z-Score aykÄ±rÄ±. Normal aralÄ±k: {det['normal_range']}\n"
                if det.get("top_anomalies"):
                    prompt += f"  En bÃ¼yÃ¼k anomaliler: {', '.join(str(v) for v in det['top_anomalies'][:5])}\n"
    
    # Korelasyon Analizi
    if analysis_type == "correlation":
        corr = correlation_analysis(df)
        if corr.get("success"):
            prompt += f"\n### DetaylÄ± Korelasyon Analizi ({corr['total_pairs']} Ã§ift incelendi):\n"
            prompt += f"- GÃ¼Ã§lÃ¼ iliÅŸki: {corr['strong_count']}, Orta iliÅŸki: {corr['moderate_count']}\n"
            for rel in corr.get("relationships", [])[:10]:
                emoji = "ğŸ”´" if abs(rel["correlation"]) > 0.7 else "ğŸŸ¡" if abs(rel["correlation"]) > 0.5 else "âšª"
                prompt += f"  {emoji} **{rel['col1']}** â†” **{rel['col2']}**: {rel['correlation']} ({rel['strength']}, {rel['direction']})\n"
    
    # DaÄŸÄ±lÄ±m Analizi
    if analysis_type == "distribution":
        dist = distribution_analysis(df)
        if dist.get("success"):
            prompt += f"\n### DaÄŸÄ±lÄ±m Analizi ({dist['columns_analyzed']} sÃ¼tun):\n"
            for col, d in dist.get("distributions", {}).items():
                prompt += f"- **{col}**: {d['distribution_type']}, Ort={d['mean']}, Medyan={d['median']}, Std={d['std']}, CV=%{d['cv_pct']}\n"
                prompt += f"  Ã‡arpÄ±klÄ±k={d['skewness']}, BasÄ±klÄ±k={d['kurtosis']}, IQR={d['iqr']}\n"
                prompt += f"  YÃ¼zdelikler: P25={d['percentiles']['P25']}, P50={d['percentiles']['P50']}, P75={d['percentiles']['P75']}, P95={d['percentiles']['P95']}\n"
    
    # Tahminleme
    if analysis_type == "forecast":
        fc = forecast_analysis(df)
        if fc.get("success"):
            prompt += f"\n### Tahminleme ({fc['method']}, RÂ²={fc['r_squared']}, GÃ¼ven: {fc['confidence']}):\n"
            prompt += f"- Mevcut deÄŸer: {fc['current_value']}, EÄŸim: {fc['trend_slope']}\n"
            prompt += f"- Lineer tahmin (sonraki {fc['forecast_periods']} dÃ¶nem): {', '.join(str(v) for v in fc['linear_forecast'])}\n"
            prompt += f"- Hareketli Ort. tahmin: {', '.join(str(v) for v in fc['ma_forecast'])}\n"
            prompt += f"- Beklenen deÄŸiÅŸim: %{fc['predicted_change_pct']}\n"
    
    # Pareto / ABC
    if analysis_type == "pareto":
        par = pareto_analysis(df)
        if par.get("success"):
            pr = par["pareto_rule"]
            prompt += f"\n### Pareto / ABC Analizi ({par['total_items']} Ã¶ÄŸe, Toplam: {par['total_value']}):\n"
            prompt += f"- **80/20 KuralÄ±**: Ãœst %20 ({pr['top_20_pct_items']} Ã¶ÄŸe) toplam deÄŸerin %{pr['top_20_contribution_pct']}'ini oluÅŸturuyor {'âœ“ Pareto geÃ§erli' if pr['is_pareto'] else 'âœ— Pareto geÃ§erli deÄŸil'}\n"
            for grade in ["A", "B", "C"]:
                abc = par["abc"][grade]
                items_str = ", ".join(f"{it['label']}({it['pct']}%)" for it in abc["items"][:5])
                prompt += f"- **SÄ±nÄ±f {grade}** ({abc['count']} Ã¶ÄŸe): {abc['description']}. {items_str}\n"
    
    # Veri Kalitesi
    if analysis_type == "quality":
        qual = data_quality_analysis(df)
        if qual.get("success"):
            prompt += f"\n### Veri Kalitesi Raporu (Skor: {qual['quality_score']}/100, Not: {qual['quality_grade']}):\n"
            prompt += f"- BÃ¼tÃ¼nlÃ¼k: %{qual['completeness']['score']} ({qual['completeness']['total_missing']} eksik hÃ¼cre)\n"
            prompt += f"- Teksillik: %{qual['uniqueness']['score']} ({qual['uniqueness']['duplicate_rows']} tekrar satÄ±r)\n"
            prompt += f"- TutarlÄ±lÄ±k: %{qual['consistency']['score']} ({len(qual['consistency']['type_issues'])} tip sorunu)\n"
            if qual.get("recommendations"):
                prompt += "- **Tavsiyeler**:\n"
                for rec in qual["recommendations"]:
                    prompt += f"  â€¢ {rec}\n"
    
    # Veri Ã¶rneÄŸi
    sample_rows = min(5, len(df))
    prompt += f"\n### Veri Ã–rneÄŸi (Ä°lk {sample_rows} SatÄ±r):\n"
    prompt += f"```\n{df.head(sample_rows).to_string()}\n```\n"
    
    # â”€â”€ TÄ°P-SPESÄ°FÄ°K GÃ–REV TALÄ°MATLARI â”€â”€
    
    if analysis_type == "pivot":
        prompt += """
**GÃ–REV**: YukarÄ±daki pivot tablo verilerini detaylÄ± analiz et:
1. Hangi kategoriler Ã¶ne Ã§Ä±kÄ±yor ve neden?
2. Kategoriler arasÄ± performans farklarÄ± ve oranlarÄ±
3. En dikkat Ã§ekici Ã§apraz kesiÅŸimler
4. YÃ¶neticiler iÃ§in karar Ã¶nerileri
TablolarÄ± ve sayÄ±sal karÅŸÄ±laÅŸtÄ±rmalarÄ± mutlaka kullan."""

    elif analysis_type == "trend":
        prompt += """
**GÃ–REV**: Trend analizini profesyonelce yorumla:
1. Ana trend yÃ¶nÃ¼ ve gÃ¼cÃ¼ (momentum deÄŸerlendirmesi)
2. Hareketli ortalamalarÄ±n gÃ¶sterdiÄŸi kÄ±sa/uzun vadeli sinyaller
3. Volatilite ve risk deÄŸerlendirmesi
4. DÃ¶nemsel bÃ¼yÃ¼me oranlarÄ±nÄ±n analizi
5. Mevsimsel veya dÃ¶ngÃ¼sel paternler varsa belirt
6. Gelecek dÃ¶nem iÃ§in beklentiler ve Ã¶neriler
Her bulguyu verilerle destekle."""

    elif analysis_type == "compare":
        prompt += """
**GÃ–REV**: GruplarÄ± kapsamlÄ± karÅŸÄ±laÅŸtÄ±r:
1. En iyi ve en kÃ¶tÃ¼ performans gÃ¶steren gruplar (neden?)
2. Medyan vs ortalama farklarÄ±nÄ±n gÃ¶sterdiÄŸi daÄŸÄ±lÄ±m Ã¶zellikleri
3. Gruplar arasÄ± fark yÃ¼zdeleri ve anlamlÄ±lÄ±ÄŸÄ±
4. Genel ortalamadan sapma analizi
5. Standart sapma ile tutarlÄ±lÄ±k deÄŸerlendirmesi
6. Her grup iÃ§in spesifik aksiyon Ã¶nerileri"""

    elif analysis_type == "recommend":
        prompt += """
**GÃ–REV**: Bu verilere dayanarak somut, uygulanabilir ve Ã¶nceliklendirilmiÅŸ TAVSÄ°YELER sun:
1. **Acil Aksiyonlar** (0-1 ay): Hemen yapÄ±lmasÄ± gerekenler
2. **KÄ±sa Vadeli** (1-3 ay): PlanlÄ± iyileÅŸtirmeler
3. **Uzun Vadeli** (3-12 ay): Stratejik dÃ¶nÃ¼ÅŸÃ¼mler
Her tavsiyeyi:
- Verilerle destekle (hangi sayÄ±/oran bunu gerektiriyor?)
- Beklenen etkiyi belirt
- Risk/maliyet analizi yap
- Ã–ncelik seviyesi ata (Kritik/YÃ¼ksek/Orta/DÃ¼ÅŸÃ¼k)
En az 5-7 madde sun."""

    elif analysis_type == "report":
        prompt += """
**GÃ–REV**: Profesyonel bir YÃ–NETÄ°CÄ° RAPORU oluÅŸtur:
1. **ğŸ“‹ YÃ¶netici Ã–zeti** (3-5 cÃ¼mle, en kritik bulgular)
2. **ğŸ“Š Temel Metrikler** (KPI tablosu formatÄ±nda)
3. **ğŸ“ˆ DetaylÄ± Bulgular** (her kategori/metrik iÃ§in derinlemesine analiz)
4. **ğŸ” KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz** (dÃ¶nemler arasÄ±, gruplar arasÄ±)
5. **âš ï¸ Risk ve UyarÄ±lar** (dikkat edilmesi gerekenler)
6. **âœ… Aksiyon PlanÄ±** (somut adÄ±mlar, sorumlular, zaman Ã§izelgesi)
7. **ğŸ“Œ SonuÃ§** (genel deÄŸerlendirme)
TÃ¼m bÃ¶lÃ¼mlerde sayÄ±sal veriler kullan. Tablolarla destekle."""

    elif analysis_type == "summary":
        prompt += """
**GÃ–REV**: Bu veri setini 8-10 cÃ¼mlelik etkili bir Ã¶zete dÃ¶nÃ¼ÅŸtÃ¼r:
1. Verinin ne hakkÄ±nda olduÄŸu ve kapsamÄ±
2. En Ã§arpÄ±cÄ± 3 sayÄ±sal bulgu
3. Dikkat Ã§ekici pattern veya anomali varsa
4. Genel durum deÄŸerlendirmesi (iyi/kÃ¶tÃ¼/kritik)
5. Tek cÃ¼mlelik sonuÃ§ ve Ã¶neri
KÄ±sa, Ã¶z ama bilgi dolu olsun."""

    elif analysis_type == "anomaly":
        prompt += """
**GÃ–REV**: Anomali ve aykÄ±rÄ± deÄŸer tespitini detaylÄ± raporla:
1. Tespit edilen anomalilerin listesi ve ciddiyet seviyeleri
2. Her anomalinin olasÄ± nedenleri (veri hatasÄ± mÄ±, gerÃ§ek sapma mÄ±?)
3. Hangi sÃ¼tunlar en fazla anomali iÃ§eriyor ve bunun anlamÄ±
4. Anomalilerin iÅŸ sÃ¼reÃ§lerine potansiyel etkisi
5. Temizleme/dÃ¼zeltme tavsiyeler (hangileri silinmeli, hangileri araÅŸtÄ±rÄ±lmalÄ±)
6. Anomalilerin kÃ¶k neden analizi
Her bulguyu IQR ve Z-Score deÄŸerleriyle destekle."""

    elif analysis_type == "correlation":
        prompt += """
**GÃ–REV**: Korelasyon iliÅŸkilerini iÅŸ perspektifinden yorumla:
1. En gÃ¼Ã§lÃ¼ pozitif ve negatif iliÅŸkiler ve ne anlama geldikleri
2. Beklenmeyen veya ilginÃ§ iliÅŸkiler (neden-sonuÃ§ tartÄ±ÅŸmasÄ±)
3. Ä°ÅŸ kararlarÄ±nda kullanÄ±labilecek iliÅŸki Ã¶nerileri
4. Korelasyon â‰  nedensellik uyarÄ±sÄ± ile yorumlar
5. Birbirine baÄŸÄ±mlÄ± deÄŸiÅŸken gruplarÄ± (cluster)
6. Stratejik Ã¶neriler: "X'i artÄ±rÄ±rsanÄ±z Y de artma/azalma eÄŸiliminde"
Her iliÅŸkiyi korelasyon katsayÄ±sÄ±yla birlikte sun."""

    elif analysis_type == "distribution":
        prompt += """
**GÃ–REV**: Veri daÄŸÄ±lÄ±mlarÄ±nÄ± detaylÄ± analiz et:
1. Her sÃ¼tunun daÄŸÄ±lÄ±m tipi ve bunun anlamÄ±
2. Normal daÄŸÄ±lÄ±mdan sapmalarÄ±n yorumu (Ã§arpÄ±klÄ±k, basÄ±klÄ±k)
3. YÃ¼zdelik dilim analizi â€” deÄŸerlerin nerede yoÄŸunlaÅŸtÄ±ÄŸÄ±
4. Ortalama vs Medyan farkÄ±nÄ±n gÃ¶sterdiÄŸi dengesizlik
5. DeÄŸiÅŸkenlik katsayÄ±sÄ± (CV) ile tutarlÄ±lÄ±k deÄŸerlendirmesi
6. Verinin hangi aralÄ±klarda yoÄŸunlaÅŸtÄ±ÄŸÄ± ve uÃ§ deÄŸerler
Ä°statistiksel terimleri anlaÅŸÄ±lÄ±r iÅŸ diline Ã§evir."""

    elif analysis_type == "forecast":
        prompt += """
**GÃ–REV**: Tahminleme sonuÃ§larÄ±nÄ± yorumla ve iÅŸ Ã¶nerileri sun:
1. Mevcut trendin gÃ¼cÃ¼ ve gÃ¼venilirliÄŸi (RÂ² ve gÃ¼ven seviyesi)
2. Lineer ve hareketli ortalama tahminlerinin karÅŸÄ±laÅŸtÄ±rmasÄ±
3. Tahmin edilen deÄŸiÅŸim yÃ¶nÃ¼ ve bÃ¼yÃ¼klÃ¼ÄŸÃ¼
4. En iyi/en kÃ¶tÃ¼ senaryo tahminleri
5. Tahminlerin kÄ±sÄ±tlamalarÄ± ve varsayÄ±mlarÄ±
6. Bu tahminlere gÃ¶re alÄ±nmasÄ± gereken stratejik aksiyonlar
âš ï¸ Basit modeller olduÄŸunu belirt, kesin olmadÄ±ÄŸÄ±nÄ± vurgula."""

    elif analysis_type == "pareto":
        prompt += """
**GÃ–REV**: Pareto/ABC analizini iÅŸ deÄŸeri perspektifinden yorumla:
1. 80/20 kuralÄ±nÄ±n bu veride geÃ§erli olup olmadÄ±ÄŸÄ±
2. A sÄ±nÄ±fÄ± Ã¶ÄŸeler â€” neden en deÄŸerli, nasÄ±l bÃ¼yÃ¼tÃ¼lÃ¼r?
3. B sÄ±nÄ±fÄ± Ã¶ÄŸeler â€” A'ya Ã§Ä±kma potansiyeli olanlar
4. C sÄ±nÄ±fÄ± Ã¶ÄŸeler â€” optimize edilmeli mi, kesilmeli mi?
5. Kaynak daÄŸÄ±lÄ±mÄ± Ã¶nerileri (bÃ¼tÃ§e, zaman, personel)
6. Somut aksiyon planÄ±: "Ã–ÄŸeX'e %Y daha fazla yatÄ±rÄ±m yapÄ±n"
Her Ã¶neriyi katkÄ± yÃ¼zdeleriyle destekle."""

    elif analysis_type == "quality":
        prompt += """
**GÃ–REV**: Veri kalitesi denetim raporunu profesyonelce sun:
1. Genel kalite skoru ve notunun deÄŸerlendirmesi
2. BÃ¼tÃ¼nlÃ¼k â€” eksik verilerin etkisi ve Ã§Ã¶zÃ¼m Ã¶nerileri
3. Teksillik â€” tekrar satÄ±rlarÄ±n neden oluÅŸtuÄŸu ve temizleme stratejisi
4. TutarlÄ±lÄ±k â€” tip uyumsuzluklarÄ± ve dÃ¼zeltme adÄ±mlarÄ±
5. Ã–ncelikli iyileÅŸtirme planÄ± (en kritikten en az kritiÄŸe)
6. Veri kalitesi iyileÅŸtikten sonra beklenen analiz doÄŸruluÄŸu artÄ±ÅŸÄ±
Bu raporu veri mÃ¼hendisliÄŸi ekibine sunulacakmÄ±ÅŸ gibi yaz."""

    else:  # full
        prompt += """
**GÃ–REV**: Bu veri setini kapsamlÄ± analiz et ve aÅŸaÄŸÄ±daki baÅŸlÄ±klarda yanÄ±t ver:

1. **ğŸ“‹ Veri Ã–zeti**: Veri setinin genel yapÄ±sÄ±nÄ± ve kalitesini deÄŸerlendir
2. **ğŸ“Š Temel Bulgular**: En Ã¶nemli sayÄ±sal bulgular (en yÃ¼ksek, en dÃ¼ÅŸÃ¼k, ortalamalar)
3. **ğŸ“ˆ Trend & DeÄŸiÅŸim**: Zaman bazlÄ± veya kategorik deÄŸiÅŸimler
4. **ğŸ” Dikkat Ã‡ekici Noktalar**: AykÄ±rÄ± deÄŸerler, beklenmeyen paternler, eksik veriler
5. **ğŸ’¡ Yorumlar**: Verilerin ne anlama geldiÄŸi hakkÄ±nda profesyonel yorumlar
6. **âœ… Tavsiyeler**: Somut, uygulanabilir Ã¶neriler (en az 3-5 madde)
7. **âš ï¸ Riskler**: Dikkat edilmesi gereken riskler ve uyarÄ±lar
"""
    
    # KullanÄ±cÄ± sorusu varsa ekle
    if question:
        prompt += f"\n**KullanÄ±cÄ±nÄ±n sorusu/talebi**: {question}\nBu soruyu da mutlaka cevapla.\n"
    
    return prompt


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 8. METÄ°N TABANLI DÃ–KÃœMAN ANALÄ°ZÄ° (PDF/DOCX/TXT)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_text_analysis_prompt(
    text: str,
    analysis_type: str = "full",
    question: str = None,
    filename: str = None,
) -> str:
    """
    Metin tabanlÄ± dokÃ¼manlar iÃ§in analiz prompt'u.
    PDF, DOCX, TXT gibi yapÄ±landÄ±rÄ±lmamÄ±ÅŸ veriler iÃ§in.
    """
    # Metin istatistikleri
    word_count = len(text.split())
    line_count = len(text.split('\n'))
    char_count = len(text)
    
    # Anahtar kelimeler (en sÄ±k geÃ§en kelimeler)
    words = re.findall(r'\b[a-zA-ZÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄIÄ°Ã–ÅÃœ]{4,}\b', text.lower())
    word_freq = {}
    stop_words = {'iÃ§in', 'olan', 'olarak', 'veya', 'gibi', 'kadar', 'daha', 'ancak', 'fakat', 'bile'}
    for w in words:
        if w not in stop_words:
            word_freq[w] = word_freq.get(w, 0) + 1
    
    top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:15]
    
    # SayÄ±sal deÄŸerler
    numbers = re.findall(r'\b\d+[.,]?\d*\b', text)
    
    # Metnin kÄ±saltÄ±lmÄ±ÅŸ hali (Ã§ok uzunsa)
    max_text = 8000
    display_text = text[:max_text] + f"\n\n[... {char_count - max_text} karakter daha ...]" if len(text) > max_text else text
    
    prompt = f"""## ğŸ“„ DokÃ¼man Analizi: {filename or 'YÃ¼klenen DokÃ¼man'}

### DokÃ¼man Bilgileri:
- **Kelime sayÄ±sÄ±**: {word_count:,}
- **SatÄ±r sayÄ±sÄ±**: {line_count:,}
- **Karakter sayÄ±sÄ±**: {char_count:,}
- **Ä°Ã§erdiÄŸi sayÄ±sal deÄŸerler**: {len(numbers)} adet
- **Anahtar kelimeler**: {', '.join(f'{w}({c})' for w, c in top_keywords[:10])}

### DokÃ¼man Ä°Ã§eriÄŸi:
```
{display_text}
```

"""
    
    if analysis_type == "summary":
        prompt += "**GÃ–REV**: Bu dokÃ¼manÄ± 5-10 cÃ¼mlede Ã¶zetle. Ana konularÄ± ve en Ã¶nemli bilgileri vurgula."
    elif analysis_type == "recommend":
        prompt += "**GÃ–REV**: Bu dokÃ¼mandaki bilgilere dayanarak somut tavsiyeler sun. Her tavsiyeyi dokÃ¼mandaki verilerle destekle."
    elif analysis_type == "report":
        prompt += """**GÃ–REV**: Bu dokÃ¼man hakkÄ±nda kapsamlÄ± bir rapor oluÅŸtur:
1. YÃ¶netici Ã–zeti
2. Ana Bulgular
3. DetaylÄ± DeÄŸerlendirme
4. Ã–neriler ve Aksiyon Maddeleri
5. Riskler ve UyarÄ±lar"""
    else:
        prompt += """**GÃ–REV**: Bu dokÃ¼manÄ± detaylÄ± analiz et:
1. **ğŸ“‹ Ã–zet**: DokÃ¼manÄ±n ana konusu ve amacÄ±
2. **ğŸ” Temel Bulgular**: Ä°Ã§indeki en Ã¶nemli bilgiler
3. **ğŸ’¡ Yorumlar**: Profesyonel deÄŸerlendirme
4. **âœ… Tavsiyeler**: Somut Ã¶neriler
5. **âš ï¸ Dikkat Edilecekler**: Riskler ve uyarÄ±lar
"""
    
    if question:
        prompt += f"\n**KullanÄ±cÄ±nÄ±n sorusu/talebi**: {question}\nBu soruyu da mutlaka cevapla.\n"
    
    return prompt


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 9. DOÄAL DÄ°L Ä°LE VERÄ° SORGULAMA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def natural_language_query(df: pd.DataFrame, question: str) -> dict:
    """
    DoÄŸal dil sorusunu pandas iÅŸlemine Ã§evir.
    Basit sorgularÄ± otomatik Ã§alÄ±ÅŸtÄ±r.
    """
    q = question.lower()
    result = {"success": False, "answer": None, "query_type": None}
    
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    try:
        # Toplam/Sum
        if re.search(r'toplam|sum|genel\s*toplam', q):
            for col in num_cols:
                if col.lower() in q:
                    result = {
                        "success": True,
                        "answer": f"{col} toplamÄ±: {df[col].sum():,.2f}",
                        "value": float(df[col].sum()),
                        "query_type": "sum"
                    }
                    return result
            # TÃ¼m toplamlar
            sums = {col: round(float(df[col].sum()), 2) for col in num_cols}
            result = {"success": True, "answer": str(sums), "value": sums, "query_type": "sum_all"}
            return result
        
        # Ortalama
        if re.search(r'ortalama|mean|average', q):
            for col in num_cols:
                if col.lower() in q:
                    result = {
                        "success": True,
                        "answer": f"{col} ortalamasÄ±: {df[col].mean():,.2f}",
                        "value": float(df[col].mean()),
                        "query_type": "mean"
                    }
                    return result
            means = {col: round(float(df[col].mean()), 2) for col in num_cols}
            result = {"success": True, "answer": str(means), "value": means, "query_type": "mean_all"}
            return result
        
        # En yÃ¼ksek/max
        if re.search(r'en (yÃ¼ksek|fazla|bÃ¼yÃ¼k|Ã§ok)|max|maksimum', q):
            for col in num_cols:
                if col.lower() in q:
                    idx = df[col].idxmax()
                    row = df.loc[idx]
                    result = {
                        "success": True,
                        "answer": f"{col} en yÃ¼ksek: {row[col]:,.2f}\nSatÄ±r: {row.to_dict()}",
                        "value": float(row[col]),
                        "row": row.to_dict(),
                        "query_type": "max"
                    }
                    return result
        
        # En dÃ¼ÅŸÃ¼k/min
        if re.search(r'en (dÃ¼ÅŸÃ¼k|az|kÃ¼Ã§Ã¼k)|min|minimum', q):
            for col in num_cols:
                if col.lower() in q:
                    idx = df[col].idxmin()
                    row = df.loc[idx]
                    result = {
                        "success": True,
                        "answer": f"{col} en dÃ¼ÅŸÃ¼k: {row[col]:,.2f}\nSatÄ±r: {row.to_dict()}",
                        "value": float(row[col]),
                        "row": row.to_dict(),
                        "query_type": "min"
                    }
                    return result
        
        # SatÄ±r sayÄ±sÄ±
        if re.search(r'kaÃ§\s*(tane|adet|satÄ±r)|satÄ±r\s*sayÄ±sÄ±|count', q):
            result = {
                "success": True,
                "answer": f"Toplam {len(df)} satÄ±r var.",
                "value": len(df),
                "query_type": "count"
            }
            return result
        
        # Filtre (belirli bir deÄŸer arama)
        for col in df.columns:
            col_lower = col.lower()
            if col_lower in q:
                # "X olan satÄ±rlar" gibi
                for val in df[col].unique():
                    val_str = str(val).lower()
                    if val_str in q and len(val_str) > 2:
                        filtered = df[df[col] == val]
                        result = {
                            "success": True,
                            "answer": f"{col}={val} olan {len(filtered)} satÄ±r bulundu.\n{filtered.to_string()[:1000]}",
                            "value": len(filtered),
                            "query_type": "filter"
                        }
                        return result
        
    except Exception as e:
        result["error"] = str(e)
    
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 10. ANALÄ°Z SONUÃ‡LARINI FORMATLA (JSON â†’ LLM-Ready)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def format_analysis_for_llm(
    df: pd.DataFrame = None,
    text: str = None,
    analysis_type: str = "full",
    question: str = None,
    filename: str = None,
) -> str:
    """
    Dosya tipine gÃ¶re uygun analiz prompt'u dÃ¶ndÃ¼r.
    DataFrame varsa tablolu analiz, yoksa metin analizi.
    """
    if df is not None and not df.empty:
        return generate_analysis_prompt(df, analysis_type, question, filename)
    elif text:
        return generate_text_analysis_prompt(text, analysis_type, question, filename)
    else:
        return "Analiz edilecek veri bulunamadÄ±."
