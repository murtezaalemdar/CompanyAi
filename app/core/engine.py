"""Merkezi Ä°ÅŸlem Motoru - TÃ¼m AI sorgu iÅŸlemleri burada koordine edilir

RAG + Web Arama + Semantik HafÄ±za + KiÅŸiselleÅŸtirme
+ Tool Calling + Multi-Step Reasoning + Structured Output
+ Forecasting + KPI Engine + Textile Knowledge + Risk Analysis
"""

from typing import Optional
import re
import structlog

from app.router.router import decide
from app.llm.client import ollama_client
from app.llm.prompts import build_prompt, build_rag_prompt
from app.memory.vector_memory import remember, recall, search_memory

# Few-shot sohbet Ã¶rnekleri
try:
    from app.llm.chat_examples import get_pattern_response, get_few_shot_examples
    CHAT_EXAMPLES_AVAILABLE = True
except ImportError:
    CHAT_EXAMPLES_AVAILABLE = False
    get_pattern_response = lambda q: None
    get_few_shot_examples = lambda q, c=2: ""

# RAG modÃ¼lÃ¼nÃ¼ gÃ¼venli ÅŸekilde import et
try:
    from app.rag.vector_store import search_documents, get_stats as get_rag_stats
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
    search_documents = lambda q, n=3: []
    get_rag_stats = lambda: {"available": False}

# Web arama modÃ¼lÃ¼
try:
    from app.llm.web_search import search_and_summarize
    WEB_SEARCH_AVAILABLE = True
except ImportError:
    WEB_SEARCH_AVAILABLE = False
    search_and_summarize = None

# Export modÃ¼lÃ¼
try:
    from app.core.export_service import detect_export_request, generate_export, FORMAT_LABELS
    EXPORT_AVAILABLE = True
except ImportError:
    EXPORT_AVAILABLE = False
    detect_export_request = lambda q: None

# â”€â”€ YENÄ° MODÃœLLER â”€â”€

# Tool Calling
try:
    from app.core.tool_registry import tool_registry, detect_tool_calls
    TOOLS_AVAILABLE = True
except ImportError:
    TOOLS_AVAILABLE = False
    tool_registry = None

# Multi-step Reasoning
try:
    from app.core.reasoning import needs_multi_step, plan_reasoning_steps, execute_reasoning_chain, format_reasoning_result
    REASONING_AVAILABLE = True
except ImportError:
    REASONING_AVAILABLE = False

# Structured Output
try:
    from app.llm.structured_output import force_json_output, auto_structure, get_schema_for_mode
    STRUCTURED_OUTPUT_AVAILABLE = True
except ImportError:
    STRUCTURED_OUTPUT_AVAILABLE = False

# KPI Engine
try:
    from app.core.kpi_engine import interpret_kpi_value, list_kpis, kpi_scorecard
    KPI_ENGINE_AVAILABLE = True
except ImportError:
    KPI_ENGINE_AVAILABLE = False

# Textile Knowledge
try:
    from app.core.textile_knowledge import get_glossary_term, analyze_waste, get_efficiency_loss_framework
    TEXTILE_AVAILABLE = True
except ImportError:
    TEXTILE_AVAILABLE = False

# Risk Analyzer
try:
    from app.core.risk_analyzer import assess_risk, risk_heatmap, fmea_analysis, build_risk_report_prompt
    RISK_AVAILABLE = True
except ImportError:
    RISK_AVAILABLE = False

# SQL Generator
try:
    from app.core.sql_generator import generate_sql, build_sql_prompt
    SQL_AVAILABLE = True
except ImportError:
    SQL_AVAILABLE = False

logger = structlog.get_logger()


async def process_question(
    question: str, 
    department_override: Optional[str] = None,
    use_rag: bool = True,
    user_name: Optional[str] = None,
    user_department: Optional[str] = None,
    session_history: Optional[list] = None,
    memory_context: Optional[str] = None,
) -> dict:
    """
    Ana soru iÅŸleme fonksiyonu â€” AkÄ±llÄ± Pipeline.
    
    AkÄ±ÅŸ:
    1. Router ile niyet analizi
    2. Semantik hafÄ±za â€” benzer geÃ§miÅŸ konuÅŸmalar
    3. Niyete gÃ¶re bilgi kaynaklarÄ± (RAG / Web / HafÄ±za)
    4. KiÅŸiselleÅŸtirilmiÅŸ prompt oluÅŸturma
    5. LLM yanÄ±t Ã¼retme
    6. Otomatik Ã¶ÄŸrenme (hafÄ±za + web â†’ RAG)
    """
    logger.info("processing_question", question=question[:100])
    
    # 1. AkÄ±llÄ± yÃ¶nlendirme
    context = decide(question)
    intent = context.get("intent", "sohbet")
    needs_web = context.get("needs_web", False)
    
    if department_override:
        context["dept"] = department_override
    
    # KullanÄ±cÄ± bilgisi
    if user_name:
        context["user_name"] = user_name
    if user_department:
        context["user_dept"] = user_department
    
    logger.info("intent_detected", intent=intent, mode=context["mode"], 
                dept=context["dept"], needs_web=needs_web)
    
    # â”€â”€ HIZLI SOHBET YOLU â”€â”€ KalÄ±p eÅŸleÅŸmesi varsa LLM'e gitmeden cevapla
    # Ã–NEMLÄ°: Kimlik/hafÄ±za sorularÄ± ("beni tanÄ±yor musun", "ismimi biliyor musun")
    # pattern matcher'a girmeden LLM'e yÃ¶nlendirilir. Ã‡Ã¼nkÃ¼ pattern matcher kullanÄ±cÄ±
    # ismi hafÄ±zasÄ±na eriÅŸemez, sadece LLM context'inde user_name bilgisi var.
    _is_identity_question = bool(re.search(
        r"(beni\s*tanÄ±|ismimi|adÄ±mÄ±|hatÄ±rlÄ±yor|biliyor\s*mu|kim\s*olduÄŸ|tanÄ±yor\s*mu)",
        question.lower()
    ))
    
    if intent == "sohbet" and CHAT_EXAMPLES_AVAILABLE and not _is_identity_question:
        pattern_answer = get_pattern_response(question)
        if pattern_answer:
            # KiÅŸiselleÅŸtirme ekle
            if user_name and "{name}" not in pattern_answer:
                # Ä°simle hitap et (rastgele, her seferinde deÄŸil)
                import random
                if random.random() < 0.4:
                    first_name = user_name.split()[0] if user_name else ""
                    if first_name:
                        pattern_answer = f"{first_name}, {pattern_answer[0].lower()}{pattern_answer[1:]}"
            
            logger.info("fast_pattern_response", pattern=True)
            remember(question, pattern_answer, context)
            return {
                "answer": pattern_answer,
                "department": context["dept"],
                "mode": "Sohbet",
                "risk": context["risk"],
                "intent": "sohbet",
                "confidence": 0.95,
                "sources": ["KalÄ±p EÅŸleÅŸmesi"],
                "web_searched": False,
            }
    
    # 2. Semantik hafÄ±za â€” soruya EN BENZER geÃ§miÅŸ konuÅŸmalar
    similar_memories = []
    try:
        similar_memories = search_memory(question, limit=3)
        if similar_memories:
            logger.info("similar_memories_found", count=len(similar_memories))
    except Exception as e:
        logger.warning("memory_search_error", error=str(e))
    
    # 3. Bilgi kaynaklarÄ±nÄ± topla
    relevant_docs = []
    web_results = None
    
    # RAG aramasÄ± (sohbet dÄ±ÅŸÄ±nda + sadece SORU varsa)
    # "fabrikamÄ±zÄ±n adÄ± X" gibi bilgi verme cÃ¼mlelerinde RAG Ã§alÄ±ÅŸtÄ±rma
    is_statement = not any(c in question for c in "??") and len(question.split()) < 10
    if use_rag and RAG_AVAILABLE and intent != "sohbet" and not is_statement:
        try:
            raw_docs = search_documents(question, n_results=3)
            # AlakasÄ±z dokÃ¼manlarÄ± filtrele (distance skoru yÃ¼ksekse = alakasÄ±z)
            if raw_docs:
                for doc in raw_docs:
                    score = doc.get('distance', doc.get('score', 999))
                    # ChromaDB distance: dÃ¼ÅŸÃ¼k = benzer. 1.0'dan bÃ¼yÃ¼kse alakasÄ±z.
                    if score < 1.0:
                        relevant_docs.append(doc)
                if relevant_docs:
                    logger.info("rag_documents_found", count=len(relevant_docs))
                else:
                    logger.info("rag_documents_filtered_out", raw=len(raw_docs))
        except Exception as e:
            logger.error("rag_search_error", error=str(e))
    
    # Web aramasÄ±
    web_results = None
    web_rich_data = None
    if WEB_SEARCH_AVAILABLE and search_and_summarize:
        should_search_web = (
            needs_web or 
            (intent == "bilgi") or
            (intent == "iÅŸ" and not relevant_docs)
        )
        if should_search_web:
            try:
                web_results, web_rich_data = await search_and_summarize(question)
                if web_results:
                    logger.info("web_search_results_found", has_rich_data=web_rich_data is not None)
            except Exception as e:
                logger.warning("web_search_error", error=str(e))
    
    # 4. Prompt oluÅŸtur (KISA tut â€” Mistral 7B CPU)
    if relevant_docs:
        system_prompt, user_prompt = build_rag_prompt(question, context, relevant_docs)
    else:
        system_prompt, user_prompt = build_prompt(question, context)
    
    # KiÅŸiselleÅŸtirme â€” kullanÄ±cÄ± kimliÄŸi (LLM bunun Ã¼zerinden hitap eder)
    if user_name:
        system_prompt += (f"\n\nÃ–NEMLÄ°: Åu an seninle konuÅŸan kullanÄ±cÄ±nÄ±n adÄ± '{user_name}'.\n"
                         f"KullanÄ±cÄ± sana adÄ±nÄ± veya kim olduÄŸunu sorarsa kesinlikle '{user_name}' olarak cevap ver.\n"
                         f"Her zaman kullanÄ±cÄ±ya '{user_name.split()[0]}' diye hitap edebilirsin.")    
    # KalÄ±cÄ± hafÄ±za baÄŸlamÄ± â€” PostgreSQL'den gelen kullanÄ±cÄ± bilgileri + geÃ§miÅŸ
    if memory_context:
        system_prompt += f"\n\nKullanÄ±cÄ± HafÄ±zasÄ± (geÃ§miÅŸ konusmalardan Ã¶ÄŸrenilen bilgiler):\n{memory_context}"    
    
    # KullanÄ±cÄ± kimliÄŸi tekrar (LLM recency bias â€” son gelen bilgi gÃ¼Ã§lÃ¼)
    if user_name:
        system_prompt += f"\n\nHATIRLATMA: KullanÄ±cÄ±nÄ±n adÄ± kesinlikle '{user_name}'. GeÃ§miÅŸ konuÅŸmalardaki farklÄ± isimler BAÅKA kiÅŸilere aittir."
    
    # Web sonuÃ§larÄ±nÄ± prompt'a ekle
    if web_results:
        system_prompt += f"\n\nAÅŸaÄŸÄ±da internetten bulunan gÃ¼ncel bilgiler var. Bu bilgileri kullanarak yanÄ±t ver:\n{web_results[:1500]}"
    
    # Chat history â€” system prompt'a DEÄÄ°L, client'a ayrÄ± gÃ¶nder
    # Her intent'te (sohbet dahil) geÃ§miÅŸi gÃ¶nder â€” "biraz daha basit anlat" gibi takip sorularÄ± iÃ§in
    chat_history = []
    if session_history:
        chat_history = session_history[-5:]
    
    # 5. LLM'e sor
    try:
        if await ollama_client.is_available():
            temp = 0.3
            if context.get("mode") in ["Sohbet", "Bilgi", "Ã–neri", "Beyin FÄ±rtÄ±nasÄ±"]:
                temp = 0.7
                
            llm_answer = await ollama_client.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=temp,
                max_tokens=512,
                history=chat_history if chat_history else None,
            )
        else:
            logger.warning("ollama_not_available", using_fallback=True)
            llm_answer = f"[Sistem Notu: LLM ÅŸu an eriÅŸilemez] Soru alÄ±ndÄ±: {question}"
    except Exception as e:
        logger.error("llm_error", error=str(e))
        llm_answer = f"[Hata] LLM yanÄ±t Ã¼retemedi: {str(e)}"
    
    # â”€â”€ 5b. TOOL CALLING â€” LLM Ã§Ä±ktÄ±sÄ±nda tool Ã§aÄŸrÄ±sÄ± var mÄ±? â”€â”€
    tool_results = []
    if TOOLS_AVAILABLE and llm_answer and not llm_answer.startswith("[Hata]"):
        try:
            detected_tools = detect_tool_calls(llm_answer)
            if detected_tools:
                for tool_call in detected_tools[:3]:  # Max 3 tool
                    tool_name = tool_call.get("tool", "")
                    tool_params = tool_call.get("params", {})
                    result = tool_registry.execute(tool_name, tool_params)
                    if result and not result.get("error"):
                        tool_results.append({
                            "tool": tool_name,
                            "result": result,
                        })
                if tool_results:
                    # Tool sonuÃ§larÄ±nÄ± cevaba ekle
                    tool_text = "\n\n---\nğŸ“Š **Hesaplama SonuÃ§larÄ±:**\n"
                    for tr in tool_results:
                        tool_text += f"\n**{tr['tool']}**: {_format_tool_result(tr['result'])}"
                    llm_answer += tool_text
                    logger.info("tools_executed", count=len(tool_results))
        except Exception as e:
            logger.warning("tool_execution_error", error=str(e))
    
    # â”€â”€ 5c. STRUCTURED OUTPUT â€” Analiz/Rapor modunda JSON yapÄ±landÄ±rma â”€â”€
    structured_data = None
    if STRUCTURED_OUTPUT_AVAILABLE and context.get("mode") in ["Analiz", "Rapor", "Acil"]:
        try:
            structured_data = auto_structure(llm_answer)
            if structured_data and structured_data.get("sections"):
                logger.info("output_structured", sections=len(structured_data.get("sections", [])))
        except Exception as e:
            logger.debug("structured_output_skipped", error=str(e))
    
    # 6. SonuÃ§
    sources = []
    if relevant_docs:
        sources.extend([doc.get("source") for doc in relevant_docs])
    if web_results:
        sources.append("Ä°nternet AramasÄ±")
    
    # Rich data listesi
    rich_data = web_rich_data if web_rich_data else []
    if not isinstance(rich_data, list):
        rich_data = [rich_data]
    
    # 6b. Export talebi varsa dosya Ã¼ret
    export_format = None
    if EXPORT_AVAILABLE:
        export_format = detect_export_request(question)
    
    if export_format and llm_answer and not llm_answer.startswith("[Hata]"):
        try:
            # BaÅŸlÄ±ÄŸÄ± sorudan Ã§Ä±kar
            export_title = question.strip()[:60].rstrip("?.!")
            export_result = generate_export(llm_answer, export_format, export_title)
            if export_result:
                fmt_info = FORMAT_LABELS.get(export_format, {})
                rich_data.append({
                    "type": "export",
                    "file_id": export_result["file_id"],
                    "filename": export_result["filename"],
                    "format": export_format,
                    "format_label": fmt_info.get("label", export_format),
                    "format_icon": fmt_info.get("icon", "ğŸ“„"),
                    "download_url": f"/api/export/download/{export_result['file_id']}",
                })
                logger.info("export_auto_generated", format=export_format, file_id=export_result["file_id"])
        except Exception as e:
            logger.warning("export_auto_failed", error=str(e))
    
    result = {
        "answer": llm_answer,
        "department": context["dept"],
        "mode": context["mode"],
        "risk": context["risk"],
        "intent": intent,
        "confidence": 0.85 if not relevant_docs else 0.92,
        "sources": sources,
        "web_searched": web_results is not None,
        "rich_data": rich_data if rich_data else None,
        "tool_results": tool_results if tool_results else None,
        "structured_data": structured_data,
    }
    
    # 7. HafÄ±zaya kaydet (Ã¶ÄŸrenme)
    remember(question, llm_answer, context)
    
    # 8. Otomatik Ã¶ÄŸrenme: Web sonuÃ§larÄ±nÄ± RAG'a kaydet
    if web_results and RAG_AVAILABLE:
        try:
            _auto_learn_from_web(question, web_results)
        except Exception as e:
            logger.warning("auto_learn_failed", error=str(e))
    
    logger.info("question_processed", 
                intent=intent,
                department=context["dept"], 
                rag_used=bool(relevant_docs),
                web_used=web_results is not None,
                memories_used=len(similar_memories))
    
    return result


def _auto_learn_from_web(question: str, web_text: str):
    """Web'den bulunan bilgiyi RAG'a kaydet â€” bir sonraki sefere daha hÄ±zlÄ±"""
    try:
        from app.rag.vector_store import add_document
        
        # Web sonuÃ§larÄ±nÄ± temizle ve kaydet
        clean_text = web_text.replace("## ğŸŒ Ä°nternet AramasÄ± SonuÃ§larÄ±:\n", "").strip()
        if len(clean_text) > 50:  # Ã‡ok kÄ±sa ise kaydetme
            add_document(
                content=clean_text,
                source=f"web_search: {question[:80]}",
                metadata={
                    "type": "web_learned",
                    "original_query": question,
                    "auto_learned": True,
                }
            )
            logger.info("auto_learned_from_web", query=question[:60])
    except (ImportError, Exception) as e:
        logger.debug("auto_learn_skipped", reason=str(e))


async def get_system_status() -> dict:
    """Sistem durumu Ã¶zeti"""
    llm_available = await ollama_client.is_available()
    models = await ollama_client.get_models() if llm_available else []
    memory_size = len(recall())
    
    # RAG durumu
    rag_stats = get_rag_stats() if RAG_AVAILABLE else {"available": False}
    
    return {
        "llm_available": llm_available,
        "llm_model": ollama_client.model,
        "available_models": models,
        "memory_entries": memory_size,
        "rag": rag_stats,
        "modules": {
            "tools": TOOLS_AVAILABLE,
            "reasoning": REASONING_AVAILABLE,
            "structured_output": STRUCTURED_OUTPUT_AVAILABLE,
            "kpi_engine": KPI_ENGINE_AVAILABLE,
            "textile_knowledge": TEXTILE_AVAILABLE,
            "risk_analyzer": RISK_AVAILABLE,
            "sql_generator": SQL_AVAILABLE,
            "export": EXPORT_AVAILABLE,
            "web_search": WEB_SEARCH_AVAILABLE,
        },
    }


def _format_tool_result(result: dict) -> str:
    """Tool sonucunu kullanÄ±cÄ± dostu formata Ã§evir."""
    if not result:
        return ""
    
    parts = []
    for key, value in result.items():
        if key in ("error", "tool"):
            continue
        if isinstance(value, float):
            parts.append(f"{key}: {value:.2f}")
        elif isinstance(value, dict):
            inner = ", ".join(f"{k}: {v}" for k, v in value.items())
            parts.append(f"{key}: {{{inner}}}")
        elif isinstance(value, list):
            parts.append(f"{key}: {', '.join(str(v) for v in value[:5])}")
        else:
            parts.append(f"{key}: {value}")
    
    return " | ".join(parts) if parts else str(result)