"""Merkezi Ä°ÅŸlem Motoru - TÃ¼m AI sorgu iÅŸlemleri burada koordine edilir

RAG + Web Arama + Semantik HafÄ±za + KiÅŸiselleÅŸtirme
+ Tool Calling + Multi-Step Reasoning + Structured Output
+ Forecasting + KPI Engine + Textile Knowledge + Risk Analysis
+ Module Synapse Network â€” ModÃ¼ller ArasÄ± Ã–z-Ã–ÄŸrenen Zeka AÄŸÄ±
+ Enterprise Platform: Event Bus, Orchestrator, Policy Engine,
  Observability 2.0, Security Layer

v5.9.0 DEÄÄ°ÅÄ°KLÄ°KLER (Ã–NEMLÄ° â€” Ä°ÅE YARADI):
- SÄ±caklÄ±k: Bilgi/Ã–neri 0.7 â†’ 0.4 (halÃ¼sinasyon azalmasÄ±)
- max_tokens: Analiz/Rapor 1024 â†’ 2048 (kesilme sorunu Ã§Ã¶zÃ¼ldÃ¼)
- KullanÄ±cÄ± kimliÄŸi: 3x tekrar â†’ 1 satÄ±r (token tasarrufu)
- Multi-perspective LLM Ã§aÄŸrÄ±sÄ±: Devre dÄ±ÅŸÄ± (10-30 sn tasarruf)
- Post-processing: 12 bÃ¶lÃ¼m cevaptan kaldÄ±rÄ±ldÄ± â†’ JSON metadata'da
  Korunan: confidence badge, sayÄ±sal doÄŸrulama, OOD, politika, governance alert

TODO GELÄ°ÅTÄ°RÄ°LMELÄ°:
- [ ] Self-correction loop token maliyeti yÃ¼ksek â†’ sadece Analiz/Rapor'da aktif tut
- [ ] KaldÄ±rÄ±lan 12 post-processing bÃ¶lÃ¼mÃ¼ iÃ§in frontend UI bileÅŸenleri yok
- [ ] "DetaylÄ±/KÄ±sa cevap" kullanÄ±cÄ± tercihi ekle (post-processing gÃ¶sterimi iÃ§in)
- [ ] router.py regex pattern'leri Ã§ok geniÅŸ â†’ false positive azalt
- [ ] Context window: 8Kâ†’12K test et (TPS etkisini Ã¶lÃ§)
- [ ] Reflection LLM Ã§aÄŸrÄ±sÄ± (60-75% gÃ¼ven): token bÃ¼tÃ§esinin %20'sini aÅŸmasÄ±n

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  CompanyAI v5.5.0 â€” 49 AI ModÃ¼l + 5 Enterprise Platform KatmanÄ±
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  #  ModÃ¼l                        Puan   SatÄ±r   SÄ±nÄ±f  Not
  â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€   â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1  Tool Registry                 88     858      2     ReAct pattern, 8+ araÃ§, Ollama function calling
  2  Reasoning                     86     650+     3     CoT zincir, varsayÄ±m Ã§Ä±karma, argÃ¼man gÃ¼cÃ¼, Ã§eliÅŸki tespiti
  3  Structured Output             86     550+     2     JSON Schema Draft-7, oto-dÃ¼zeltme, MD/CSV/YAML, ÅŸema Ã¶nbellek
  4  KPI Engine                    85     442      0     50+ KPI, Balanced Scorecard, sektÃ¶rel benchmark
  5  Textile Knowledge             80     373      0     200+ terim, fire analizi, kalite kontrol, kapasite
  6  Risk Analyzer                 82     339      1     FMEA, 5Ã—5 matris, what-if, maliyet kÄ±rÄ±lÄ±m
  7  Reflection                    90     673      0     5 kriter kalite, hallucination algÄ±lama, auto-retry, sayÄ±sal doÄŸrulama
  8  Agent Pipeline                78     554      2     6 uzman ajan, sequential + parallel pipeline
  9  Scenario Engine               86     600+     1     Tornado/hassasiyet, baÅŸabaÅŸ, Monte Carlo, stres testi, Ã§ok deÄŸiÅŸkenli
  10 Monte Carlo                   80     264      0     N-iterasyon simÃ¼lasyon, VaR, CI, volatilite
  11 Decision Ranking              76     261      2     ROIÃ—RiskÃ—Strateji puanlama, priority bands
  12 Governance                    92     643      8     Bias algÄ±lama, drift izleme, 12 politika, hash chain audit
  13 Experiment Layer              86     700+     5     Welch t-test, ki-kare, Bayesian A/B, Ã§ok deÄŸiÅŸkenli, power analysis
  14 Graph Impact                  86     650+     6     PageRank, kaskad yayÄ±lÄ±m, what-if, dÃ¶ngÃ¼ tespiti, hassasiyet
  15 ARIMA Forecasting             89     844      0     ARIMA/SARIMA, Holt-Winters, SES, anomaly detection
  16 SQL Generator                 77     409      0     DoÄŸal dilâ†’SQL, ÅŸema algÄ±lama, feature engineering
  17 Export Service                83     683      1     Excel/PDF/PPTX/Word/CSV, auto-cleanup, format algÄ±lama
  18 Web Search                    79     515      0     SerpAPI+Google+DuckDuckGo fallback, HTML scraping
  19 Model Registry                86     500+     3     YaÅŸam dÃ¶ngÃ¼sÃ¼, A/B karÅŸÄ±laÅŸtÄ±rma, regresyon tespiti, baÄŸÄ±mlÄ±lÄ±k grafÄ±
  20 Data Versioning               86     550+     3     Diff motoru, branching, rollback, Ã§akÄ±ÅŸma tespiti
  21 Human-in-the-Loop             81     287      2     YÃ¼ksek riskli onay kuyruÄŸu, feedback Ã¶ÄŸrenme
  22 Monitoring                    84     586      5     GPU/API/error izleme, z-score anomaly, SLA monitoring
  23 Textile Vision                86     600+     2     Batch analiz, renk tutarlÄ±lÄ±k (Delta-E), desen tekrar, trend takip
  24 Explainability                91     1209     3     XAI v4, faktÃ¶r skoru, TF-IDF, PostgreSQL kayÄ±t, kalibrasyon
  25 Bottleneck Engine             77     421      2     SÃ¼reÃ§ darboÄŸaz, kaynak haritasÄ±, kuyruk analizi, iyileÅŸtirme
  26 Executive Health              82     688      2     Åirket saÄŸlÄ±k skoru 0-100, 4 boyut, A+â†’F grade
  27 OCR Engine                    76     450      0     EasyOCR (TR+EN), etiket/fatura/tablo parse, PDF OCR
  28 Numerical Validation          86     480+     1     Birim farkÄ±ndalÄ±klÄ± eÅŸleÅŸtirme, yÃ¼zde tutarlÄ±lÄ±k, trend doÄŸrulama
  29 Meta Learning                 93     824      7     Strategy profiling, knowledge gap, quality trend, failure mining
  30 Self Improvement              94     1042     12    ThresholdOptimizer, RAGTuner, PromptEvolver, auto-rollback
  31 Multi-Agent Debate            92     1098     11    6 perspektif ajan, round-robin, consensus, sentez
  32 Causal Inference              91     1208     18    5 Whys, Ishikawa, DAG, counterfactual, intervention analizi
  33 Strategic Planner             90     1171     19    PESTEL, Porter 5F, SMART, OKR, strateji formÃ¼lasyonu
  34 Executive Intelligence        89     1008     19    CEO brifing, KPI korelasyon, RAPID/RACI, board raporu
  35 Knowledge Graph               88     944      13    Entity/relation extraction, BFS, kÃ¼meleme, baÄŸlam zenginleÅŸtirme
  36 Decision Gatekeeper           87     635      12    PASS/WARN/BLOCK/ESCALATE, eskalasyon, risk sinyal toplama
  37 Uncertainty Quantification    85     404      9     Epistemik/Aleatoric, ensemble gÃ¼ven skoru, hata payÄ± hesaplama
  38 Chart Engine                  78     350+     0     Matplotlib grafik Ã¼retimi, bar/line/pie, trend Ã§izimi
  39 Decision Quality Score        88     500+     3     8-sinyal birleÅŸik kalite skoru, gÃ¼ven bandÄ±, yÃ¶netim Ã¶zeti
  40 KPI Impact Mapping            87     550+     2     28 anahtar kelimeâ†’KPI eÅŸleme, domino etki, finansal tahmin
  41 Decision Memory               86     500+     3     Karar hafÄ±zasÄ±, benzer karar arama, sonuÃ§ takibi, doÄŸruluk raporu
  42 Executive Digest              85     400+     2     5 madde + Risk + FÄ±rsat + Net Ã–neri format, Ã¶ncelik sÄ±nÄ±flandÄ±rma
  43 OOD Detector                  86     480+     2     Semantik yenilik, alan uyumu, yapÄ±sal normallik, OOD skor
  44 Module Synapse Network         90     700+     3     NÃ¶ral sinaps iletiÅŸim, Hebbian Ã¶ÄŸrenme, kaskad tetikleme, 40+ sinaps

  â”€â”€â”€ GENEL ORTALAMA: 85.3 / 100 â”€â”€â”€

  Toplam: ~27.700+ satÄ±r AI kodu, ~195+ sÄ±nÄ±f, ~835+ fonksiyon/metot
  En gÃ¼Ã§lÃ¼: Self Improvement (94), Meta Learning (93), Governance (92),
            Multi-Agent Debate (92), Causal Inference (91), Explainability (91)
  GeliÅŸime aÃ§Ä±k: Decision Ranking (76), OCR Engine (76)

  Son gÃ¼ncelleme: v5.4.0
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

from typing import Optional
import re
import time
import structlog

from app.router.router import decide, async_decide
from app.llm.client import ollama_client
from app.llm.prompts import build_prompt, build_rag_prompt
from app.memory.vector_memory import remember, recall, search_memory

# Few-shot sohbet Ã¶rnekleri
try:
    from app.llm.chat_examples import get_pattern_response, get_few_shot_examples
    CHAT_EXAMPLES_AVAILABLE = True
except ImportError:
    CHAT_EXAMPLES_AVAILABLE = False
    get_pattern_response = lambda q: None
    get_few_shot_examples = lambda q, c=2: ""

# RAG modÃ¼lÃ¼nÃ¼ gÃ¼venli ÅŸekilde import et
try:
    from app.rag.vector_store import search_documents, agentic_search, get_stats as get_rag_stats, add_document as rag_add_document
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
    search_documents = lambda q, n=3: []
    agentic_search = lambda q, n=5, d=None: []
    get_rag_stats = lambda: {"available": False}
    rag_add_document = lambda *a, **k: False

# Bilgi Ã‡Ä±karma ve Otomatik Ã–ÄŸrenme Motoru
try:
    from app.core.knowledge_extractor import learn_from_conversation, learn_from_user_message
    KNOWLEDGE_EXTRACTOR_AVAILABLE = True
except ImportError:
    KNOWLEDGE_EXTRACTOR_AVAILABLE = False
    learn_from_conversation = lambda **k: {"user_learned": False, "ai_learned": False}
    learn_from_user_message = lambda *a, **k: False

# Web arama modÃ¼lÃ¼
try:
    from app.llm.web_search import search_and_summarize
    WEB_SEARCH_AVAILABLE = True
except ImportError:
    WEB_SEARCH_AVAILABLE = False
    search_and_summarize = None

# Export modÃ¼lÃ¼
try:
    from app.core.export_service import detect_export_request, generate_export, FORMAT_LABELS
    EXPORT_AVAILABLE = True
except ImportError:
    EXPORT_AVAILABLE = False
    detect_export_request = lambda q: None

# â”€â”€ YENÄ° MODÃœLLER â”€â”€

# Tool Calling
try:
    from app.core.tool_registry import tool_registry, detect_tool_calls, detect_tool_chain
    TOOLS_AVAILABLE = True
except ImportError:
    TOOLS_AVAILABLE = False
    tool_registry = None

# Multi-step Reasoning
try:
    from app.core.reasoning import needs_multi_step, plan_reasoning_steps, execute_reasoning_chain, format_reasoning_result
    REASONING_AVAILABLE = True
except ImportError:
    REASONING_AVAILABLE = False

# Structured Output
try:
    from app.llm.structured_output import force_json_output, auto_structure, get_schema_for_mode
    STRUCTURED_OUTPUT_AVAILABLE = True
except ImportError:
    STRUCTURED_OUTPUT_AVAILABLE = False

# KPI Engine
try:
    from app.core.kpi_engine import interpret_kpi_value, list_kpis, kpi_scorecard
    KPI_ENGINE_AVAILABLE = True
except ImportError:
    KPI_ENGINE_AVAILABLE = False

# Textile Knowledge
try:
    from app.core.textile_knowledge import get_glossary_term, analyze_waste, get_efficiency_loss_framework
    TEXTILE_AVAILABLE = True
except ImportError:
    TEXTILE_AVAILABLE = False

# Risk Analyzer
try:
    from app.core.risk_analyzer import assess_risk, risk_heatmap, fmea_analysis, build_risk_report_prompt
    RISK_AVAILABLE = True
except ImportError:
    RISK_AVAILABLE = False

# Reflection Layer (Self-Evaluation)
try:
    from app.core.reflection import quick_evaluate, build_retry_prompt, format_reflection_footer, format_confidence_badge, self_correction_loop
    REFLECTION_AVAILABLE = True
except ImportError:
    REFLECTION_AVAILABLE = False

# SayÄ±sal DoÄŸrulama Motoru (v4.4.0 â†’ v5.2.0 standalone)
try:
    from app.core.numerical_validation import validate_numbers_against_source
    NUMERICAL_VALIDATION_AVAILABLE = True
except ImportError:
    try:
        from app.core.reflection import validate_numbers_against_source
        NUMERICAL_VALIDATION_AVAILABLE = True
    except ImportError:
        NUMERICAL_VALIDATION_AVAILABLE = False
        validate_numbers_against_source = None

# OCR Motor (v4.4.0)
try:
    from app.core.ocr_engine import extract_text_from_image, extract_text_from_image_bytes, EASYOCR_AVAILABLE
    OCR_AVAILABLE = EASYOCR_AVAILABLE
except ImportError:
    OCR_AVAILABLE = False

# Multi-Agent Pipeline
try:
    from app.core.agent_pipeline import should_use_pipeline, execute_agent_pipeline, format_pipeline_summary
    AGENT_PIPELINE_AVAILABLE = True
except ImportError:
    AGENT_PIPELINE_AVAILABLE = False

# Scenario Engine
try:
    from app.core.scenario_engine import simulate_scenarios, project_financial_impact, format_scenario_table, format_financial_impact
    SCENARIO_AVAILABLE = True
except ImportError:
    SCENARIO_AVAILABLE = False

# Monte Carlo Risk Engine
try:
    from app.core.monte_carlo import monte_carlo_simulate, format_monte_carlo_table
    MONTE_CARLO_AVAILABLE = True
except ImportError:
    MONTE_CARLO_AVAILABLE = False

# Decision Impact Ranking
try:
    from app.core.decision_ranking import rank_decisions, extract_decisions_from_llm, format_ranking_table
    DECISION_RANKING_AVAILABLE = True
except ImportError:
    DECISION_RANKING_AVAILABLE = False

# AI Governance
try:
    from app.core.governance import governance_engine, format_governance_alert
    GOVERNANCE_AVAILABLE = True
except ImportError:
    GOVERNANCE_AVAILABLE = False
    governance_engine = None

# Experiment Layer (A/B + Cross-Dept)
try:
    from app.core.experiment_layer import simulate_ab_strategy, analyze_cross_dept_impact, format_ab_result, format_cross_dept_impact
    EXPERIMENT_AVAILABLE = True
except ImportError:
    EXPERIMENT_AVAILABLE = False

# Graph Impact Mapping
try:
    from app.core.graph_impact import auto_graph_analysis, format_graph_impact
    GRAPH_IMPACT_AVAILABLE = True
except ImportError:
    GRAPH_IMPACT_AVAILABLE = False

# ARIMA / SARIMA Forecasting (v3.3.0)
try:
    from app.core.forecasting import STATSMODELS_AVAILABLE as ARIMA_AVAILABLE
except ImportError:
    ARIMA_AVAILABLE = False

# Bottleneck Engine (v3.8.0)
try:
    from app.core.bottleneck_engine import analyze_from_data as bottleneck_analyze, format_bottleneck_report
    BOTTLENECK_AVAILABLE = True
except ImportError:
    BOTTLENECK_AVAILABLE = False

# Executive Health Index (v3.8.0)
try:
    from app.core.executive_health import calculate_health_index, format_health_dashboard
    EXECUTIVE_HEALTH_AVAILABLE = True
except ImportError:
    EXECUTIVE_HEALTH_AVAILABLE = False

# Insight Engine â€” Otomatik Ä°Ã§gÃ¶rÃ¼ (v3.9.0)
try:
    from app.core.insight_engine import extract_insights, format_insight_report, insights_to_dict
    INSIGHT_ENGINE_AVAILABLE = True
except ImportError:
    INSIGHT_ENGINE_AVAILABLE = False

# SQL Generator
try:
    from app.core.sql_generator import generate_sql, build_sql_prompt
    SQL_AVAILABLE = True
except ImportError:
    SQL_AVAILABLE = False

# Model Registry (v3.4.0)
try:
    from app.core.model_registry import model_registry
    MODEL_REGISTRY_AVAILABLE = True
except ImportError:
    MODEL_REGISTRY_AVAILABLE = False
    model_registry = None

# Data Versioning (v3.4.0)
try:
    from app.core.data_versioning import data_version_manager
    DATA_VERSIONING_AVAILABLE = True
except ImportError:
    DATA_VERSIONING_AVAILABLE = False
    data_version_manager = None

# Human-in-the-Loop (v3.4.0)
try:
    from app.core.hitl import hitl_manager
    HITL_AVAILABLE = True
except ImportError:
    HITL_AVAILABLE = False
    hitl_manager = None

# Enhanced Monitoring (v3.4.0)
try:
    from app.core.monitoring import metrics_collector, alert_manager, get_full_telemetry, calculate_health_score
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False
    metrics_collector = None

# Textile Vision (v3.4.0)
try:
    from app.core.textile_vision import analyze_colors, analyze_pattern, compare_images, generate_quality_report, get_textile_vision_capabilities
    TEXTILE_VISION_AVAILABLE = True
except ImportError:
    TEXTILE_VISION_AVAILABLE = False

# Explainability / XAI (v3.4.0)
try:
    from app.core.explainability import decision_explainer
    XAI_AVAILABLE = True
except ImportError:
    XAI_AVAILABLE = False
    decision_explainer = None

# Token Budget Manager (v4.3.0)
try:
    from app.core.token_budget import truncate_to_budget, smart_truncate_all, estimate_tokens
    TOKEN_BUDGET_AVAILABLE = True
except ImportError:
    TOKEN_BUDGET_AVAILABLE = False
    truncate_to_budget = lambda text, section, **kw: text
    smart_truncate_all = None

# Meta Learning Engine (v4.6.0)
try:
    from app.core.meta_learning import meta_learning_engine, record_query_outcome
    META_LEARNING_AVAILABLE = True
except ImportError:
    META_LEARNING_AVAILABLE = False
    meta_learning_engine = None
    record_query_outcome = lambda **k: {}

# Self-Improvement Loop (v4.6.0)
try:
    from app.core.self_improvement import self_improvement_loop, on_query_completed as si_on_query_completed, get_threshold_override
    SELF_IMPROVEMENT_AVAILABLE = True
except ImportError:
    SELF_IMPROVEMENT_AVAILABLE = False
    self_improvement_loop = None
    si_on_query_completed = lambda **k: None
    get_threshold_override = lambda d, m: None

# Multi-Agent Debate (v4.7.0)
try:
    from app.core.multi_agent_debate import debate_engine, check_debate_trigger, get_debate_dashboard
    MULTI_AGENT_DEBATE_AVAILABLE = True
except ImportError:
    MULTI_AGENT_DEBATE_AVAILABLE = False
    debate_engine = None
    check_debate_trigger = lambda **k: (False, "unavailable")
    get_debate_dashboard = lambda: {"available": False}

# Causal Inference Engine (v4.7.0)
try:
    from app.core.causal_inference import causal_engine, check_causal_trigger, get_causal_dashboard
    CAUSAL_INFERENCE_AVAILABLE = True
except ImportError:
    CAUSAL_INFERENCE_AVAILABLE = False
    causal_engine = None
    check_causal_trigger = lambda **k: (False, "unavailable")
    get_causal_dashboard = lambda: {"available": False}

# Strategic Planner (v5.0.0)
try:
    from app.core.strategic_planner import strategic_planner, check_strategic_trigger, get_strategic_dashboard
    STRATEGIC_PLANNER_AVAILABLE = True
except ImportError:
    STRATEGIC_PLANNER_AVAILABLE = False
    strategic_planner = None
    check_strategic_trigger = lambda **k: (False, "unavailable")
    get_strategic_dashboard = lambda: {"available": False}

# Executive Intelligence (v5.0.0)
try:
    from app.core.executive_intelligence import executive_intelligence, check_executive_trigger, get_executive_dashboard
    EXECUTIVE_INTELLIGENCE_AVAILABLE = True
except ImportError:
    EXECUTIVE_INTELLIGENCE_AVAILABLE = False
    executive_intelligence = None
    check_executive_trigger = lambda **k: (False, "unavailable")
    get_executive_dashboard = lambda: {"available": False}

# Knowledge Graph (v5.0.0)
try:
    from app.core.knowledge_graph import knowledge_graph, check_kg_trigger, get_kg_dashboard
    KNOWLEDGE_GRAPH_AVAILABLE = True
except ImportError:
    KNOWLEDGE_GRAPH_AVAILABLE = False
    knowledge_graph = None
    check_kg_trigger = lambda **k: (False, "unavailable")
    get_kg_dashboard = lambda: {"available": False}

# Decision Risk Gatekeeper (v5.1.0)
try:
    from app.core.decision_gatekeeper import decision_gatekeeper, check_gate_trigger, get_gate_dashboard
    DECISION_GATEKEEPER_AVAILABLE = True
except ImportError:
    DECISION_GATEKEEPER_AVAILABLE = False
    decision_gatekeeper = None
    check_gate_trigger = lambda **k: (False, "unavailable")
    get_gate_dashboard = lambda: {"available": False}

# Uncertainty Quantification (v5.1.0)
try:
    from app.core.uncertainty_quantification import uncertainty_quantifier, check_uncertainty_trigger, get_uncertainty_dashboard
    UNCERTAINTY_AVAILABLE = True
except ImportError:
    UNCERTAINTY_AVAILABLE = False
    uncertainty_quantifier = None
    check_uncertainty_trigger = lambda **k: (False, "unavailable")
    get_uncertainty_dashboard = lambda: {"available": False}

# Decision Quality Score (v5.3.0)
try:
    from app.core.decision_quality import evaluate_decision_quality, format_quality_score, format_quality_badge
    DECISION_QUALITY_AVAILABLE = True
except ImportError:
    DECISION_QUALITY_AVAILABLE = False
    evaluate_decision_quality = None

# KPI Impact Mapping (v5.3.0)
try:
    from app.core.kpi_impact import analyze_kpi_impact, format_kpi_impact, format_kpi_impact_brief
    KPI_IMPACT_AVAILABLE = True
except ImportError:
    KPI_IMPACT_AVAILABLE = False
    analyze_kpi_impact = None

# Decision Memory (v5.3.0)
try:
    from app.core.decision_memory import store_decision, find_similar_decisions, format_similar_decisions
    DECISION_MEMORY_AVAILABLE = True
except ImportError:
    DECISION_MEMORY_AVAILABLE = False
    store_decision = None
    find_similar_decisions = None

# Executive Digest (v5.3.0)
try:
    from app.core.executive_digest import generate_executive_digest, format_executive_digest, format_digest_micro
    EXECUTIVE_DIGEST_AVAILABLE = True
except ImportError:
    EXECUTIVE_DIGEST_AVAILABLE = False
    generate_executive_digest = None

# OOD Detector (v5.3.0)
try:
    from app.core.ood_detector import check_ood, format_ood_warning, format_ood_badge
    OOD_DETECTOR_AVAILABLE = True
except ImportError:
    OOD_DETECTOR_AVAILABLE = False
    check_ood = None

# Module Synapse Network â€” ModÃ¼ller ArasÄ± Ã–z-Ã–ÄŸrenen Zeka AÄŸÄ± (v5.4.0)
try:
    from app.core.module_synapse import (
        create_pipeline_context, emit_signal, gather_module_inputs,
        check_cascades, finalize_context, format_signal_trace,
        format_network_summary, learn_from_outcome as synapse_learn
    )
    SYNAPSE_AVAILABLE = True
except ImportError:
    SYNAPSE_AVAILABLE = False
    create_pipeline_context = None

# â”€â”€ v5.5.0 Enterprise Platform KatmanlarÄ± â”€â”€

# Event Bus â€” Event-Driven Architecture + Event Sourcing
try:
    from app.core.event_bus import event_bus
    EVENT_BUS_AVAILABLE = True
except ImportError:
    EVENT_BUS_AVAILABLE = False
    event_bus = None

# Workflow Orchestrator â€” DAG-based durable workflows
try:
    from app.core.orchestrator import workflow_engine
    ORCHESTRATOR_AVAILABLE = True
except ImportError:
    ORCHESTRATOR_AVAILABLE = False
    workflow_engine = None

# Policy Engine â€” OPA-style JSON rule engine
try:
    from app.core.policy_engine import policy_engine as enterprise_policy_engine
    POLICY_ENGINE_AVAILABLE = True
except ImportError:
    POLICY_ENGINE_AVAILABLE = False
    enterprise_policy_engine = None

# Observability 2.0 â€” Decision drift, concept drift, latency profiling
try:
    from app.core.observability import observability
    OBSERVABILITY_AVAILABLE = True
except ImportError:
    OBSERVABILITY_AVAILABLE = False
    observability = None

# Security Layer â€” Zero-trust, prompt injection firewall, rate limiting
try:
    from app.core.security import security_layer
    SECURITY_AVAILABLE = True
except ImportError:
    SECURITY_AVAILABLE = False
    security_layer = None

# Decision Quality v5.5.0 â€” Outcome prediction recording
try:
    from app.core.decision_quality import record_prediction as dq_record_prediction
    DQ_OUTCOME_AVAILABLE = True
except ImportError:
    DQ_OUTCOME_AVAILABLE = False
    dq_record_prediction = None

# v6.02.00: PDF GÃ¶rsel DesteÄŸi
try:
    from app.rag.pdf_images import (
        get_pdf_images_for_pages, get_all_pdf_images, PDF_IMAGES_DIR, _safe_dirname
    )
    PDF_IMAGES_AVAILABLE = True
except ImportError:
    PDF_IMAGES_AVAILABLE = False
    get_pdf_images_for_pages = lambda s, p: []
    get_all_pdf_images = lambda s: []

# v6.02.00: GÃ¶rsel istemi algÄ±lama anahtar kelimeleri
_IMAGE_INTENT_KEYWORDS = {
    "resim", "resmini", "resimler", "resimlerini", "resminden",
    "gÃ¶rsel", "gÃ¶rseli", "gÃ¶rseller", "gÃ¶rsellerini",
    "fotoÄŸraf", "fotoÄŸrafÄ±", "fotoÄŸraflar", "fotoÄŸrafÄ±nÄ±", "foto",
    "image", "picture", "photo",
    "gÃ¶ster", "gÃ¶sterir",
}

def _detect_image_intent(question: str) -> bool:
    """KullanÄ±cÄ±nÄ±n gÃ¶rsel isteyip istemediÄŸini algÄ±la (v6.02.00)"""
    q_lower = question.lower()
    words = set(re.findall(r'\w+', q_lower))
    return bool(words & _IMAGE_INTENT_KEYWORDS)


def _extract_page_numbers_from_chunk(content: str) -> list:
    """Chunk iÃ§eriÄŸinden sayfa numaralarÄ±nÄ± Ã§Ä±kar (v6.02.00)
    
    '--- Sayfa N ---' marker'larÄ±nÄ± arar. OCR ile iÅŸlenmiÅŸ PDF'lerde
    her sayfa bu ÅŸekilde iÅŸaretlenmiÅŸtir.
    """
    pages = re.findall(r'---\s*Sayfa\s+(\d+)\s*---', content)
    return [int(p) for p in pages]


def _build_pdf_image_rich_data(question: str, relevant_docs: list) -> dict | None:
    """
    RAG sonuÃ§larÄ±ndan PDF gÃ¶rsellerini rich_data formatÄ±nda dÃ¶ndÃ¼r (v6.02.00).
    
    AkÄ±ÅŸ:
    1. relevant_docs'tan PDF kaynaklarÄ±nÄ± bul
    2. Chunk'lardan sayfa numaralarÄ±nÄ± Ã§Ä±kar
    3. Ä°lgili sayfalardaki gÃ¶rselleri al
    4. ImageResultsCard formatÄ±nda rich_data dÃ¶ndÃ¼r
    """
    if not PDF_IMAGES_AVAILABLE or not relevant_docs:
        return None
    
    all_images = []
    seen_sources = set()
    
    for doc in relevant_docs:
        source = doc.get("source", "")
        doc_type = doc.get("type", "")
        
        # Sadece PDF kaynaklarÄ±nÄ± iÅŸle
        if not source.lower().endswith(".pdf"):
            continue
        
        # Sayfa numaralarÄ±nÄ± chunk iÃ§eriÄŸinden Ã§Ä±kar
        content = doc.get("content", "")
        page_numbers = _extract_page_numbers_from_chunk(content)
        
        if page_numbers:
            # Belirli sayfalardaki gÃ¶rselleri al
            images = get_pdf_images_for_pages(source, page_numbers)
        elif source not in seen_sources:
            # Sayfa bilgisi yoksa tÃ¼m gÃ¶rselleri al (max 6)
            images = get_all_pdf_images(source)[:6]
        else:
            continue
        
        seen_sources.add(source)
        all_images.extend(images)
    
    if not all_images:
        return None
    
    # Tekrarlayan gÃ¶rselleri kaldÄ±r
    unique_images = []
    seen_srcs = set()
    for img in all_images:
        if img["src"] not in seen_srcs:
            seen_srcs.add(img["src"])
            unique_images.append(img)
    
    # Maksimum 12 gÃ¶rsel
    unique_images = unique_images[:12]
    
    return {
        "type": "images",
        "query": question,
        "images": unique_images,
        "source": "PDF DokÃ¼man GÃ¶rselleri",
    }

logger = structlog.get_logger()


async def process_question(
    question: str, 
    department_override: Optional[str] = None,
    use_rag: bool = True,
    user_name: Optional[str] = None,
    user_department: Optional[str] = None,
    session_history: Optional[list] = None,
    memory_context: Optional[str] = None,
) -> dict:
    """
    Ana soru iÅŸleme fonksiyonu â€” AkÄ±llÄ± Pipeline.
    
    AkÄ±ÅŸ:
    1. Router ile niyet analizi
    2. Semantik hafÄ±za â€” benzer geÃ§miÅŸ konuÅŸmalar
    3. Niyete gÃ¶re bilgi kaynaklarÄ± (RAG / Web / HafÄ±za)
    4. KiÅŸiselleÅŸtirilmiÅŸ prompt oluÅŸturma
    5. LLM yanÄ±t Ã¼retme
    6. OTOMATÄ°K Ã–ÄRENME â€” Her konuÅŸmadan bilgi Ã§Ä±kar ve RAG'a kaydet
       (kullanÄ±cÄ± mesajÄ± + AI yanÄ±tÄ± â†’ knowledge_extractor â†’ ChromaDB)
    """
    logger.info("processing_question", question=question[:100])
    _t0 = time.time()  # Governance elapsed_ms iÃ§in zamanlama
    
    # â”€â”€ Synapse Network â€” Pipeline Context oluÅŸtur â”€â”€
    synapse_ctx = None
    if SYNAPSE_AVAILABLE and create_pipeline_context:
        synapse_ctx = create_pipeline_context(
            question=question,
            department=department_override or user_department or "",
        )

    # â•â•â• v5.5.0: Security Layer â€” Zero-trust giriÅŸ kontrolÃ¼ â•â•â•
    security_result = None
    if SECURITY_AVAILABLE and security_layer:
        try:
            security_result = security_layer.check_request(
                user_input=question,
                user_id=user_name or "anonymous",
                endpoint="query",
                model_name=getattr(ollama_client, 'model', 'unknown'),
                user_role="analyst",  # Default role â€” HITL'den gelecek
            )
            if security_result.get("blocked"):
                logger.warning("security_blocked",
                              reason=security_result.get("block_reason", "unknown"))
                return {
                    "answer": f"â›” GÃ¼venlik kontrolÃ¼ baÅŸarÄ±sÄ±z: {security_result.get('block_reason', 'Ä°stek engellendi')}",
                    "department": department_override or user_department or "genel",
                    "mode": "GÃ¼venlik",
                    "risk": "yÃ¼ksek",
                    "intent": "blocked",
                    "confidence": 0,
                    "sources": [],
                    "web_searched": False,
                    "security": security_result,
                }
            logger.debug("security_passed", threat_score=security_result.get("threat_score", 0))
        except Exception as sec_err:
            logger.debug("security_check_skipped", error=str(sec_err))

    # â•â•â• v5.5.0: Event Bus â€” Sorgu baÅŸlangÄ±Ã§ olayÄ± â•â•â•
    _query_event_id = None
    if EVENT_BUS_AVAILABLE and event_bus:
        try:
            import uuid as _uuid
            _query_event_id = str(_uuid.uuid4())[:12]
            event_bus.emit("query.received", {
                "query_id": _query_event_id,
                "question": question[:200],
                "user": user_name or "anonymous",
                "department": department_override or user_department or "",
            })
        except Exception:
            pass
    
    # 1. AkÄ±llÄ± yÃ¶nlendirme â€” v4.3.0: LLM-based router (primary) + regex (fallback)
    try:
        context = await async_decide(question)
    except Exception:
        context = decide(question)  # Fallback to sync regex
    intent = context.get("intent", "sohbet")
    needs_web = context.get("needs_web", False)
    
    if department_override:
        context["dept"] = department_override
    
    # KullanÄ±cÄ± bilgisi
    if user_name:
        context["user_name"] = user_name
    if user_department:
        context["user_dept"] = user_department
    
    logger.info("intent_detected", intent=intent, mode=context["mode"], 
                dept=context["dept"], needs_web=needs_web)
    
    # â”€â”€ HIZLI SOHBET YOLU â”€â”€ KalÄ±p eÅŸleÅŸmesi varsa LLM'e gitmeden cevapla
    # Ã–NEMLÄ°: Kimlik/hafÄ±za sorularÄ± ("beni tanÄ±yor musun", "ismimi biliyor musun")
    # pattern matcher'a girmeden LLM'e yÃ¶nlendirilir. Ã‡Ã¼nkÃ¼ pattern matcher kullanÄ±cÄ±
    # ismi hafÄ±zasÄ±na eriÅŸemez, sadece LLM context'inde user_name bilgisi var.
    _is_identity_question = bool(re.search(
        r"(beni\s*tanÄ±|ismimi|adÄ±mÄ±|hatÄ±rlÄ±yor|biliyor\s*mu|kim\s*olduÄŸ|tanÄ±yor\s*mu)",
        question.lower()
    ))
    
    if intent == "sohbet" and CHAT_EXAMPLES_AVAILABLE and not _is_identity_question:
        pattern_answer = get_pattern_response(question)
        if pattern_answer:
            # KiÅŸiselleÅŸtirme ekle
            if user_name and "{name}" not in pattern_answer:
                # Ä°simle hitap et (rastgele, her seferinde deÄŸil)
                import random
                if random.random() < 0.4:
                    first_name = user_name.split()[0] if user_name else ""
                    if first_name:
                        pattern_answer = f"{first_name}, {pattern_answer[0].lower()}{pattern_answer[1:]}"
            
            logger.info("fast_pattern_response", pattern=True)
            remember(question, pattern_answer, context)
            return {
                "answer": pattern_answer,
                "department": context["dept"],
                "mode": "Sohbet",
                "risk": context["risk"],
                "intent": "sohbet",
                "confidence": 0.95,
                "sources": ["KalÄ±p EÅŸleÅŸmesi"],
                "web_searched": False,
            }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v5.7.0: SOHBET FAST-PATH â€” Enterprise modÃ¼ller ATLANIR
    # Sohbet intent'inde pattern tutmayanlar iÃ§in:
    #   Router â†’ LLM Ã§aÄŸrÄ±sÄ± â†’ hafÄ±zaya kaydet â†’ dÃ¶n
    # 30+ enterprise modÃ¼l (governance, XAI, gatekeeper vb.) Ã‡ALIÅMAZ.
    # Tahmini kazanÃ§: 10-30x hÄ±z artÄ±ÅŸÄ± (sohbet sorularÄ±nda)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if intent == "sohbet":
        logger.info("sohbet_fast_path", question=question[:80])
        
        # HafÄ±za baÄŸlamÄ± â€” sohbette de kullanÄ±cÄ±yÄ± hatÄ±rlasÄ±n
        chat_system_prompt = (
            "Sen CompanyAI kurumsal yapay zeka asistanÄ±sÄ±n. "
            "KullanÄ±cÄ±yla doÄŸal, sÄ±cak ve yardÄ±mcÄ± bir ÅŸekilde sohbet et. "
            "TÃ¼rkÃ§e yanÄ±t ver. YanÄ±tlarÄ±n 1-2 cÃ¼mle olsun. "
            "Liste, baÅŸlÄ±k, madde KULLANMA. KÄ±sa ve doÄŸal konuÅŸ."
        )
        if user_name:
            chat_system_prompt += (
                f"\n\nKullanÄ±cÄ±nÄ±n adÄ±: '{user_name}'. "
                f"KullanÄ±cÄ±ya '{user_name.split()[0]}' diye hitap edebilirsin."
            )
        if memory_context:
            chat_system_prompt += f"\n\nKullanÄ±cÄ± HafÄ±zasÄ±:\n{memory_context}"
        
        # Session history â€” takip sorularÄ± iÃ§in gerekli
        chat_history = []
        if session_history:
            chat_history = session_history[-5:]
        
        try:
            if await ollama_client.is_available():
                chat_answer = await ollama_client.generate(
                    prompt=question,
                    system_prompt=chat_system_prompt,
                    temperature=0.7,
                    max_tokens=512,
                    history=chat_history if chat_history else None,
                )
            else:
                chat_answer = "Åu an yanÄ±t veremiyorum, biraz sonra tekrar dener misin?"
        except Exception as e:
            logger.error("sohbet_fast_path_llm_error", error=str(e))
            chat_answer = "Bir hata oluÅŸtu, tekrar dener misin?"
        
        # HafÄ±zaya kaydet
        remember(question, chat_answer, context)
        
        # Arka planda Ã¶ÄŸrenme (event loop'u bloklamaz)
        if KNOWLEDGE_EXTRACTOR_AVAILABLE:
            import asyncio
            loop = asyncio.get_event_loop()
            loop.run_in_executor(None, _background_learn,
                question, chat_answer, user_name, context.get("dept"), False)
        
        logger.info("sohbet_fast_path_done", 
                    elapsed_ms=round((time.time() - _t0) * 1000, 1))
        
        return {
            "answer": chat_answer,
            "department": context["dept"],
            "mode": "Sohbet",
            "risk": context["risk"],
            "intent": "sohbet",
            "confidence": 0.90,
            "sources": [],
            "web_searched": False,
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ä°Å / BÄ°LGÄ° MODU â€” RAG + Web + LLM Pipeline
    # Sohbet atlandÄ± (yukarÄ±da fast-path ile dÃ¶ndÃ¼).
    # Bilgi modunda: RAG + Web + LLM â†’ sonra enterprise modÃ¼ller ATLANIR.
    # Ä°ÅŸ/Analiz/Rapor modunda: RAG + Web + LLM â†’ enterprise modÃ¼ller Ã‡ALIÅIR.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # 2. Semantik hafÄ±za â€” soruya EN BENZER geÃ§miÅŸ konuÅŸmalar
    similar_memories = []
    try:
        similar_memories = search_memory(question, limit=3)
        if similar_memories:
            logger.info("similar_memories_found", count=len(similar_memories))
    except Exception as e:
        logger.warning("memory_search_error", error=str(e))
    
    # 3. Bilgi kaynaklarÄ±nÄ± topla
    relevant_docs = []
    web_results = None
    
    # RAG aramasÄ± â€” Agentic RAG: karmaÅŸÄ±k sorular alt parÃ§alara ayrÄ±lÄ±r
    # Her alt parÃ§a baÄŸÄ±msÄ±z aranÄ±r, sonuÃ§lar birleÅŸtirilip re-rank edilir
    if use_rag and RAG_AVAILABLE:
        try:
            raw_docs = agentic_search(question, n_results=5, department=department_override)
            # Hybrid skor filtreleme â€” vector_store zaten semantic + keyword
            # hybrid scoring yapÄ±yor. Burada sadece tamamen alakasÄ±z olanlarÄ± ele.
            if raw_docs:
                for doc in raw_docs:
                    hybrid = doc.get('relevance', 0)
                    dist = doc.get('distance', 999)
                    # Hybrid skor > 0.12 VEYA distance < 1.4 ise dahil et
                    # Eski eÅŸikler (0.03/1.8) neredeyse hiÃ§bir ÅŸeyi reddetmiyordu
                    if hybrid > 0.12 or dist < 1.4:
                        relevant_docs.append(doc)
                if relevant_docs:
                    logger.info("rag_documents_found", count=len(relevant_docs),
                                scores=[f"h={d.get('relevance',0):.3f}/d={d.get('distance',0):.2f}" for d in relevant_docs])
                else:
                    logger.info("rag_documents_filtered_out", raw=len(raw_docs),
                                scores=[f"h={d.get('relevance',0):.3f}/d={d.get('distance',0):.2f}" for d in raw_docs])
        except Exception as e:
            logger.error("rag_search_error", error=str(e))
    
    # Web aramasÄ± â€” RAG'da iyi sonuÃ§ varsa web aramayÄ± atla (Ã¶ÄŸretilen iÃ§erik Ã¶ncelikli)
    web_results = None
    web_rich_data = None
    _rag_has_good = any(d.get('relevance', 0) > 0.12 or d.get('distance', 999) < 1.4 for d in relevant_docs) if relevant_docs else False
    if WEB_SEARCH_AVAILABLE and search_and_summarize and not _rag_has_good:
        should_search_web = (
            needs_web or 
            (intent == "bilgi" and not relevant_docs) or
            (intent == "iÅŸ" and not relevant_docs)
        )
        if should_search_web:
            try:
                web_results, web_rich_data = await search_and_summarize(question)
                if web_results:
                    logger.info("web_search_results_found", has_rich_data=web_rich_data is not None)
            except Exception as e:
                logger.warning("web_search_error", error=str(e))
    
    # 4. Prompt oluÅŸtur (KISA tut â€” Mistral 7B CPU)
    if relevant_docs:
        system_prompt, user_prompt = build_rag_prompt(question, context, relevant_docs)
    else:
        system_prompt, user_prompt = build_prompt(question, context)
    
    # v6.02.00: GÃ¶rsel intent + varlÄ±k kontrolÃ¼ â€” LLM'e KOÅULLU bilgi ver
    _has_pdf_images = False
    if _detect_image_intent(question) and relevant_docs and PDF_IMAGES_AVAILABLE:
        # GÃ¶rsellerin disk Ã¼zerinde gerÃ§ekten mevcut olup olmadÄ±ÄŸÄ±nÄ± kontrol et
        _pre_image_card = _build_pdf_image_rich_data(question, relevant_docs)
        if _pre_image_card and _pre_image_card.get("images"):
            _has_pdf_images = True
            system_prompt += """

ğŸ“¸ GÃ–RSEL BÄ°LGÄ°SÄ°: Bu konuyla ilgili PDF dokÃ¼manlarÄ±ndan Ã§Ä±karÄ±lmÄ±ÅŸ gÃ¶rseller MEVCUT.
- "Metin tabanlÄ± asistanÄ±m, gÃ¶rsel gÃ¶steremem" gibi ÅŸeyler SÃ–YLEME.
- GÃ¶rsellerin yanÄ±tla birlikte otomatik olarak aÅŸaÄŸÄ±da gÃ¶sterildiÄŸini belirt.
- Konu hakkÄ±nda bildiklerini kÄ±saca aÃ§Ä±kla, "ilgili gÃ¶rselleri aÅŸaÄŸÄ±da bulabilirsiniz" de."""
    
    # KiÅŸiselleÅŸtirme â€” kullanÄ±cÄ± kimliÄŸi (tek seferde, v5.9.0)
    if user_name:
        system_prompt += f"\n\nKullanÄ±cÄ±nÄ±n adÄ±: '{user_name}'. Ona '{user_name.split()[0]}' diye hitap edebilirsin. GeÃ§miÅŸ konuÅŸmalardaki farklÄ± isimler baÅŸka kiÅŸilere aittir."
    
    # KalÄ±cÄ± hafÄ±za baÄŸlamÄ± â€” PostgreSQL'den gelen kullanÄ±cÄ± bilgileri + geÃ§miÅŸ
    if memory_context:
        system_prompt += f"\n\nKullanÄ±cÄ± HafÄ±zasÄ± (geÃ§miÅŸ konusmalardan Ã¶ÄŸrenilen bilgiler):\n{memory_context}"    
    
    # Web sonuÃ§larÄ±nÄ± prompt'a ekle (RAG yoksa ana kaynak, RAG varsa ek referans)
    if web_results:
        _web_text = web_results[:1500]
        if TOKEN_BUDGET_AVAILABLE:
            _web_text = truncate_to_budget(_web_text, "web_results")
        if relevant_docs:
            system_prompt += f"\n\nEk referans (internetten): AÅŸaÄŸÄ±daki bilgiler tamamlayÄ±cÄ±dÄ±r. Ã–nceliÄŸi yukarÄ±daki dokÃ¼man bilgilerine ver:\n{_web_text}"
        else:
            system_prompt += f"\n\nAÅŸaÄŸÄ±da internetten bulunan gÃ¼ncel bilgiler var. Bu bilgileri kullanarak yanÄ±t ver:\n{_web_text}"
    
    # v4.3.0: Token BÃ¼tÃ§e KontrolÃ¼ â€” tÃ¼m bileÅŸenleri bÃ¼tÃ§eye sÄ±ÄŸdÄ±r
    if TOKEN_BUDGET_AVAILABLE and smart_truncate_all:
        _memory_text = memory_context or ""
        _history_text = ""
        if session_history:
            _history_text = "\n".join(
                f"{m.get('role','?')}: {m.get('content','')[:200]}" 
                for m in session_history[-5:]
            )
        budget_result = smart_truncate_all(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            rag_context="",  # Zaten system_prompt iÃ§inde
            memory_context=_memory_text,
            web_results="",  # Zaten system_prompt iÃ§inde
            chat_history_text=_history_text,
        )
        system_prompt = budget_result["system_prompt"]
        user_prompt = budget_result["user_prompt"]
    
    # Chat history â€” system prompt'a DEÄÄ°L, client'a ayrÄ± gÃ¶nder
    # Her intent'te (sohbet dahil) geÃ§miÅŸi gÃ¶nder â€” "biraz daha basit anlat" gibi takip sorularÄ± iÃ§in
    chat_history = []
    if session_history:
        chat_history = session_history[-5:]
    
    # 5. LLM'e sor
    try:
        if await ollama_client.is_available():
            # v5.9.1: Mod bazlÄ± sÄ±caklÄ±k ve token limiti + detay algÄ±lama
            _mode = context.get("mode", "Sohbet")
            if _mode in ("Sohbet", "Beyin FÄ±rtÄ±nasÄ±"):
                temp = 0.7
            elif _mode in ("Bilgi", "Ã–neri"):
                temp = 0.4
            else:  # Analiz, Rapor, Acil, Ã–zet
                temp = 0.3
            
            # KullanÄ±cÄ± detaylÄ± yanÄ±t mÄ± istiyor?
            import re as _re
            _wants_detail = bool(_re.search(
                r'(detayl[Ä±i]|kapsaml[Ä±i]|ayr[Ä±i]nt[Ä±i]l[Ä±i]|madde\s*madde|listele|'
                r's[Ä±i]rala|a[Ã§c][Ä±i]kla|t[Ã¼u]m|hepsini|tam\s*liste|uzun\s*anlat)',
                question.lower()
            ))
            
            if _mode in ("Analiz", "Rapor"):
                _max_tokens = 2048
            elif _wants_detail:
                _max_tokens = 1024  # Detay isteniyorsa biraz daha uzun
            elif _mode in ("Bilgi", "Ã–neri"):
                _max_tokens = 384   # VarsayÄ±lan kÄ±sa
            else:
                _max_tokens = 256   # Sohbet, Ã–zet, Acil
                
            llm_answer = await ollama_client.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=temp,
                max_tokens=_max_tokens,
                history=chat_history if chat_history else None,
            )
        else:
            logger.warning("ollama_not_available", using_fallback=True)
            llm_answer = f"[Sistem Notu: LLM ÅŸu an eriÅŸilemez] Soru alÄ±ndÄ±: {question}"
    except Exception as e:
        logger.error("llm_error", error=str(e))
        llm_answer = f"[Hata] LLM yanÄ±t Ã¼retemedi: {str(e)}"
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v5.7.0: BÄ°LGÄ° MODU FAST-PATH â€” LLM cevabÄ±ndan sonra
    # Enterprise post-processing (governance, XAI, gatekeeper, debate,
    # causal, strategic, executive, KG, uncertainty, OOD, quality,
    # KPI impact, decision memory, executive digest, observability,
    # policy engine, meta learning) ATLANIR.
    # Sadece: RAG kaynaklarÄ± + export + hafÄ±za kaydÄ± yapÄ±lÄ±r.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    _is_enterprise_mode = context.get("mode") in ("Analiz", "Rapor", "Ã–neri", "Acil")
    
    if not _is_enterprise_mode and intent != "sohbet":
        logger.info("bilgi_fast_path_return", 
                    mode=context.get("mode"), intent=intent,
                    elapsed_ms=round((time.time() - _t0) * 1000, 1))
        
        sources = []
        if relevant_docs:
            sources.extend([doc.get("source") for doc in relevant_docs])
        if web_results:
            sources.append("Ä°nternet AramasÄ±")
        
        rich_data = web_rich_data if web_rich_data else []
        if not isinstance(rich_data, list):
            rich_data = [rich_data]
        
        # v6.02.00: PDF gÃ¶rsel desteÄŸi â€” Ã¶nceden hesaplanan _pre_image_card'Ä± kullan
        if _has_pdf_images and _pre_image_card:
            rich_data.append(_pre_image_card)
            logger.info("pdf_images_injected_bilgi",
                       image_count=len(_pre_image_card["images"]))
        
        # Export talebi varsa dosya Ã¼ret (bu hÄ±zlÄ±)
        if EXPORT_AVAILABLE:
            export_format = detect_export_request(question)
            if export_format and llm_answer and not llm_answer.startswith("[Hata]"):
                try:
                    export_title = question.strip()[:60].rstrip("?.!")
                    export_result = generate_export(llm_answer, export_format, export_title)
                    if export_result:
                        fmt_info = FORMAT_LABELS.get(export_format, {})
                        rich_data.append({
                            "type": "export",
                            "file_id": export_result["file_id"],
                            "filename": export_result["filename"],
                            "format": export_format,
                            "format_label": fmt_info.get("label", export_format),
                            "format_icon": fmt_info.get("icon", "ğŸ“„"),
                            "download_url": f"/api/export/download/{export_result['file_id']}",
                        })
                except Exception:
                    pass
        
        # HafÄ±zaya kaydet
        remember(question, llm_answer, context)
        
        # Arka planda Ã¶ÄŸrenme
        if KNOWLEDGE_EXTRACTOR_AVAILABLE:
            import asyncio
            loop = asyncio.get_event_loop()
            loop.run_in_executor(None, _background_learn,
                question, llm_answer, user_name, context.get("dept"), bool(relevant_docs))
        
        return {
            "answer": llm_answer,
            "department": context["dept"],
            "mode": context["mode"],
            "risk": context["risk"],
            "intent": intent,
            "confidence": 0.88 if relevant_docs else 0.82,
            "sources": sources,
            "web_searched": web_results is not None,
            "rich_data": rich_data if rich_data else None,
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ENTERPRISE PIPELINE â€” Sadece Analiz/Rapor/Ã–neri/Acil modlarÄ±nda
    # AÅŸaÄŸÄ±daki tÃ¼m modÃ¼ller sadece iÅŸ analizi modlarÄ±nda Ã§alÄ±ÅŸÄ±r.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # â”€â”€ 5b. TOOL CALLING + DYNAMIC CHAIN â€” Tool Ã§aÄŸrÄ±sÄ± + zincirleme â”€â”€
    tool_results = []
    if TOOLS_AVAILABLE and llm_answer and not llm_answer.startswith("[Hata]"):
        try:
            # Ã–nce zincirleme tool call var mÄ± kontrol et
            chain_calls = detect_tool_chain(question)
            
            if chain_calls and len(chain_calls) >= 2:
                # Dynamic Tool Chaining â€” araÃ§larÄ± sÄ±ralÄ± Ã§alÄ±ÅŸtÄ±r
                chain_result = await tool_registry.chain_execute(chain_calls)
                if chain_result.get("success"):
                    for cr in chain_result.get("chain_results", []):
                        if cr["result"].get("success"):
                            tool_results.append({
                                "tool": cr["tool"],
                                "result": cr["result"],
                                "chain_step": cr["step"],
                            })
                    logger.info("tool_chain_executed", steps=chain_result["tools_executed"])
            
            if not tool_results:
                # Tekil tool detection (fallback)
                detected_tools = detect_tool_calls(llm_answer)
                if detected_tools:
                    for tool_call in detected_tools[:3]:  # Max 3 tool
                        tool_name = tool_call.get("tool", "")
                        tool_params = tool_call.get("params", {})
                        result = await tool_registry.execute(tool_name, tool_params)
                        if result and not result.get("error"):
                            tool_results.append({
                                "tool": tool_name,
                                "result": result,
                            })
            
            if tool_results:
                # Tool sonuÃ§larÄ±nÄ± cevaba ekle
                tool_text = "\n\n---\nğŸ“Š **Hesaplama SonuÃ§larÄ±:**\n"
                for tr in tool_results:
                    chain_info = f" (AdÄ±m {tr['chain_step']})" if tr.get("chain_step") else ""
                    tool_text += f"\n**{tr['tool']}{chain_info}**: {_format_tool_result(tr['result'])}"
                llm_answer += tool_text
                logger.info("tools_executed", count=len(tool_results))
        except Exception as e:
            logger.warning("tool_execution_error", error=str(e))
    
    # â”€â”€ 5c. STRUCTURED OUTPUT â€” Analiz/Rapor modunda JSON yapÄ±landÄ±rma â”€â”€
    structured_data = None
    if STRUCTURED_OUTPUT_AVAILABLE and context.get("mode") in ["Analiz", "Rapor", "Acil"]:
        try:
            structured_data = auto_structure(llm_answer)
            if structured_data and structured_data.get("sections"):
                logger.info("output_structured", sections=len(structured_data.get("sections", [])))
        except Exception as e:
            logger.debug("structured_output_skipped", error=str(e))
    
    # â”€â”€ 5d. REFLECTION LAYER + SELF-CORRECTION LOOP â”€â”€
    reflection_data = None
    dynamic_confidence = 0.85  # Default
    if REFLECTION_AVAILABLE and llm_answer and not llm_answer.startswith("[Hata]"):
        try:
            evaluation = quick_evaluate(llm_answer, question, context.get("mode", "Sohbet"))
            reflection_data = evaluation
            dynamic_confidence = evaluation["confidence"] / 100.0  # 0-1 arasÄ±
            
            # v4.3.0: LLM-Based Deep Reflection â€” Analiz/Rapor/Ã–neri modunda
            # Orta gÃ¼venli yanÄ±tlarda (60-75%) REFLECTION_PROMPT ile LLM self-eval yap
            _mode = context.get("mode", "Sohbet")
            if (_mode in ("Analiz", "Rapor", "Ã–neri") and 
                60 <= evaluation["confidence"] <= 75 and
                not llm_answer.startswith("[Hata]")):
                try:
                    from app.core.reflection import REFLECTION_PROMPT
                    _refl_prompt = REFLECTION_PROMPT.format(
                        question=question[:500],
                        answer=llm_answer[:2000]
                    )
                    _refl_result = await ollama_client.generate(
                        prompt=_refl_prompt,
                        system_prompt="Sen bir kalite kontrol uzmanÄ±sÄ±n. JSON formatÄ±nda yanÄ±t ver.",
                        temperature=0.1,
                        max_tokens=500,
                    )
                    if _refl_result:
                        import json as _json
                        try:
                            _refl_json = _json.loads(
                                _refl_result.strip().strip("```json").strip("```").strip()
                            )
                            # LLM reflection sonuÃ§larÄ±nÄ± evaluation'a merge et
                            if isinstance(_refl_json, dict) and "overall_confidence" in _refl_json:
                                evaluation["llm_reflection"] = _refl_json
                                # LLM reflection gÃ¼veni ile heuristic'in ortalamasÄ±nÄ± al
                                llm_conf = _refl_json.get("overall_confidence", evaluation["confidence"])
                                evaluation["confidence"] = (evaluation["confidence"] + llm_conf) / 2
                                dynamic_confidence = evaluation["confidence"] / 100.0
                                reflection_data = evaluation
                                logger.info("llm_reflection_applied",
                                           heuristic_conf=evaluation["confidence"],
                                           llm_conf=llm_conf)
                        except (ValueError, _json.JSONDecodeError):
                            logger.debug("llm_reflection_parse_failed")
                except Exception as refl_err:
                    logger.debug("llm_reflection_skipped", error=str(refl_err))
            
            # Self-correction loop â€” Analiz/Rapor/Ã–neri modunda dÃ¼ÅŸÃ¼k gÃ¼venli yanÄ±tlarÄ±
            # iteratif olarak iyileÅŸtir (max 2 tur)
            if evaluation.get("should_retry") and not llm_answer.startswith("[Hata]"):
                logger.info("self_correction_triggered", 
                           confidence=evaluation["confidence"],
                           issues=evaluation.get("issues", []))
                try:
                    correction_result = await self_correction_loop(
                        question=question,
                        initial_answer=llm_answer,
                        mode=context.get("mode", "Sohbet"),
                        llm_generate=ollama_client.generate,
                        system_prompt=system_prompt,
                        chat_history=chat_history if chat_history else None,
                        max_rounds=2,
                    )
                    if correction_result["improved"]:
                        llm_answer = correction_result["answer"]
                        reflection_data = correction_result["evaluation"]
                        dynamic_confidence = correction_result["confidence"] / 100.0
                        logger.info("self_correction_success",
                                   rounds=correction_result["rounds"],
                                   new_confidence=correction_result["confidence"])
                except Exception as retry_err:
                    logger.warning("self_correction_failed", error=str(retry_err))
            
            # Confidence badge â€” sadece Analiz/Rapor modlarÄ±nda gÃ¶ster
            if context.get("mode") in ["Analiz", "Rapor", "Ã–neri", "Acil"]:
                badge = format_confidence_badge(evaluation["confidence"])
                llm_answer += f"\n\n---\n{badge}"
            
            logger.info("reflection_evaluated", 
                        confidence=evaluation["confidence"],
                        passed=evaluation["pass"])
            
            # â”€â”€ Sinaps: Reflection sinyalleri â”€â”€
            if SYNAPSE_AVAILABLE and synapse_ctx and reflection_data:
                emit_signal(synapse_ctx, "reflection", "reflection_score",
                           reflection_data.get("confidence", 0), 0.85)
                emit_signal(synapse_ctx, "reflection", "confidence",
                           dynamic_confidence, 0.80)
        except Exception as e:
            logger.debug("reflection_skipped", error=str(e))
    
    # â”€â”€ 5d-1.5. SAYISAL DOÄRULAMA + KAYNAK ATIF (v4.4.0) â”€â”€
    numerical_validation = None
    source_citation_valid = True
    if (NUMERICAL_VALIDATION_AVAILABLE and validate_numbers_against_source
        and relevant_docs and llm_answer and not llm_answer.startswith("[Hata]")
        and context.get("mode") in ("Analiz", "Rapor", "Ã–neri", "Acil")):
        try:
            # RAG baÄŸlamÄ±nÄ± birleÅŸtir
            _rag_context = "\n".join(
                doc.get("content", "")[:500] for doc in relevant_docs[:5]
            )
            numerical_validation = validate_numbers_against_source(llm_answer, _rag_context)
            
            if numerical_validation:
                _num_score = numerical_validation.get("score", 100)
                _mismatches = numerical_validation.get("mismatch_count", 0)
                _fabricated = numerical_validation.get("fabricated_count", 0)
                
                if _fabricated > 0 or _mismatches > 1:
                    # Uyumsuz sayÄ±lar bulundu â€” uyarÄ± ekle
                    _issues = numerical_validation.get("issues", [])
                    _issue_text = "\n".join(f"  - {i}" for i in _issues[:3])
                    llm_answer += (
                        f"\n\n---\nâš ï¸ **SayÄ±sal DoÄŸrulama UyarÄ±sÄ±** (skor: {_num_score}/100)\n"
                        f"{_issue_text}\n"
                        f"LÃ¼tfen kaynak verileri kontrol ediniz."
                    )
                    # Confidence dÃ¼ÅŸÃ¼r
                    dynamic_confidence = max(0.3, dynamic_confidence - 0.15)
                    source_citation_valid = False
                    logger.warning("numerical_validation_issues",
                                  score=_num_score, mismatches=_mismatches,
                                  fabricated=_fabricated)
                elif _num_score >= 80:
                    logger.info("numerical_validation_passed", score=_num_score)
                
                # Kaynak atÄ±f doÄŸrulama â€” yanÄ±tta bahsedilen kaynaklar RAG'da var mÄ±?
                _mentioned_sources = re.findall(
                    r'(?:kaynaÄŸ?a?|dosya|rapor|belge|dokÃ¼man)[:\s]+["\']?([^"\',\n]{5,50})["\']?',
                    llm_answer, re.IGNORECASE
                )
                if _mentioned_sources and relevant_docs:
                    _actual_sources = set()
                    for doc in relevant_docs:
                        _src = doc.get("source", "").lower()
                        if _src:
                            _actual_sources.add(_src)
                            # Dosya adÄ±nÄ± da ekle
                            from pathlib import Path as _Path
                            _actual_sources.add(_Path(_src).stem.lower())
                    
                    _unverified = []
                    for ms in _mentioned_sources:
                        ms_lower = ms.strip().lower()
                        if not any(ms_lower in src or src in ms_lower for src in _actual_sources):
                            _unverified.append(ms.strip())
                    
                    if _unverified:
                        llm_answer += (
                            f"\nâš ï¸ DoÄŸrulanamayan kaynak atÄ±flarÄ±: {', '.join(_unverified[:3])}"
                        )
                        source_citation_valid = False
                        logger.warning("unverified_source_citations", sources=_unverified)
        except Exception as num_err:
            logger.debug("numerical_validation_skipped", error=str(num_err))
    
    # â”€â”€ Sinaps: Numerical Validation sinyalleri â”€â”€
    if SYNAPSE_AVAILABLE and synapse_ctx and numerical_validation:
        _nv_score = numerical_validation.get("score", 100)
        emit_signal(synapse_ctx, "numerical_validation", "numerical_valid",
                   _nv_score >= 80, 0.75)
        emit_signal(synapse_ctx, "numerical_validation", "numerical_score",
                   _nv_score, 0.70)
    
    # â”€â”€ 5d-2. ACTIVE LEARNING (v4.3.0) â€” DÃ¼ÅŸÃ¼k gÃ¼venli yanÄ±tlarda kullanÄ±cÄ±dan doÄŸrulama iste â”€â”€
    if (REFLECTION_AVAILABLE and reflection_data and 
        40 <= reflection_data.get("confidence", 100) <= 60 and
        context.get("mode") not in ("Sohbet",) and
        not llm_answer.startswith("[Hata]")):
        # v5.9.0: Active learning notu cevaba eklenmez (gÃ¼rÃ¼ltÃ¼ azaltma)
        logger.info("active_learning_triggered", confidence=reflection_data["confidence"])
    
    # â”€â”€ 5d-3. MULTI-PERSPECTIVE DECISIONS (v4.3.0 â†’ v5.9.0 devre dÄ±ÅŸÄ±) â”€â”€
    # v5.9.0: Ekstra LLM Ã§aÄŸrÄ±sÄ± kaldÄ±rÄ±ldÄ± â€” CoT ÅŸablonu zaten Ã§oklu perspektif saÄŸlÄ±yor
    # Bu 10-30 saniyelik ek gecikmeyi ve token israfÄ±nÄ± Ã¶nler
    _is_strategic = False  # Devre dÄ±ÅŸÄ±
    
    # â”€â”€ 5d-4. ROI RECOMMENDATIONS (v4.3.0) â”€â”€
    # YatÄ±rÄ±m/maliyet sorularÄ±nda Monte Carlo simÃ¼lasyonu tetikle
    _is_investment = bool(re.search(
        r'(yatÄ±rÄ±m|maliyet|bÃ¼tÃ§e|roi\b|getiri|tasarruf|amorti|geri.?Ã¶deme)',
        question.lower()
    ))
    if (_is_investment and MONTE_CARLO_AVAILABLE 
        and context.get("mode") in ("Analiz", "Rapor", "Ã–neri")
        and not llm_answer.startswith("[Hata]")):
        try:
            # Monte Carlo simÃ¼lasyonu â€” yatÄ±rÄ±m senaryosu
            mc_result = monte_carlo_simulate(
                base_value=100.0,  # Normalize edilmiÅŸ baz deÄŸer
                volatility=0.25,
                simulations=1000,
                periods=12,
            )
            if mc_result:
                # v5.9.0: Monte Carlo tablosu JSON'a taÅŸÄ±ndÄ±, cevaba eklenmez
                logger.info("roi_monte_carlo_applied")
        except Exception as roi_err:
            logger.debug("roi_monte_carlo_skipped", error=str(roi_err))
    
    # â”€â”€ 5e. MULTI-AGENT PIPELINE â€” KarmaÅŸÄ±k Analiz SorularÄ±nda â”€â”€
    # v4.3.0: Cross-Module Orchestrator â€” geniÅŸ sorularda paralel modÃ¼l Ã§alÄ±ÅŸtÄ±r
    _is_broad_query = bool(re.search(
        r'(genel\s*durum|Ã¶zet|dashboard|saÄŸlÄ±k|health|tÃ¼m\s*departman|panorama|bÃ¼yÃ¼k\s*resim)',
        question.lower()
    ))
    if _is_broad_query and context.get("mode") in ("Analiz", "Rapor", "Ã–neri"):
        import asyncio
        _parallel_tasks = []
        _parallel_labels = []
        
        if EXECUTIVE_HEALTH_AVAILABLE:
            _parallel_tasks.append(asyncio.to_thread(calculate_health_index))
            _parallel_labels.append("health")
        if BOTTLENECK_AVAILABLE:
            _parallel_tasks.append(asyncio.to_thread(bottleneck_analyze, question))
            _parallel_labels.append("bottleneck")
        if GRAPH_IMPACT_AVAILABLE:
            _parallel_tasks.append(asyncio.to_thread(auto_graph_analysis, question, llm_answer))
            _parallel_labels.append("graph")
        
        if _parallel_tasks:
            try:
                _parallel_results = await asyncio.gather(*_parallel_tasks, return_exceptions=True)
                _orchestrator_text = "\n\n---\nğŸ¯ **Cross-Module Panorama**\n"
                for label, res in zip(_parallel_labels, _parallel_results):
                    if isinstance(res, Exception):
                        continue
                    if label == "health" and res:
                        _orchestrator_text += f"\n{format_health_dashboard(res)}"
                    elif label == "bottleneck" and res:
                        _orchestrator_text += f"\n{format_bottleneck_report(res)}"
                    elif label == "graph" and res and res.total_nodes_affected > 0:
                        _orchestrator_text += f"\n{format_graph_impact(res)}"
                if len(_orchestrator_text) > 50:
                    # v5.9.0: Cross-module panorama JSON'a taÅŸÄ±ndÄ±
                    logger.info("cross_module_orchestrator_done", modules=len(_parallel_tasks))
            except Exception as orch_err:
                logger.debug("cross_module_orchestrator_failed", error=str(orch_err))
    
    pipeline_data = None
    if AGENT_PIPELINE_AVAILABLE and should_use_pipeline(question, context.get("mode", ""), intent):
        try:
            # BaÄŸlam oluÅŸtur
            pipeline_context = ""
            if relevant_docs:
                pipeline_context = "\n".join(
                    doc.get("content", "")[:200] for doc in relevant_docs[:3]
                )
            if web_results:
                pipeline_context += f"\n{web_results[:500]}"
            
            pipeline_result = await execute_agent_pipeline(
                question=question,
                context=pipeline_context,
                llm_generate=ollama_client.generate,
                mode=context.get("mode", "Analiz"),
            )
            
            if pipeline_result and pipeline_result.final_answer:
                # v5.9.0: Pipeline Ã§Ä±ktÄ±sÄ± JSON'da, cevaba eklenmez
                pipeline_data = pipeline_result.to_dict()
                # Pipeline confidence ile override
                if pipeline_result.overall_confidence > dynamic_confidence * 100:
                    dynamic_confidence = pipeline_result.overall_confidence / 100.0
                logger.info("agent_pipeline_completed", 
                           agents=len(pipeline_result.agent_results))
        except Exception as e:
            logger.warning("agent_pipeline_error", error=str(e))
    
    # â”€â”€ 5f. AI GOVERNANCE â€” Bias / Drift / Confidence Monitoring â”€â”€
    governance_data = None
    if GOVERNANCE_AVAILABLE and governance_engine and llm_answer and not llm_answer.startswith("[Hata]"):
        try:
            _elapsed_ms = (time.time() - _t0) * 1000
            
            # KullanÄ±lan modÃ¼lleri topla
            _modules_invoked = []
            if relevant_docs:
                _modules_invoked.append("rag")
            if web_results:
                _modules_invoked.append("web_search")
            if tool_results:
                _modules_invoked.append("tool_calling")
            if structured_data:
                _modules_invoked.append("structured_output")
            if reflection_data:
                _modules_invoked.append("reflection")
            if pipeline_data:
                _modules_invoked.append("agent_pipeline")
            
            # Reasoning adÄ±mlarÄ±nÄ± topla
            _reasoning_steps = []
            if reflection_data and isinstance(reflection_data, dict):
                _reasoning_steps = reflection_data.get("issues", [])
            
            # Model adÄ±nÄ± al
            _model_name = getattr(ollama_client, 'model', 'unknown')
            
            gov_record = governance_engine.evaluate(
                question=question,
                answer=llm_answer,
                mode=context.get("mode", "Sohbet"),
                confidence=dynamic_confidence * 100 if dynamic_confidence <= 1 else dynamic_confidence,
                elapsed_ms=_elapsed_ms,
                model_name=_model_name,
                modules_invoked=_modules_invoked,
                reasoning_steps=_reasoning_steps,
            )
            if gov_record.alert_triggered:
                alert_text = format_governance_alert(gov_record)
                if alert_text and context.get("mode") in ["Analiz", "Rapor", "Ã–neri", "Acil"]:
                    llm_answer += f"\n\n{alert_text}"
            governance_data = {
                "confidence": gov_record.confidence,
                "bias_score": gov_record.bias_score,
                "drift_detected": gov_record.drift_detected,
                "alert": gov_record.alert_reason if gov_record.alert_triggered else None,
                "compliance_score": getattr(gov_record, 'compliance_score', None),
                "trace_id": getattr(gov_record, 'trace_id', None),
                "drift_types": getattr(gov_record, 'drift_types', []),
                "policy_violations": getattr(gov_record, 'policy_violations', []),
                "risk_level": getattr(gov_record, 'risk_level', None) if hasattr(gov_record, 'risk_level') else None,
            }
            logger.info("governance_evaluated", 
                       bias=gov_record.bias_score, 
                       drift=gov_record.drift_detected,
                       compliance=getattr(gov_record, 'compliance_score', None),
                       trace_id=getattr(gov_record, 'trace_id', None))
            
            # â”€â”€ Sinaps: Governance sinyalleri â”€â”€
            if SYNAPSE_AVAILABLE and synapse_ctx and governance_data:
                emit_signal(synapse_ctx, "governance", "bias_flags",
                           governance_data.get("policy_violations", []), 0.85)
                emit_signal(synapse_ctx, "governance", "drift_status",
                           governance_data.get("drift_detected", False), 0.70)
                emit_signal(synapse_ctx, "governance", "compliance_score",
                           governance_data.get("compliance_score", 1.0), 0.80)
                _gov_risk = governance_data.get("risk_level")
                if _gov_risk:
                    emit_signal(synapse_ctx, "governance", "risk_level",
                               _gov_risk, 0.85)
                # Kaskad kontrol â€” bias tetikleme
                _cascaded = check_cascades(synapse_ctx, "governance")
                if _cascaded:
                    logger.info("synapse_cascade_from_governance", targets=_cascaded)
        except Exception as e:
            logger.debug("governance_skipped", error=str(e))
    
    # â”€â”€ 5g. XAI â€” AÃ§Ä±klanabilir Yapay Zeka Analizi â”€â”€
    xai_data = None
    if XAI_AVAILABLE and decision_explainer and llm_answer and not llm_answer.startswith("[Hata]"):
        try:
            xai_result = decision_explainer.explain(
                query=question,
                response=llm_answer,
                mode=context.get("mode", "Sohbet"),
                confidence=dynamic_confidence,
                sources=[doc.get("source", "") for doc in relevant_docs] if relevant_docs else [],
                rag_docs=relevant_docs if relevant_docs else None,
                web_searched=web_results is not None,
                reflection_data=reflection_data,
                module_source="engine",
            )
            if xai_result:
                xai_data = xai_result
                logger.info("xai_evaluated",
                           weighted_confidence=xai_result.get("weighted_confidence", 0),
                           risk_level=xai_result.get("risk", {}).get("level", "?"))
                
                # â”€â”€ Sinaps: XAI sinyalleri â”€â”€
                if SYNAPSE_AVAILABLE and synapse_ctx:
                    emit_signal(synapse_ctx, "explainability", "xai_factors",
                               xai_result.get("factors", []), 0.65)
                    emit_signal(synapse_ctx, "explainability", "weighted_confidence",
                               xai_result.get("weighted_confidence", 0), 0.60)
        except Exception as e:
            logger.debug("xai_skipped", error=str(e))
    
    # â”€â”€ 5i. DECISION IMPACT RANKING â€” Analiz modunda kararlarÄ± sÄ±rala â”€â”€
    # v4.3.0: Trend Detection â€” AynÄ± KPI/konu tekrar soruluyorsa trend raporla
    if (session_history and len(session_history) >= 3 
        and context.get("mode") in ("Analiz", "Rapor", "Ã–neri")
        and not llm_answer.startswith("[Hata]")):
        try:
            # GeÃ§miÅŸ sorularda benzer KPI/konu arayÄ±ÅŸÄ±
            _kpi_pattern = re.compile(
                r'(verimlilik|fire\s*oranÄ±|maliyet|ciro|Ã¼retim|stok|kalite|karlÄ±lÄ±k|'
                r'sipariÅŸ|teslimat|devamsÄ±zlÄ±k|enerji|hurda|duruÅŸ)',
                re.IGNORECASE
            )
            _current_kpis = set(_kpi_pattern.findall(question.lower()))
            
            if _current_kpis:
                _past_mentions = []
                for msg in session_history[-10:]:
                    _q = msg.get("q", msg.get("content", ""))
                    _past_kpis = set(_kpi_pattern.findall(_q.lower()))
                    overlap = _current_kpis & _past_kpis
                    if overlap:
                        _past_mentions.append({"q": _q[:100], "kpis": overlap})
                
                if len(_past_mentions) >= 2:
                    _trend_kpis = ", ".join(_current_kpis)
                    # v5.9.0: Trend notu JSON metadata'da
                    logger.info("trend_detected", kpis=list(_current_kpis), 
                               mentions=len(_past_mentions))
        except Exception as trend_err:
            logger.debug("trend_detection_skipped", error=str(trend_err))
    
    ranking_data = None
    if DECISION_RANKING_AVAILABLE and llm_answer and context.get("mode") in ["Analiz", "Rapor", "Ã–neri"]:
        try:
            decisions = extract_decisions_from_llm(llm_answer, question)
            if decisions and len(decisions) >= 2:
                ranking_result = rank_decisions(decisions)
                # v5.9.0: Ranking tablosu JSON'da
                ranking_data = {
                    "total": ranking_result.total_evaluated,
                    "top_action": ranking_result.top_action,
                }
                logger.info("decision_ranking_applied", count=len(decisions))
        except Exception as e:
            logger.debug("decision_ranking_skipped", error=str(e))
    
    # â”€â”€ 5j. GRAPH IMPACT MAPPING â€” KPI/Risk/Departman iliÅŸki grafiÄŸi â”€â”€
    graph_data = None
    if GRAPH_IMPACT_AVAILABLE and llm_answer and context.get("mode") in ["Analiz", "Rapor", "Ã–neri"]:
        try:
            graph_result = auto_graph_analysis(question, llm_answer)
            if graph_result and graph_result.total_nodes_affected > 0:
                # v5.9.0: Graph impact JSON'da
                graph_data = {
                    "focus": graph_result.focus_node,
                    "affected": graph_result.total_nodes_affected,
                    "critical_chain": graph_result.critical_chain,
                }
                logger.info("graph_impact_applied", focus=graph_result.focus_node,
                           affected=graph_result.total_nodes_affected)
        except Exception as e:
            logger.debug("graph_impact_skipped", error=str(e))
    
    # 6. SonuÃ§
    sources = []
    if relevant_docs:
        sources.extend([doc.get("source") for doc in relevant_docs])
    if web_results:
        sources.append("Ä°nternet AramasÄ±")
    
    # Rich data listesi
    rich_data = web_rich_data if web_rich_data else []
    if not isinstance(rich_data, list):
        rich_data = [rich_data]
    
    # v6.02.00: PDF gÃ¶rsel desteÄŸi â€” Enterprise modunda da Ã§alÄ±ÅŸÄ±r (Ã¶nceden hesaplandÄ±)
    if _has_pdf_images and _pre_image_card:
        rich_data.append(_pre_image_card)
        logger.info("pdf_images_injected_enterprise",
                   image_count=len(_pre_image_card["images"]))
    
    # 6b. Export talebi varsa dosya Ã¼ret
    export_format = None
    if EXPORT_AVAILABLE:
        export_format = detect_export_request(question)
    
    if export_format and llm_answer and not llm_answer.startswith("[Hata]"):
        try:
            # BaÅŸlÄ±ÄŸÄ± sorudan Ã§Ä±kar
            export_title = question.strip()[:60].rstrip("?.!")
            export_result = generate_export(llm_answer, export_format, export_title)
            if export_result:
                fmt_info = FORMAT_LABELS.get(export_format, {})
                rich_data.append({
                    "type": "export",
                    "file_id": export_result["file_id"],
                    "filename": export_result["filename"],
                    "format": export_format,
                    "format_label": fmt_info.get("label", export_format),
                    "format_icon": fmt_info.get("icon", "ğŸ“„"),
                    "download_url": f"/api/export/download/{export_result['file_id']}",
                })
                logger.info("export_auto_generated", format=export_format, file_id=export_result["file_id"])
        except Exception as e:
            logger.warning("export_auto_failed", error=str(e))
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6c. MULTI-AGENT DEBATE â€” Ã‡ok perspektifli tartÄ±ÅŸma (v4.7.0)
    #     KarmaÅŸÄ±k / stratejik sorularda birden fazla perspektif ajanÄ±
    #     tartÄ±ÅŸÄ±r ve konsensÃ¼s sentezi Ã¼retir.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    debate_data = None
    if MULTI_AGENT_DEBATE_AVAILABLE:
        try:
            should_debate, debate_reason = check_debate_trigger(
                question=question,
                mode=context.get("mode", ""),
                intent=intent,
                confidence=dynamic_confidence if REFLECTION_AVAILABLE else 85,
            )
            if should_debate:
                logger.info("multi_agent_debate_triggered", reason=debate_reason)
                debate_data = {"triggered": True, "reason": debate_reason}
        except Exception as e:
            logger.debug("multi_agent_debate_check_error", error=str(e))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6d. CAUSAL INFERENCE â€” Nedensellik analizi tetikleme (v4.7.0)
    #     "Neden", "kÃ¶k neden", "sebep" gibi sorularda otomatik tetiklenir.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    causal_data = None
    if CAUSAL_INFERENCE_AVAILABLE:
        try:
            should_causal, causal_reason = check_causal_trigger(
                question=question,
                mode=context.get("mode", ""),
                intent=intent,
            )
            if should_causal:
                logger.info("causal_inference_triggered", reason=causal_reason)
                causal_data = {"triggered": True, "reason": causal_reason}
        except Exception as e:
            logger.debug("causal_inference_check_error", error=str(e))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6e. STRATEGIC PLANNER â€” Stratejik planlama motoru (v5.0.0)
    #     Strateji, hedef, vizyon, roadmap sorularÄ±nda tetiklenir.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    strategic_data = None
    if STRATEGIC_PLANNER_AVAILABLE:
        try:
            should_strategic, strategic_reason = check_strategic_trigger(
                question=question,
                mode=context.get("mode", ""),
                intent=intent,
            )
            if should_strategic:
                logger.info("strategic_planner_triggered", reason=strategic_reason)
                strategic_data = {"triggered": True, "reason": strategic_reason}
        except Exception as e:
            logger.debug("strategic_planner_check_error", error=str(e))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6f. EXECUTIVE INTELLIGENCE â€” YÃ¶netici zekasÄ± (v5.0.0)
    #     CEO/CFO/CTO brifingleri, KPI korelasyon, board raporu.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    exec_intel_data = None
    if EXECUTIVE_INTELLIGENCE_AVAILABLE:
        try:
            should_exec, exec_reason = check_executive_trigger(
                question=question,
                mode=context.get("mode", ""),
                intent=intent,
            )
            if should_exec:
                logger.info("executive_intelligence_triggered", reason=exec_reason)
                exec_intel_data = {"triggered": True, "reason": exec_reason}
        except Exception as e:
            logger.debug("executive_intelligence_check_error", error=str(e))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6g. KNOWLEDGE GRAPH â€” Bilgi grafiÄŸi zenginleÅŸtirme (v5.0.0)
    #     VarlÄ±k/iliÅŸki Ã§Ä±karma ve baÄŸlam zenginleÅŸtirme.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    kg_data = None
    if KNOWLEDGE_GRAPH_AVAILABLE:
        try:
            should_kg, kg_reason = check_kg_trigger(
                question=question,
                mode=context.get("mode", ""),
                intent=intent,
            )
            if should_kg:
                logger.info("knowledge_graph_triggered", reason=kg_reason)
                kg_data = {"triggered": True, "reason": kg_reason}
        except Exception as e:
            logger.debug("knowledge_graph_check_error", error=str(e))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6h. UNCERTAINTY QUANTIFICATION â€” Belirsizlik Ã¶lÃ§Ã¼mleme (v5.1.0)
    #     TÃ¼m gÃ¼ven kaynaklarÄ±nÄ± birleÅŸtirip ensemble skor Ã¼retir.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    uncertainty_data = None
    if UNCERTAINTY_AVAILABLE:
        try:
            uq_result = uncertainty_quantifier.quantify(
                question=question,
                reflection_data=reflection_data,
                engine_confidence=dynamic_confidence if REFLECTION_AVAILABLE else 0.85,
                governance_data=governance_data,
            )
            uncertainty_data = uq_result.to_dict()
            logger.debug("uncertainty_quantified",
                         confidence=uq_result.ensemble_confidence,
                         margin=uq_result.margin_of_error)
            
            # â”€â”€ Sinaps: Uncertainty sinyalleri â”€â”€
            if SYNAPSE_AVAILABLE and synapse_ctx:
                emit_signal(synapse_ctx, "uncertainty_quantification", "uncertainty",
                           uncertainty_data.get("uncertainty_pct", 50), 0.80)
                emit_signal(synapse_ctx, "uncertainty_quantification", "ensemble_confidence",
                           uq_result.ensemble_confidence, 0.75)
                emit_signal(synapse_ctx, "uncertainty_quantification", "confidence_adjustment",
                           uncertainty_data.get("confidence_adjustment", 0), 0.70)
        except Exception as e:
            logger.debug("uncertainty_quantification_error", error=str(e))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6i. DECISION RISK GATEKEEPER â€” Karar risk kapÄ±sÄ± (v5.1.0)
    #     TÃ¼m risk sinyallerini birleÅŸtirip kararÄ± geÃ§ir/engeller.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    gate_data = None
    if DECISION_GATEKEEPER_AVAILABLE:
        try:
            should_gate, gate_reason = check_gate_trigger(
                question=question,
                mode=context.get("mode", ""),
                intent=intent,
            )
            if should_gate:
                # â”€â”€ Sinaps: risk_data artÄ±k None deÄŸil â€” sinaps aÄŸÄ±ndan topla â”€â”€
                _synapse_risk = None
                if SYNAPSE_AVAILABLE and synapse_ctx:
                    _synapse_risk = gather_module_inputs(synapse_ctx, "decision_gatekeeper")
                    # risk_analyzer sinyallerini risk_data olarak kullan
                    if _synapse_risk.get("risk_level") or _synapse_risk.get("risk_factors"):
                        _synapse_risk = {
                            "risk_level": _synapse_risk.get("risk_level", "LOW"),
                            "risk_factors": _synapse_risk.get("risk_factors", []),
                            "risk_score": _synapse_risk.get("risk_score", 0),
                        }
                    else:
                        _synapse_risk = None

                gate_result = decision_gatekeeper.evaluate(
                    question=question,
                    answer=llm_answer,
                    governance_data=governance_data,
                    reflection_data=reflection_data,
                    confidence=dynamic_confidence if REFLECTION_AVAILABLE else 0.85,
                    risk_data=_synapse_risk,
                    ranking_data=ranking_data,
                )
                gate_data = gate_result.to_dict()
                logger.info("decision_gate_evaluated",
                            verdict=gate_result.verdict.value,
                            risk_score=gate_result.composite_risk_score)
                
                # â”€â”€ Sinaps: Gatekeeper sinyalleri + kaskad â”€â”€
                if SYNAPSE_AVAILABLE and synapse_ctx:
                    emit_signal(synapse_ctx, "decision_gatekeeper", "gate_verdict",
                               gate_result.verdict.value, 0.90)
                    emit_signal(synapse_ctx, "decision_gatekeeper", "composite_risk_score",
                               gate_result.composite_risk_score, 0.80)
                    emit_signal(synapse_ctx, "decision_gatekeeper", "escalation_required",
                               gate_data.get("escalation_required", False), 0.75)
                    emit_signal(synapse_ctx, "decision_gatekeeper", "risk_signals",
                               gate_data.get("risk_signals", []), 0.70)
                    _cascaded = check_cascades(synapse_ctx, "decision_gatekeeper")
                    if _cascaded:
                        logger.info("synapse_cascade_from_gate", targets=_cascaded)
        except Exception as e:
            logger.debug("decision_gatekeeper_error", error=str(e))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6j. OOD DETECTOR â€” DaÄŸÄ±lÄ±m dÄ±ÅŸÄ± girdi algÄ±lama (v5.3.0)
    #     Sorunun bilinen alan iÃ§inde olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ood_data = None
    if OOD_DETECTOR_AVAILABLE and check_ood:
        try:
            ood_result = check_ood(question=question, department=context.get("dept", ""))
            ood_data = ood_result.to_dict()

            # OOD ise uyarÄ± ekle
            if ood_result.is_ood and context.get("mode") in ["Analiz", "Rapor", "Ã–neri", "Acil"]:
                ood_warning_text = format_ood_warning(ood_result)
                if ood_warning_text:
                    llm_answer += f"\n{ood_warning_text}"

            # Confidence/uncertainty ayarla
            if ood_result.confidence_adjustment != 0:
                dynamic_confidence = max(0.1, dynamic_confidence + ood_result.confidence_adjustment / 100)

            logger.debug("ood_checked", severity=ood_result.severity.value, score=ood_result.ood_score)
            
            # â”€â”€ Sinaps: OOD sinyalleri + kaskad â”€â”€
            if SYNAPSE_AVAILABLE and synapse_ctx:
                emit_signal(synapse_ctx, "ood_detector", "ood_score",
                           ood_result.ood_score, 0.80)
                emit_signal(synapse_ctx, "ood_detector", "ood_severity",
                           ood_result.severity.value, 0.85)
                if ood_result.confidence_adjustment != 0:
                    emit_signal(synapse_ctx, "ood_detector", "confidence_adjustment",
                               ood_result.confidence_adjustment, 0.75)
                _cascaded = check_cascades(synapse_ctx, "ood_detector")
                if _cascaded:
                    logger.info("synapse_cascade_from_ood", targets=_cascaded)
        except Exception as e:
            logger.debug("ood_detector_error", error=str(e))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6k. DECISION QUALITY SCORE â€” BirleÅŸik karar kalitesi (v5.3.0)
    #     TÃ¼m modÃ¼l Ã§Ä±ktÄ±larÄ±nÄ± toplayarak tek bir kalite skoru Ã¼retir.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    quality_data = None
    if DECISION_QUALITY_AVAILABLE and evaluate_decision_quality:
        try:
            # â”€â”€ Sinaps: meta_data artÄ±k None deÄŸil â€” sinaps aÄŸÄ±ndan topla â”€â”€
            _synapse_meta = None
            if SYNAPSE_AVAILABLE and synapse_ctx:
                _meta_inputs = gather_module_inputs(synapse_ctx, "decision_quality")
                if _meta_inputs.get("meta_strategy") or _meta_inputs.get("quality_trend"):
                    _synapse_meta = _meta_inputs

            quality_result = evaluate_decision_quality(
                reflection_data=reflection_data,
                uncertainty_data=uncertainty_data,
                gate_data=gate_data,
                meta_data=_synapse_meta,
                governance_data=governance_data,
                debate_data=debate_data,
                causal_data=causal_data,
                rag_used=bool(relevant_docs),
                web_searched=web_results is not None,
                sources=sources,
                source_citation_valid=source_citation_valid,
                question=question,
                department=context.get("dept", ""),
            )
            quality_data = {
                "overall_score": quality_result.overall_score,
                "band": quality_result.band.value,
                "confidence_interval": quality_result.confidence_interval,
                "executive_line": quality_result.executive_line,
            }

            # Analiz/Rapor modlarÄ±nda badge ekle
            # v5.9.0: Quality badge JSON'da (confidence badge yeterli)

            logger.info("decision_quality_scored",
                        score=quality_result.overall_score,
                        band=quality_result.band.value)
            
            # â”€â”€ Sinaps: Decision Quality sinyalleri + kaskad â”€â”€
            if SYNAPSE_AVAILABLE and synapse_ctx and quality_data:
                emit_signal(synapse_ctx, "decision_quality", "quality_score",
                           quality_result.overall_score, 0.90)
                emit_signal(synapse_ctx, "decision_quality", "quality_band",
                           quality_result.band.value, 0.80)
                _cascaded = check_cascades(synapse_ctx, "decision_quality")
                if _cascaded:
                    logger.info("synapse_cascade_from_quality", targets=_cascaded)
        except Exception as e:
            logger.debug("decision_quality_error", error=str(e))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6l. KPI IMPACT MAPPING â€” KPI etki analizi (v5.3.0)
    #     KararÄ±n KPI'lara etkisini tahmin eder, domino etkilerini hesaplar.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    kpi_impact_data = None
    if KPI_IMPACT_AVAILABLE and analyze_kpi_impact:
        try:
            _decision_text = f"{question} {llm_answer[:500]}"
            kpi_result = analyze_kpi_impact(decision_text=_decision_text)
            if kpi_result and kpi_result.primary_impacts:
                kpi_impact_data = {
                    "primary_count": len(kpi_result.primary_impacts),
                    "domino_count": len(kpi_result.domino_effects),
                    "impact_score": kpi_result.impact_score,
                    "net_financial": kpi_result.net_financial_direction,
                    "executive_summary": kpi_result.executive_summary,
                    "impacts": [
                        {"kpi_id": i.kpi_id, "kpi_name": i.kpi_name, "direction": i.direction.value,
                         "magnitude": i.magnitude.value, "estimated_change_pct": i.estimated_change_pct}
                        for i in kpi_result.primary_impacts[:5]
                    ],
                }

                # Analiz modlarÄ±nda etki Ã¶zeti ekle
                # v5.9.0: KPI impact brief JSON'da

                logger.info("kpi_impact_analyzed",
                            impacts=len(kpi_result.primary_impacts),
                            score=kpi_result.impact_score)
                
                # â”€â”€ Sinaps: KPI Impact sinyalleri + kaskad â”€â”€
                if SYNAPSE_AVAILABLE and synapse_ctx:
                    emit_signal(synapse_ctx, "kpi_impact", "kpi_impacts",
                               kpi_impact_data.get("impacts", []), 0.80)
                    emit_signal(synapse_ctx, "kpi_impact", "impact_score",
                               kpi_result.impact_score, 0.75)
                    emit_signal(synapse_ctx, "kpi_impact", "financial_estimate",
                               kpi_result.net_financial_direction, 0.70)
                    _cascaded = check_cascades(synapse_ctx, "kpi_impact")
                    if _cascaded:
                        logger.info("synapse_cascade_from_kpi", targets=_cascaded)
        except Exception as e:
            logger.debug("kpi_impact_error", error=str(e))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6m. DECISION MEMORY â€” Benzer geÃ§miÅŸ kararlarÄ± bul (v5.3.0)
    #     Soruyu geÃ§miÅŸ kararlarla karÅŸÄ±laÅŸtÄ±rÄ±r ve benzer olanlarÄ± gÃ¶sterir.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    memory_data = None
    similar_decisions = None
    if DECISION_MEMORY_AVAILABLE and find_similar_decisions:
        try:
            similar = find_similar_decisions(
                question=question,
                department=context.get("dept", ""),
                top_n=3,
            )
            if similar:
                memory_data = {
                    "similar_count": len(similar),
                    "top_similarity": similar[0].similarity_score,
                    "similar_decisions": [
                        {"question": s.record.question[:100], "outcome": s.record.outcome.value,
                         "similarity": round(s.similarity_score, 2), "quality": s.record.quality_score}
                        for s in similar
                    ],
                }
                similar_decisions = similar

                # Analiz modlarÄ±nda benzer kararlarÄ± gÃ¶ster
                # v5.9.0: Similar decisions JSON'da

                logger.info("similar_decisions_found", count=len(similar),
                            top_sim=similar[0].similarity_score)
                
                # â”€â”€ Sinaps: Decision Memory sinyalleri â”€â”€
                if SYNAPSE_AVAILABLE and synapse_ctx:
                    emit_signal(synapse_ctx, "decision_memory", "similar_decisions",
                               memory_data.get("similar_decisions", []), 0.70)
                    emit_signal(synapse_ctx, "decision_memory", "accuracy_data",
                               memory_data.get("top_similarity", 0), 0.60)
        except Exception as e:
            logger.debug("decision_memory_search_error", error=str(e))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6n. EXECUTIVE DIGEST â€” YÃ¶netim Ã¶zeti (v5.3.0)
    #     5 madde + Risk + FÄ±rsat + Net Ã–neri formatÄ±nda sade Ã¶zet Ã¼retir.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    digest_data = None
    if EXECUTIVE_DIGEST_AVAILABLE and generate_executive_digest:
        try:
            if context.get("mode") in ["Analiz", "Rapor", "Ã–neri", "Acil"]:
                _gate_verdict = gate_data.get("verdict", "") if gate_data else ""
                _gate_risks = gate_data.get("risk_signals", []) if gate_data else []
                _unc_val = uncertainty_data.get("uncertainty_pct", 50) if uncertainty_data else 50
                _conf_val = uncertainty_data.get("ensemble_confidence", 50) if uncertainty_data else dynamic_confidence * 100
                _moe = uncertainty_data.get("margin_of_error", 0) if uncertainty_data else 0
                _refl_score = reflection_data.get("confidence", 0) if reflection_data else 0
                _q_score = quality_data.get("overall_score", 0) if quality_data else 0
                _q_band = quality_data.get("band", "") if quality_data else ""
                _kpi_impacts_list = kpi_impact_data.get("impacts", []) if kpi_impact_data else []
                _kpi_iscore = kpi_impact_data.get("impact_score", 0) if kpi_impact_data else 0
                _kpi_summary = kpi_impact_data.get("executive_summary", "") if kpi_impact_data else ""
                _ood_flag = ood_data.get("is_ood", False) if ood_data else False
                _ood_note = ood_data.get("warning_message", "") if ood_data else ""

                digest = generate_executive_digest(
                    question=question,
                    ai_answer=llm_answer[:2000],
                    department=context.get("dept", ""),
                    quality_score=_q_score,
                    quality_band=_q_band,
                    kpi_impacts=_kpi_impacts_list,
                    impact_score=_kpi_iscore,
                    kpi_executive_summary=_kpi_summary,
                    gate_verdict=_gate_verdict,
                    gate_risks=_gate_risks,
                    uncertainty=_unc_val,
                    confidence=_conf_val,
                    margin_of_error=_moe,
                    reflection_score=_refl_score,
                    ood_detected=_ood_flag,
                    ood_note=_ood_note,
                )
                digest_data = digest.to_dict()

                # v5.9.0: Executive digest JSON'da, cevaba eklenmez

                logger.info("executive_digest_generated",
                            priority=digest.priority.value,
                            impact=digest.impact_level)
                
                # â”€â”€ Sinaps: Executive Digest sinyalleri â”€â”€
                if SYNAPSE_AVAILABLE and synapse_ctx:
                    emit_signal(synapse_ctx, "executive_digest", "executive_digest",
                               digest_data, 0.85)
                    emit_signal(synapse_ctx, "executive_digest", "digest_priority",
                               digest.priority.value, 0.70)
        except Exception as e:
            logger.debug("executive_digest_error", error=str(e))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6p. OBSERVABILITY 2.0 â€” Karar drift + kalite trend izleme (v5.5.0)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    observability_data = None
    if OBSERVABILITY_AVAILABLE and observability:
        try:
            _elapsed_ms = (time.time() - _t0) * 1000
            _obs_confidence = dynamic_confidence * 100 if dynamic_confidence <= 1 else dynamic_confidence
            _obs_quality = quality_data.get("overall_score", 70) if quality_data else 70

            observability.record_decision(
                confidence=_obs_confidence,
                quality_score=_obs_quality,
                latency_ms=_elapsed_ms,
                intent=intent,
                department=context.get("dept", "genel"),
            )

            # Drift kontrolÃ¼
            drift_report = observability.check_all_drifts()
            if drift_report:
                _any_drift = any(
                    d.get("severity", "none") not in ("none", "NONE")
                    for d in drift_report.values()
                    if isinstance(d, dict)
                )
                if _any_drift:
                    observability_data = drift_report
                    logger.info("observability_drift_detected",
                               drifts={k: v.get("severity") for k, v in drift_report.items() if isinstance(v, dict)})

            logger.debug("observability_recorded", latency_ms=round(_elapsed_ms, 1))
        except Exception as obs_err:
            logger.debug("observability_error", error=str(obs_err))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6q. POLICY ENGINE â€” Enterprise kural motoru (v5.5.0)
    #     SonuÃ§ dÃ¶ndÃ¼rÃ¼lmeden Ã¶nce tÃ¼m politika kurallarÄ±nÄ± kontrol eder.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    policy_data = None
    if POLICY_ENGINE_AVAILABLE and enterprise_policy_engine:
        try:
            _elapsed_ms = (time.time() - _t0) * 1000
            _policy_context = {
                "risk_score": gate_data.get("composite_risk_score", 0) if gate_data else 0,
                "confidence": dynamic_confidence * 100 if dynamic_confidence <= 1 else dynamic_confidence,
                "quality_score": quality_data.get("overall_score", 70) if quality_data else 70,
                "response_time_ms": _elapsed_ms,
                "department": context.get("dept", "genel"),
                "mode": context.get("mode", "Sohbet"),
                "data_source_count": len(sources) if sources else 0,
                "model_name": getattr(ollama_client, 'model', 'unknown'),
            }
            policy_result = enterprise_policy_engine.evaluate(_policy_context)
            if policy_result and not policy_result.allowed:
                policy_data = {
                    "allowed": policy_result.allowed,
                    "action": policy_result.action.value,
                    "violations": [
                        {"rule": v.rule_id, "message": v.message, "severity": v.severity.value}
                        for v in policy_result.violations
                    ],
                }
                # Block ise uyarÄ± ekle
                if policy_result.action.value == "block":
                    _violations_text = "; ".join(v.message for v in policy_result.violations[:3])
                    llm_answer += f"\n\n---\nâš ï¸ **Politika UyarÄ±sÄ±**: {_violations_text}"
                logger.info("policy_violations",
                            action=policy_result.action.value,
                            violation_count=len(policy_result.violations))
            elif policy_result:
                policy_data = {"allowed": True, "action": "allow", "violations": []}
        except Exception as pol_err:
            logger.debug("policy_engine_error", error=str(pol_err))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6r. OUTCOME PREDICTION â€” Decision Quality v5.5.0 tahmin kaydÄ±
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if DQ_OUTCOME_AVAILABLE and dq_record_prediction and _query_event_id:
        try:
            _q_score = quality_data.get("overall_score", 0) if quality_data else 0
            dq_record_prediction(
                decision_id=_query_event_id,
                predicted_outcome=llm_answer[:200],
                confidence=dynamic_confidence * 100 if dynamic_confidence <= 1 else dynamic_confidence,
                quality_score=_q_score,
            )
        except Exception:
            pass

    # â•â•â• v5.5.0: Event Bus â€” Sorgu tamamlanma olayÄ± â•â•â•
    if EVENT_BUS_AVAILABLE and event_bus and _query_event_id:
        try:
            _elapsed_ms = (time.time() - _t0) * 1000
            event_bus.emit("query.completed", {
                "query_id": _query_event_id,
                "intent": intent,
                "department": context.get("dept", ""),
                "confidence": dynamic_confidence,
                "quality_score": quality_data.get("overall_score", 0) if quality_data else 0,
                "latency_ms": round(_elapsed_ms, 1),
                "modules_used": len([v for v in (relevant_docs, web_results, tool_results, pipeline_data, governance_data) if v]),
            })
        except Exception:
            pass

    # â”€â”€ Sinaps: Pipeline sonlandÄ±r ve sonucu zenginleÅŸtir â”€â”€
    synapse_summary = None
    synapse_trace_text = ""
    if SYNAPSE_AVAILABLE and synapse_ctx:
        try:
            finalize_context(synapse_ctx)
            synapse_summary = synapse_ctx.summary()
            synapse_trace_text = format_network_summary(synapse_ctx)
            
            # Analiz modlarÄ±nda sinyal akÄ±ÅŸ izini cevaba ekle
            # v5.9.0: Signal trace JSON'da
            pass
        except Exception as syn_err:
            logger.debug("synapse_finalize_error", error=str(syn_err))

    result = {
        "answer": llm_answer,
        "department": context["dept"],
        "mode": context["mode"],
        "risk": context["risk"],
        "intent": intent,
        "confidence": dynamic_confidence if REFLECTION_AVAILABLE else (0.85 if not relevant_docs else 0.92),
        "sources": sources,
        "web_searched": web_results is not None,
        "rich_data": rich_data if rich_data else None,
        "tool_results": tool_results if tool_results else None,
        "structured_data": structured_data,
        "reflection": reflection_data,
        "pipeline": pipeline_data,
        "governance": governance_data,
        "ranking": ranking_data,
        "graph_impact": graph_data,
        "xai": xai_data,
        "numerical_validation": numerical_validation,
        "source_citation_valid": source_citation_valid,
        "debate": debate_data,
        "causal": causal_data,
        "strategic": strategic_data,
        "executive_intel": exec_intel_data,
        "knowledge_graph": kg_data,
        "uncertainty": uncertainty_data,
        "gate": gate_data,
        "ood": ood_data,
        "decision_quality": quality_data,
        "kpi_impact": kpi_impact_data,
        "decision_memory": memory_data,
        "executive_digest": digest_data,
        "synapse_network": synapse_summary,
        # v5.5.0 Enterprise Platform
        "security": security_result,
        "observability": observability_data,
        "policy": policy_data,
    }
    
    # 7. HafÄ±zaya kaydet (semantik hafÄ±za)
    remember(question, llm_answer, context)

    # 7b. Decision Memory'ye kaydet (v5.3.0)
    if DECISION_MEMORY_AVAILABLE and store_decision:
        try:
            _q_score = quality_data.get("overall_score", 0) if quality_data else 0
            _q_band = quality_data.get("band", "") if quality_data else ""
            _kpi_list = kpi_impact_data.get("impacts", []) if kpi_impact_data else []
            _risk_lvl = governance_data.get("risk_level", "unknown") if governance_data else "unknown"
            _gate_v = gate_data.get("verdict", "unknown") if gate_data else "unknown"
            _unc_pct = uncertainty_data.get("uncertainty_pct", 50) if uncertainty_data else 50
            _conf_pct = dynamic_confidence * 100 if dynamic_confidence <= 1 else dynamic_confidence

            store_decision(
                question=question,
                ai_recommendation=llm_answer[:1000],
                department=context.get("dept", ""),
                quality_score=_q_score,
                quality_band=_q_band,
                kpi_impacts=_kpi_list,
                risk_level=_risk_lvl,
                gate_verdict=_gate_v,
                uncertainty=_unc_pct,
                confidence=_conf_pct,
                user_id=user_name,
            )
        except Exception as e:
            logger.debug("decision_memory_store_error", error=str(e))
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 8. OTOMATÄ°K Ã–ÄRENME â€” HER konuÅŸmadan bilgi Ã§Ä±kar ve RAG'a kaydet
    #    KullanÄ±cÄ±nÄ±n "Ã¶ÄŸren" demesine GEREK YOK.
    #    Ã–ÄŸrenme ARKA PLANDA Ã§alÄ±ÅŸÄ±r â€” yanÄ±t sÃ¼resini etkilemez.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if KNOWLEDGE_EXTRACTOR_AVAILABLE:
        import asyncio
        loop = asyncio.get_event_loop()
        loop.run_in_executor(None, _background_learn,
            question, llm_answer, user_name, context.get("dept"), bool(relevant_docs))
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 9. META LEARNING â€” Performans kaydÄ± ve self-improvement hook
    #    Her sorgu sonucunu meta_learning_engine'e kaydet.
    #    Self-improvement loop'a bildirim gÃ¶nder.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if META_LEARNING_AVAILABLE:
        try:
            _reflection_conf = reflection_data.get("confidence", 0) if reflection_data else 0
            _gov_compliance = governance_data.get("compliance_score", 1.0) if governance_data else 1.0
            _criteria = reflection_data.get("criteria_scores", {}) if reflection_data else {}
            _issues = reflection_data.get("issues", []) if reflection_data else []
            _retry_count = reflection_data.get("rounds", 0) if reflection_data else 0

            meta_result = record_query_outcome(
                question=question,
                department=context.get("dept", "genel"),
                mode=context.get("mode", "Sohbet"),
                intent=intent,
                confidence=dynamic_confidence if REFLECTION_AVAILABLE else 85,
                had_rag=bool(relevant_docs),
                had_web=web_results is not None,
                had_tools=bool(tool_results),
                reflection_pass=reflection_data.get("pass", True) if reflection_data else True,
                reflection_confidence=_reflection_conf,
                governance_compliance=_gov_compliance,
                response_time_ms=(time.time() - start_time) * 1000 if 'start_time' in dir() else 0,
                knowledge_learned=False,  # background'da belirlenecek
                knowledge_type=None,
                criteria_scores=_criteria,
                issues=_issues,
                retry_count=_retry_count,
                numerical_valid=numerical_validation.get("validated", True) if numerical_validation else True,
                source_citation_valid=source_citation_valid,
            )
            result["meta_learning"] = meta_result

            # Self-Improvement Loop hook
            if SELF_IMPROVEMENT_AVAILABLE:
                domain_key = f"{context.get('dept', 'genel')}:{context.get('mode', 'Sohbet')}"
                si_on_query_completed(
                    meta_result=meta_result,
                    domain_key=domain_key,
                    current_confidence=dynamic_confidence if REFLECTION_AVAILABLE else 85,
                )
        except Exception as e:
            logger.debug("meta_learning_error", error=str(e))

    logger.info("question_processed", 
                intent=intent,
                department=context["dept"], 
                rag_used=bool(relevant_docs),
                web_used=web_results is not None,
                memories_used=len(similar_memories))
    
    return result


def _auto_learn_from_web(question: str, web_text: str):
    """Web'den bulunan bilgiyi RAG'a kaydet â€” DEVRE DIÅI
    Web aramalarÄ± ChromaDB'yi kirletip gerÃ§ek dokÃ¼manlarÄ± bastÄ±rÄ±yordu.
    Bu fonksiyon artÄ±k kullanÄ±lmÄ±yor, referans olarak duruyor.
    """
    pass


def _background_learn(question: str, answer: str, user_name: str, department: str, had_rag: bool):
    """Arka planda Ã¶ÄŸrenme â€” run_in_executor ile Ã§aÄŸrÄ±lÄ±r, event loop'u bloklamaz."""
    try:
        result = learn_from_conversation(
            question=question,
            answer=answer,
            user_name=user_name,
            department=department,
            had_rag_docs=had_rag,
        )
        if result.get("user_learned"):
            logger.info("bg_learned", knowledge_type=result.get("knowledge_type"), user=user_name)
    except Exception as e:
        logger.debug("bg_learn_error", error=str(e))


async def get_system_status() -> dict:
    """Sistem durumu Ã¶zeti"""
    llm_available = await ollama_client.is_available()
    models = await ollama_client.get_models() if llm_available else []
    memory_size = len(recall())
    
    # RAG durumu
    rag_stats = get_rag_stats() if RAG_AVAILABLE else {"available": False}
    
    return {
        "llm_available": llm_available,
        "llm_model": ollama_client.model,
        "available_models": models,
        "memory_entries": memory_size,
        "rag": rag_stats,
        "modules": {
            "tools": TOOLS_AVAILABLE,
            "reasoning": REASONING_AVAILABLE,
            "structured_output": STRUCTURED_OUTPUT_AVAILABLE,
            "kpi_engine": KPI_ENGINE_AVAILABLE,
            "textile_knowledge": TEXTILE_AVAILABLE,
            "risk_analyzer": RISK_AVAILABLE,
            "reflection": REFLECTION_AVAILABLE,
            "agent_pipeline": AGENT_PIPELINE_AVAILABLE,
            "scenario_engine": SCENARIO_AVAILABLE,
            "monte_carlo": MONTE_CARLO_AVAILABLE,
            "decision_ranking": DECISION_RANKING_AVAILABLE,
            "governance": GOVERNANCE_AVAILABLE,
            "experiment_layer": EXPERIMENT_AVAILABLE,
            "graph_impact": GRAPH_IMPACT_AVAILABLE,
            "arima_forecasting": ARIMA_AVAILABLE,
            "sql_generator": SQL_AVAILABLE,
            "export": EXPORT_AVAILABLE,
            "web_search": WEB_SEARCH_AVAILABLE,
            "model_registry": MODEL_REGISTRY_AVAILABLE,
            "data_versioning": DATA_VERSIONING_AVAILABLE,
            "human_in_the_loop": HITL_AVAILABLE,
            "monitoring": MONITORING_AVAILABLE,
            "textile_vision": TEXTILE_VISION_AVAILABLE,
            "ocr_engine": OCR_AVAILABLE,
            "numerical_validation": NUMERICAL_VALIDATION_AVAILABLE,
            "explainability": XAI_AVAILABLE,
            "bottleneck_engine": BOTTLENECK_AVAILABLE,
            "executive_health": EXECUTIVE_HEALTH_AVAILABLE,
            "meta_learning": META_LEARNING_AVAILABLE,
            "self_improvement": SELF_IMPROVEMENT_AVAILABLE,
            "multi_agent_debate": MULTI_AGENT_DEBATE_AVAILABLE,
            "causal_inference": CAUSAL_INFERENCE_AVAILABLE,
            "strategic_planner": STRATEGIC_PLANNER_AVAILABLE,
            "executive_intelligence": EXECUTIVE_INTELLIGENCE_AVAILABLE,
            "knowledge_graph": KNOWLEDGE_GRAPH_AVAILABLE,
            "decision_gatekeeper": DECISION_GATEKEEPER_AVAILABLE,
            "uncertainty_quantification": UNCERTAINTY_AVAILABLE,
            "decision_quality": DECISION_QUALITY_AVAILABLE,
            "kpi_impact": KPI_IMPACT_AVAILABLE,
            "decision_memory": DECISION_MEMORY_AVAILABLE,
            "executive_digest": EXECUTIVE_DIGEST_AVAILABLE,
            "ood_detector": OOD_DETECTOR_AVAILABLE,
            "module_synapse": SYNAPSE_AVAILABLE,
            # v5.5.0 Enterprise Platform
            "event_bus": EVENT_BUS_AVAILABLE,
            "orchestrator": ORCHESTRATOR_AVAILABLE,
            "policy_engine": POLICY_ENGINE_AVAILABLE,
            "observability": OBSERVABILITY_AVAILABLE,
            "security_layer": SECURITY_AVAILABLE,
        },
    }


def _format_tool_result(result: dict) -> str:
    """Tool sonucunu kullanÄ±cÄ± dostu formata Ã§evir."""
    if not result:
        return ""
    
    parts = []
    for key, value in result.items():
        if key in ("error", "tool"):
            continue
        if isinstance(value, float):
            parts.append(f"{key}: {value:.2f}")
        elif isinstance(value, dict):
            inner = ", ".join(f"{k}: {v}" for k, v in value.items())
            parts.append(f"{key}: {{{inner}}}")
        elif isinstance(value, list):
            parts.append(f"{key}: {', '.join(str(v) for v in value[:5])}")
        else:
            parts.append(f"{key}: {value}")
    
    return " | ".join(parts) if parts else str(result)