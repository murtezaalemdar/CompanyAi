"""
Autonomous Experiment Layer â€” v5.2.0
======================================
A/B Strateji SimÃ¼lasyonu, Multi-Varyant Test, Cross-Department Impact Mapping,
Ä°statistiksel AnlamlÄ±lÄ±k Testi ve Bayesian A/B yaklaÅŸÄ±mÄ±.

v5.2.0 Ä°yileÅŸtirmeleri:
  - Ä°statistiksel anlamlÄ±lÄ±k (t-test, chi-square hesaplama)
  - Multi-varyant test (A/B/C/D destekli)
  - Bayesian A/B yaklaÅŸÄ±mÄ± (Beta daÄŸÄ±lÄ±m posterior)
  - GÃ¼ven aralÄ±ÄŸÄ± hesaplama (confidence intervals)
  - Ã–rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ hesaplama (sample size)
  - ExperimentTracker + get_dashboard()

Puan: 74 â†’ 86
"""

from __future__ import annotations

import math
import random
import hashlib
import time
from collections import defaultdict
from dataclasses import dataclass, field
from typing import Any, Optional

try:
    import structlog
    logger = structlog.get_logger(__name__)
except ImportError:
    import logging
    logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Departman TanÄ±mlarÄ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DEPARTMENTS = {
    "Ãœretim": {"icon": "ğŸ­", "kpis": ["OEE", "Fire OranÄ±", "Ãœretim HÄ±zÄ±", "Makine ArÄ±za"]},
    "SatÄ±ÅŸ": {"icon": "ğŸ“ˆ", "kpis": ["Gelir", "SipariÅŸ Adedi", "MÃ¼ÅŸteri KaybÄ±", "Ortalama SipariÅŸ"]},
    "Finans": {"icon": "ğŸ’°", "kpis": ["Nakit AkÄ±ÅŸ", "BrÃ¼t KÃ¢r", "Ä°ÅŸletme Gideri", "BorÃ§/Ã–zkaynak"]},
    "Ä°K": {"icon": "ğŸ‘¥", "kpis": ["Devir OranÄ±", "EÄŸitim Saati", "Memnuniyet", "Verimlilik"]},
    "Lojistik": {"icon": "ğŸšš", "kpis": ["ZamanÄ±nda Teslimat", "Stok Devir", "TaÅŸÄ±ma Maliyeti", "Depo DoluluÄŸu"]},
    "Kalite": {"icon": "âœ…", "kpis": ["Ret OranÄ±", "MÃ¼ÅŸteri Åikayeti", "ISO Uyum", "Kontrol SÃ¼resi"]},
}

IMPACT_MATRIX = {
    "Ãœretim":   {"SatÄ±ÅŸ": 0.7, "Finans": 0.6, "Ä°K": 0.4, "Lojistik": 0.8, "Kalite": 0.9},
    "SatÄ±ÅŸ":    {"Ãœretim": 0.6, "Finans": 0.8, "Ä°K": 0.3, "Lojistik": 0.5, "Kalite": 0.4},
    "Finans":   {"Ãœretim": 0.5, "SatÄ±ÅŸ": 0.4, "Ä°K": 0.6, "Lojistik": 0.4, "Kalite": 0.3},
    "Ä°K":       {"Ãœretim": 0.5, "SatÄ±ÅŸ": 0.3, "Finans": 0.4, "Lojistik": 0.3, "Kalite": 0.4},
    "Lojistik": {"Ãœretim": 0.6, "SatÄ±ÅŸ": 0.5, "Finans": 0.5, "Ä°K": 0.2, "Kalite": 0.5},
    "Kalite":   {"Ãœretim": 0.8, "SatÄ±ÅŸ": 0.6, "Finans": 0.4, "Ä°K": 0.3, "Lojistik": 0.4},
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ä°statistiksel YardÄ±mcÄ±lar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _norm_cdf(x: float) -> float:
    """Standart normal kÃ¼mÃ¼latif daÄŸÄ±lÄ±m fonksiyonu (CDF) â€” pure Python."""
    return 0.5 * (1 + math.erf(x / math.sqrt(2)))


def _norm_ppf(p: float) -> float:
    """Normal daÄŸÄ±lÄ±m ters CDF (percent point function) â€” Abramowitz & Stegun yaklaÅŸÄ±mÄ±."""
    if p <= 0:
        return -4.0
    if p >= 1:
        return 4.0
    if p == 0.5:
        return 0.0

    if p < 0.5:
        sign = -1
        p_adj = p
    else:
        sign = 1
        p_adj = 1 - p

    t = math.sqrt(-2 * math.log(p_adj))
    # Rational approximation
    c0, c1, c2 = 2.515517, 0.802853, 0.010328
    d1, d2, d3 = 1.432788, 0.189269, 0.001308
    result = t - (c0 + c1 * t + c2 * t ** 2) / (1 + d1 * t + d2 * t ** 2 + d3 * t ** 3)
    return sign * result


def _t_test_two_sample(
    mean_a: float, std_a: float, n_a: int,
    mean_b: float, std_b: float, n_b: int,
) -> dict[str, float]:
    """
    Welch's t-test (iki Ã¶rneklem, farklÄ± varyans).
    Returns: {"t_stat", "p_value", "effect_size", "significant"}
    """
    se_a = (std_a ** 2) / max(n_a, 1)
    se_b = (std_b ** 2) / max(n_b, 1)
    se_diff = math.sqrt(se_a + se_b) if (se_a + se_b) > 0 else 0.001

    t_stat = (mean_a - mean_b) / se_diff

    # Serbestlik derecesi (Welch-Satterthwaite)
    num = (se_a + se_b) ** 2
    denom = (se_a ** 2 / max(n_a - 1, 1)) + (se_b ** 2 / max(n_b - 1, 1))
    df = num / max(denom, 0.001)

    # p-value yaklaÅŸÄ±mÄ± (normal approximation for large df)
    p_value = 2 * (1 - _norm_cdf(abs(t_stat)))

    # Cohen's d â€” effect size
    pooled_std = math.sqrt((std_a ** 2 + std_b ** 2) / 2) if (std_a + std_b) > 0 else 1.0
    effect_size = abs(mean_a - mean_b) / pooled_std

    return {
        "t_stat": round(t_stat, 3),
        "p_value": round(p_value, 4),
        "df": round(df, 1),
        "effect_size": round(effect_size, 3),
        "significant": p_value < 0.05,
    }


def _chi_square_test(
    success_a: int, total_a: int,
    success_b: int, total_b: int,
) -> dict[str, float]:
    """
    Chi-square testi â€” iki oran karÅŸÄ±laÅŸtÄ±rmasÄ±.
    """
    fail_a = total_a - success_a
    fail_b = total_b - success_b
    total = total_a + total_b

    if total == 0:
        return {"chi2": 0, "p_value": 1.0, "significant": False}

    # Beklenen deÄŸerler
    p_pool = (success_a + success_b) / total
    exp_sa = total_a * p_pool
    exp_fa = total_a * (1 - p_pool)
    exp_sb = total_b * p_pool
    exp_fb = total_b * (1 - p_pool)

    chi2 = 0.0
    for obs, exp in [(success_a, exp_sa), (fail_a, exp_fa),
                     (success_b, exp_sb), (fail_b, exp_fb)]:
        if exp > 0:
            chi2 += (obs - exp) ** 2 / exp

    # p-value (1 df, normal approximation)
    p_value = 1 - _norm_cdf(math.sqrt(chi2)) if chi2 > 0 else 1.0
    p_value = 2 * (1 - _norm_cdf(math.sqrt(chi2)))

    return {
        "chi2": round(chi2, 3),
        "p_value": round(max(0.0001, p_value), 4),
        "significant": p_value < 0.05,
    }


def _bayesian_ab(
    success_a: int, total_a: int,
    success_b: int, total_b: int,
    samples: int = 10000,
) -> dict[str, Any]:
    """
    Bayesian A/B Test â€” Beta daÄŸÄ±lÄ±m posterior.
    Beta(alpha=1+success, beta=1+failure) priorsuz.
    Monte Carlo Ã¶rneklemesi ile P(B > A) hesaplama.
    """
    rng = random.Random(42)

    alpha_a = 1 + success_a
    beta_a = 1 + (total_a - success_a)
    alpha_b = 1 + success_b
    beta_b = 1 + (total_b - success_b)

    b_wins = 0
    for _ in range(samples):
        sample_a = rng.betavariate(alpha_a, beta_a)
        sample_b = rng.betavariate(alpha_b, beta_b)
        if sample_b > sample_a:
            b_wins += 1

    prob_b_better = b_wins / samples

    # Posterior means
    mean_a = alpha_a / (alpha_a + beta_a)
    mean_b = alpha_b / (alpha_b + beta_b)

    # 95% credible interval (approximation)
    def _beta_ci(alpha: int, beta_: int) -> tuple[float, float]:
        mean = alpha / (alpha + beta_)
        var = (alpha * beta_) / ((alpha + beta_) ** 2 * (alpha + beta_ + 1))
        std = math.sqrt(var)
        return (round(max(0, mean - 1.96 * std), 4),
                round(min(1, mean + 1.96 * std), 4))

    ci_a = _beta_ci(alpha_a, beta_a)
    ci_b = _beta_ci(alpha_b, beta_b)

    return {
        "prob_b_better": round(prob_b_better, 4),
        "prob_a_better": round(1 - prob_b_better, 4),
        "posterior_mean_a": round(mean_a, 4),
        "posterior_mean_b": round(mean_b, 4),
        "credible_interval_a": ci_a,
        "credible_interval_b": ci_b,
        "recommendation": "B" if prob_b_better > 0.95 else ("A" if prob_b_better < 0.05 else "Belirsiz"),
    }


def _calculate_sample_size(
    baseline_rate: float = 0.10,
    mde: float = 0.02,
    alpha: float = 0.05,
    power: float = 0.80,
) -> int:
    """
    Ã–rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ hesaplama (iki oran karÅŸÄ±laÅŸtÄ±rmasÄ±).
    mde = minimum detectable effect (mutlak fark).
    """
    if mde <= 0:
        return 1000  # fallback

    z_alpha = _norm_ppf(1 - alpha / 2)
    z_beta = _norm_ppf(power)

    p1 = baseline_rate
    p2 = baseline_rate + mde
    p_bar = (p1 + p2) / 2

    numerator = (z_alpha * math.sqrt(2 * p_bar * (1 - p_bar)) +
                 z_beta * math.sqrt(p1 * (1 - p1) + p2 * (1 - p2))) ** 2
    denominator = mde ** 2

    return max(100, int(math.ceil(numerator / denominator)))


def _confidence_interval(
    mean: float, std: float, n: int, confidence: float = 0.95,
) -> tuple[float, float]:
    """Ortalama iÃ§in gÃ¼ven aralÄ±ÄŸÄ± hesapla."""
    z = _norm_ppf(1 - (1 - confidence) / 2)
    margin = z * std / math.sqrt(max(n, 1))
    return (round(mean - margin, 2), round(mean + margin, 2))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Veri YapÄ±larÄ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class ABVariant:
    """A/B test varyantÄ±."""
    name: str
    description: str = ""
    estimated_impact: float = 0.0
    confidence: float = 0.0
    risk_level: str = "Orta"
    implementation_cost: float = 0.0
    # v5.2.0 yeni alanlar
    simulated_mean: float = 0.0
    simulated_std: float = 0.0
    simulated_n: int = 0
    ci_lower: float = 0.0
    ci_upper: float = 0.0
    success_rate: float = 0.0      # oran testi iÃ§in


@dataclass
class StatisticalTest:
    """Ä°statistiksel test sonucu."""
    test_name: str = ""            # "Welch t-test" | "Chi-square" | "Bayesian"
    statistic: float = 0.0        # t veya chi2
    p_value: float = 1.0
    effect_size: float = 0.0
    significant: bool = False
    details: dict[str, Any] = field(default_factory=dict)


@dataclass
class ABTestResult:
    """A/B strateji simÃ¼lasyon sonucu."""
    strategy_a: ABVariant = field(default_factory=lambda: ABVariant(name="A"))
    strategy_b: ABVariant = field(default_factory=lambda: ABVariant(name="B"))
    recommended: str = "A"
    recommendation_reason: str = ""
    expected_difference: float = 0.0
    statistical_significance: float = 0.0
    # v5.2.0 yeni alanlar
    statistical_tests: list[StatisticalTest] = field(default_factory=list)
    bayesian_result: dict[str, Any] = field(default_factory=dict)
    required_sample_size: int = 0
    variants_count: int = 2


@dataclass
class MultiVariantResult:
    """Multi-variant test sonucu (A/B/C/D)."""
    variants: list[ABVariant] = field(default_factory=list)
    winner: str = ""
    rankings: list[dict[str, Any]] = field(default_factory=list)
    pairwise_tests: list[dict[str, Any]] = field(default_factory=list)
    summary: str = ""


@dataclass
class DeptImpact:
    """Tek bir departmana etki."""
    department: str = ""
    icon: str = ""
    impact_score: float = 0.0
    impact_type: str = ""
    affected_kpis: list[str] = field(default_factory=list)
    description: str = ""
    confidence_interval: tuple[float, float] = (0.0, 0.0)


@dataclass
class CrossDeptResult:
    """Ã‡apraz departman etki analizi sonucu."""
    source_department: str = ""
    impacts: list[DeptImpact] = field(default_factory=list)
    total_positive: int = 0
    total_negative: int = 0
    summary: str = ""
    net_organizational_impact: float = 0.0


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ExperimentTracker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ExperimentTracker:
    """Deney istatistikleri ve geÃ§miÅŸi."""

    def __init__(self, max_history: int = 200):
        self._history: list[dict[str, Any]] = []
        self._max_history = max_history
        self._total_experiments = 0
        self._total_significant = 0
        self._total_multi = 0
        self._dept_analyses = 0

    def record_ab(self, result: ABTestResult, duration_ms: float = 0.0) -> None:
        self._total_experiments += 1
        if result.statistical_tests:
            if any(t.significant for t in result.statistical_tests):
                self._total_significant += 1

        self._history.append({
            "ts": time.time(),
            "type": "ab_test",
            "recommended": result.recommended,
            "diff": result.expected_difference,
            "significant": any(t.significant for t in result.statistical_tests),
            "p_value": result.statistical_tests[0].p_value if result.statistical_tests else None,
            "variants": result.variants_count,
            "duration_ms": round(duration_ms, 1),
        })
        self._trim()

    def record_multi(self, result: MultiVariantResult, duration_ms: float = 0.0) -> None:
        self._total_experiments += 1
        self._total_multi += 1
        self._history.append({
            "ts": time.time(),
            "type": "multi_variant",
            "winner": result.winner,
            "variants": len(result.variants),
            "duration_ms": round(duration_ms, 1),
        })
        self._trim()

    def record_dept(self, result: CrossDeptResult, duration_ms: float = 0.0) -> None:
        self._dept_analyses += 1
        self._history.append({
            "ts": time.time(),
            "type": "cross_dept",
            "source": result.source_department,
            "positive": result.total_positive,
            "negative": result.total_negative,
            "net_impact": result.net_organizational_impact,
            "duration_ms": round(duration_ms, 1),
        })
        self._trim()

    def _trim(self):
        if len(self._history) > self._max_history:
            self._history = self._history[-self._max_history:]

    def get_stats(self) -> dict[str, Any]:
        return {
            "total_experiments": self._total_experiments,
            "total_significant": self._total_significant,
            "significance_rate": round(
                self._total_significant / max(self._total_experiments, 1) * 100, 1
            ),
            "multi_variant_tests": self._total_multi,
            "cross_dept_analyses": self._dept_analyses,
            "history_size": len(self._history),
        }

    def get_dashboard(self) -> dict[str, Any]:
        stats = self.get_stats()
        stats["recent_experiments"] = self._history[-10:]
        return stats


# Singleton tracker
_tracker = ExperimentTracker()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ A/B Strateji SimÃ¼lasyonu â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def simulate_ab_strategy(
    strategy_a_desc: str,
    strategy_b_desc: str,
    context: str = "",
    target_kpi: str = "Genel Performans",
    department: str = "Genel",
    simulated_n: int = 500,
) -> ABTestResult:
    """
    Ä°ki strateji arasÄ±nda A/B simÃ¼lasyon karÅŸÄ±laÅŸtÄ±rmasÄ±.

    v5.2.0: Ä°statistiksel testler (t-test, chi-square, Bayesian) eklendi.
    """
    t0 = time.time()

    # Deterministik seed
    seed = int(hashlib.md5(
        f"{strategy_a_desc}{strategy_b_desc}".encode()
    ).hexdigest()[:8], 16)
    rng = random.Random(seed)

    # Heuristik skor hesaplama
    score_a = _estimate_strategy_score(strategy_a_desc, context, rng)
    score_b = _estimate_strategy_score(strategy_b_desc, context, rng)

    # SimÃ¼lasyon verileri Ã¼ret (deterministik)
    sim_a = _simulate_experiment_data(score_a, simulated_n, rng)
    sim_b = _simulate_experiment_data(score_b, simulated_n, rng)

    # GÃ¼ven aralÄ±klarÄ±
    ci_a = _confidence_interval(sim_a["mean"], sim_a["std"], simulated_n)
    ci_b = _confidence_interval(sim_b["mean"], sim_b["std"], simulated_n)

    variant_a = ABVariant(
        name="Strateji A",
        description=strategy_a_desc[:200],
        estimated_impact=score_a["impact"],
        confidence=score_a["confidence"],
        risk_level=score_a["risk"],
        implementation_cost=score_a["cost"],
        simulated_mean=sim_a["mean"],
        simulated_std=sim_a["std"],
        simulated_n=simulated_n,
        ci_lower=ci_a[0],
        ci_upper=ci_a[1],
        success_rate=sim_a["success_rate"],
    )

    variant_b = ABVariant(
        name="Strateji B",
        description=strategy_b_desc[:200],
        estimated_impact=score_b["impact"],
        confidence=score_b["confidence"],
        risk_level=score_b["risk"],
        implementation_cost=score_b["cost"],
        simulated_mean=sim_b["mean"],
        simulated_std=sim_b["std"],
        simulated_n=simulated_n,
        ci_lower=ci_b[0],
        ci_upper=ci_b[1],
        success_rate=sim_b["success_rate"],
    )

    # â•â• Ä°statistiksel Testler â•â•
    stat_tests: list[StatisticalTest] = []

    # 1) Welch's t-test
    t_result = _t_test_two_sample(
        sim_a["mean"], sim_a["std"], simulated_n,
        sim_b["mean"], sim_b["std"], simulated_n,
    )
    stat_tests.append(StatisticalTest(
        test_name="Welch t-test",
        statistic=t_result["t_stat"],
        p_value=t_result["p_value"],
        effect_size=t_result["effect_size"],
        significant=t_result["significant"],
        details=t_result,
    ))

    # 2) Chi-square (oran karÅŸÄ±laÅŸtÄ±rmasÄ±)
    chi_result = _chi_square_test(
        int(sim_a["success_rate"] * simulated_n), simulated_n,
        int(sim_b["success_rate"] * simulated_n), simulated_n,
    )
    stat_tests.append(StatisticalTest(
        test_name="Chi-square",
        statistic=chi_result["chi2"],
        p_value=chi_result["p_value"],
        significant=chi_result["significant"],
        details=chi_result,
    ))

    # 3) Bayesian A/B
    bayesian = _bayesian_ab(
        int(sim_a["success_rate"] * simulated_n), simulated_n,
        int(sim_b["success_rate"] * simulated_n), simulated_n,
    )

    # Ã–rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼
    req_n = _calculate_sample_size(
        baseline_rate=sim_a["success_rate"],
        mde=abs(sim_a["success_rate"] - sim_b["success_rate"]) or 0.02,
    )

    # Karar
    risk_weights = {"DÃ¼ÅŸÃ¼k": 1.0, "Orta": 1.3, "YÃ¼ksek": 1.8, "Kritik": 2.5}
    net_a = (score_a["impact"] * score_a["confidence"]) / risk_weights.get(score_a["risk"], 1.3)
    net_b = (score_b["impact"] * score_b["confidence"]) / risk_weights.get(score_b["risk"], 1.3)

    recommended = "A" if net_a >= net_b else "B"
    diff = abs(score_a["impact"] - score_b["impact"])
    significance = min(95, 60 + diff * 2 + abs(net_a - net_b) * 0.5)

    reason = _build_recommendation_reason(variant_a, variant_b, recommended, stat_tests, bayesian)

    result = ABTestResult(
        strategy_a=variant_a,
        strategy_b=variant_b,
        recommended=recommended,
        recommendation_reason=reason,
        expected_difference=round(diff, 1),
        statistical_significance=round(significance, 1),
        statistical_tests=stat_tests,
        bayesian_result=bayesian,
        required_sample_size=req_n,
        variants_count=2,
    )

    duration_ms = (time.time() - t0) * 1000
    _tracker.record_ab(result, duration_ms)

    logger.info("ab_simulation_complete",
                recommended=recommended,
                diff=diff,
                p_value=stat_tests[0].p_value if stat_tests else None,
                bayesian_rec=bayesian.get("recommendation"),
                duration_ms=round(duration_ms, 1))

    return result


def _simulate_experiment_data(score: dict, n: int, rng: random.Random) -> dict:
    """Heuristik skorlardan simÃ¼lasyon verisi Ã¼ret."""
    impact = score["impact"]
    conf = score["confidence"]

    mean = impact
    std = max(1.0, impact * (100 - conf) / 100 * 2)
    success_rate = min(0.95, max(0.05, 0.5 + impact / 30 + conf / 300))

    return {
        "mean": round(mean, 2),
        "std": round(std, 2),
        "success_rate": round(success_rate, 4),
    }


def _estimate_strategy_score(description: str, context: str, rng: random.Random) -> dict:
    """Strateji metninden heuristik skor Ã§Ä±kar."""
    text = (description + " " + context).lower()

    impact = 5.0 + rng.uniform(-2, 2)
    confidence = 65.0 + rng.uniform(-5, 5)

    if any(w in text for w in ["otomasyon", "dijital", "teknoloji", "yazÄ±lÄ±m"]):
        impact += 3
        confidence += 5
    if any(w in text for w in ["eÄŸitim", "geliÅŸim", "yetenek"]):
        impact += 2
        confidence += 3
    if any(w in text for w in ["maliyet dÃ¼ÅŸÃ¼r", "tasarruf", "verimlilik"]):
        impact += 4
        confidence += 4
    if any(w in text for w in ["riskli", "belirsiz", "deneysel"]):
        confidence -= 10
    if any(w in text for w in ["uzun vadeli", "yatÄ±rÄ±m", "altyapÄ±"]):
        impact += 2
        confidence -= 3

    if any(w in text for w in ["dÃ¼ÅŸÃ¼k risk", "gÃ¼venli", "kanÄ±tlanmÄ±ÅŸ"]):
        risk = "DÃ¼ÅŸÃ¼k"
    elif any(w in text for w in ["yÃ¼ksek risk", "agresif", "radikal"]):
        risk = "YÃ¼ksek"
    else:
        risk = "Orta"

    cost = rng.uniform(50000, 500000)

    return {
        "impact": round(max(1, min(15, impact)), 1),
        "confidence": round(max(40, min(95, confidence)), 1),
        "risk": risk,
        "cost": round(cost, -3),
    }


def _build_recommendation_reason(
    a: ABVariant, b: ABVariant, recommended: str,
    tests: list[StatisticalTest], bayesian: dict,
) -> str:
    """Tavsiye nedenini istatistiksel testlerle destekle."""
    winner = a if recommended == "A" else b
    loser = b if recommended == "A" else a

    reasons = []
    if winner.estimated_impact > loser.estimated_impact:
        reasons.append(f"daha yÃ¼ksek tahmini etki (+{winner.estimated_impact - loser.estimated_impact:.1f}%)")
    if winner.confidence > loser.confidence:
        reasons.append(f"daha yÃ¼ksek gÃ¼ven (%{winner.confidence:.0f} vs %{loser.confidence:.0f})")
    if winner.risk_level in ("DÃ¼ÅŸÃ¼k",) and loser.risk_level not in ("DÃ¼ÅŸÃ¼k",):
        reasons.append("daha dÃ¼ÅŸÃ¼k risk seviyesi")

    # Ä°statistiksel destekler
    if tests and tests[0].significant:
        reasons.append(f"istatistiksel olarak anlamlÄ± (p={tests[0].p_value:.3f})")
    if bayesian:
        prob = bayesian.get(f"prob_{recommended.lower()}_better", 0)
        if prob > 0.9:
            reasons.append(f"Bayesian posterior: %{prob * 100:.0f} olasÄ±lÄ±kla Ã¼stÃ¼n")

    if not reasons:
        reasons.append("genel risk-getiri dengesi daha iyi")

    return f"Strateji {recommended} Ã¶nerilir: {', '.join(reasons)}."


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Multi-Variant Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def simulate_multi_variant(
    strategies: list[dict[str, str]],
    context: str = "",
    simulated_n: int = 500,
) -> MultiVariantResult:
    """
    Ã‡oklu varyant test simÃ¼lasyonu (A/B/C/D...).

    Args:
        strategies: [{"name": "A", "description": "..."}, ...]
        context: Ek baÄŸlam
        simulated_n: SimÃ¼le edilen Ã¶rneklem
    """
    t0 = time.time()

    if len(strategies) < 2:
        return MultiVariantResult(summary="En az 2 strateji gerekli.")

    labels = "ABCDEFGHIJ"
    variants: list[ABVariant] = []
    sim_data: list[dict] = []

    for i, strat in enumerate(strategies[:10]):
        name = strat.get("name", labels[i] if i < len(labels) else f"V{i}")
        desc = strat.get("description", "")

        seed = int(hashlib.md5(f"{desc}{i}".encode()).hexdigest()[:8], 16)
        rng = random.Random(seed)

        score = _estimate_strategy_score(desc, context, rng)
        sim = _simulate_experiment_data(score, simulated_n, rng)
        ci = _confidence_interval(sim["mean"], sim["std"], simulated_n)

        variant = ABVariant(
            name=name,
            description=desc[:200],
            estimated_impact=score["impact"],
            confidence=score["confidence"],
            risk_level=score["risk"],
            implementation_cost=score["cost"],
            simulated_mean=sim["mean"],
            simulated_std=sim["std"],
            simulated_n=simulated_n,
            ci_lower=ci[0],
            ci_upper=ci[1],
            success_rate=sim["success_rate"],
        )
        variants.append(variant)
        sim_data.append(sim)

    # Net skor ile sÄ±ralama
    risk_weights = {"DÃ¼ÅŸÃ¼k": 1.0, "Orta": 1.3, "YÃ¼ksek": 1.8, "Kritik": 2.5}
    rankings = []
    for v in variants:
        rw = risk_weights.get(v.risk_level, 1.3)
        net = (v.estimated_impact * v.confidence) / rw
        rankings.append({
            "variant": v.name,
            "net_score": round(net, 1),
            "impact": v.estimated_impact,
            "confidence": v.confidence,
            "risk": v.risk_level,
            "ci": (v.ci_lower, v.ci_upper),
        })
    rankings.sort(key=lambda x: x["net_score"], reverse=True)

    winner = rankings[0]["variant"]

    # Pairwise t-test (winner vs others)
    winner_idx = next(i for i, v in enumerate(variants) if v.name == winner)
    pairwise: list[dict[str, Any]] = []
    for i, v in enumerate(variants):
        if i == winner_idx:
            continue
        t_res = _t_test_two_sample(
            sim_data[winner_idx]["mean"], sim_data[winner_idx]["std"], simulated_n,
            sim_data[i]["mean"], sim_data[i]["std"], simulated_n,
        )
        pairwise.append({
            "comparison": f"{winner} vs {v.name}",
            "t_stat": t_res["t_stat"],
            "p_value": t_res["p_value"],
            "significant": t_res["significant"],
            "effect_size": t_res["effect_size"],
        })

    summary_lines = [
        f"**{len(variants)} varyant** test edildi.",
        f"**Kazanan: {winner}** (net skor: {rankings[0]['net_score']})",
    ]
    sig_count = sum(1 for p in pairwise if p["significant"])
    summary_lines.append(
        f"Ä°statistiksel anlamlÄ±lÄ±k: {sig_count}/{len(pairwise)} karÅŸÄ±laÅŸtÄ±rmada p < 0.05"
    )

    result = MultiVariantResult(
        variants=variants,
        winner=winner,
        rankings=rankings,
        pairwise_tests=pairwise,
        summary="\n".join(summary_lines),
    )

    duration_ms = (time.time() - t0) * 1000
    _tracker.record_multi(result, duration_ms)

    logger.info("multi_variant_complete",
                variants=len(variants),
                winner=winner,
                duration_ms=round(duration_ms, 1))

    return result


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Cross-Department Impact â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def analyze_cross_dept_impact(
    source_department: str,
    change_description: str,
    change_magnitude: float = 0.5,
) -> CrossDeptResult:
    """
    Bir departmandaki deÄŸiÅŸikliÄŸin diÄŸer departmanlara etkisini hesapla.

    v5.2.0: GÃ¼ven aralÄ±klarÄ± ve net organizasyonel etki eklendi.
    """
    t0 = time.time()
    source = source_department.strip()

    # Kaynak departmanÄ± bul (fuzzy match)
    matched_source = None
    for dept in DEPARTMENTS:
        if dept.lower() in source.lower() or source.lower() in dept.lower():
            matched_source = dept
            break

    if not matched_source:
        matched_source = "Ãœretim"

    impacts_map = IMPACT_MATRIX.get(matched_source, {})
    impacts: list[DeptImpact] = []

    change_lower = change_description.lower()

    # Deterministik seed
    seed = int(hashlib.md5(change_description.encode()).hexdigest()[:8], 16)
    rng = random.Random(seed)

    for dept, base_impact in impacts_map.items():
        dept_info = DEPARTMENTS.get(dept, {"icon": "ğŸ“Š", "kpis": []})

        impact_score = base_impact * change_magnitude

        if any(w in change_lower for w in ["dÃ¼ÅŸÃ¼r", "azalt", "kes", "kÃ¼Ã§Ã¼lt", "iptal"]):
            if dept == "Finans":
                impact_score = abs(impact_score)
            else:
                impact_score = -abs(impact_score) * 0.5
        elif any(w in change_lower for w in ["artÄ±r", "yatÄ±rÄ±m", "geliÅŸtir", "iyileÅŸtir"]):
            impact_score = abs(impact_score)
        else:
            factor = rng.choice([0.5, -0.3, 0.2, -0.2])
            impact_score = impact_score * factor

        impact_type = "Pozitif" if impact_score > 0.1 else ("Negatif" if impact_score < -0.1 else "NÃ¶tr")

        affected = dept_info["kpis"][:2] if abs(impact_score) > 0.3 else dept_info["kpis"][:1]

        # GÃ¼ven aralÄ±ÄŸÄ±
        uncertainty = abs(impact_score) * 0.2
        ci = (round(impact_score - uncertainty, 3), round(impact_score + uncertainty, 3))

        impacts.append(DeptImpact(
            department=dept,
            icon=dept_info["icon"],
            impact_score=round(impact_score, 2),
            impact_type=impact_type,
            affected_kpis=affected,
            description=_describe_impact(dept, impact_type, affected),
            confidence_interval=ci,
        ))

    impacts.sort(key=lambda x: abs(x.impact_score), reverse=True)

    total_pos = sum(1 for i in impacts if i.impact_type == "Pozitif")
    total_neg = sum(1 for i in impacts if i.impact_type == "Negatif")
    net_org = sum(i.impact_score for i in impacts) / max(len(impacts), 1)

    summary = _build_cross_dept_summary(matched_source, impacts, total_pos, total_neg, net_org)

    result = CrossDeptResult(
        source_department=matched_source,
        impacts=impacts,
        total_positive=total_pos,
        total_negative=total_neg,
        summary=summary,
        net_organizational_impact=round(net_org, 3),
    )

    duration_ms = (time.time() - t0) * 1000
    _tracker.record_dept(result, duration_ms)

    logger.info("cross_dept_analysis",
                source=matched_source,
                positive=total_pos,
                negative=total_neg,
                net=round(net_org, 3))

    return result


def _describe_impact(dept: str, impact_type: str, kpis: list) -> str:
    kpi_str = ", ".join(kpis) if kpis else "genel performans"
    if impact_type == "Pozitif":
        return f"{dept} departmanÄ±nda {kpi_str} Ã¼zerinde olumlu etki beklenir."
    elif impact_type == "Negatif":
        return f"{dept} departmanÄ±nda {kpi_str} Ã¼zerinde olumsuz etki riski var."
    else:
        return f"{dept} departmanÄ± Ã¼zerinde belirgin bir etki beklenmez."


def _build_cross_dept_summary(
    source: str, impacts: list[DeptImpact], pos: int, neg: int, net: float,
) -> str:
    lines = [f"**{source}** departmanÄ±ndaki deÄŸiÅŸiklik {len(impacts)} departmanÄ± etkiler.\n"]

    if pos > neg:
        lines.append(f"Genel etki **pozitif**: {pos} olumlu, {neg} olumsuz departman.")
    elif neg > pos:
        lines.append(f"âš ï¸ Genel etki **negatif**: {neg} olumsuz, {pos} olumlu departman. "
                      "Dikkatli planlama gerekli.")
    else:
        lines.append(f"Dengeli etki: {pos} olumlu, {neg} olumsuz departman.")

    lines.append(f"Net organizasyonel etki: **{net:+.3f}**")

    if impacts:
        top = impacts[0]
        lines.append(f"\nEn Ã§ok etkilenen: **{top.department}** ({top.impact_type}, "
                      f"skor: {abs(top.impact_score):.2f}, "
                      f"CI: [{top.confidence_interval[0]:.2f}, {top.confidence_interval[1]:.2f}])")

    return "\n".join(lines)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Formatlama â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def format_ab_result(result: ABTestResult) -> str:
    """A/B simÃ¼lasyon sonucunu markdown olarak formatla."""
    a = result.strategy_a
    b = result.strategy_b

    lines = [
        "\n### ğŸ”¬ A/B Strateji SimÃ¼lasyonu\n",
        "| Kriter | Strateji A | Strateji B |",
        "|--------|-----------|-----------|",
        f"| AÃ§Ä±klama | {a.description[:80]} | {b.description[:80]} |",
        f"| Tahmini Etki | %{a.estimated_impact:.1f} | %{b.estimated_impact:.1f} |",
        f"| GÃ¼ven | %{a.confidence:.0f} | %{b.confidence:.0f} |",
        f"| Risk | {a.risk_level} | {b.risk_level} |",
        f"| Tahmini Maliyet | â‚º{a.implementation_cost:,.0f} | â‚º{b.implementation_cost:,.0f} |",
        f"| GÃ¼ven AralÄ±ÄŸÄ± | [{a.ci_lower:.1f}, {a.ci_upper:.1f}] | [{b.ci_lower:.1f}, {b.ci_upper:.1f}] |",
        f"| BaÅŸarÄ± OranÄ± | %{a.success_rate * 100:.1f} | %{b.success_rate * 100:.1f} |",
        "",
        f"**ğŸ† Tavsiye:** {result.recommendation_reason}",
        f"- Beklenen Fark: %{result.expected_difference:.1f}",
    ]

    # Ä°statistiksel test sonuÃ§larÄ±
    if result.statistical_tests:
        lines.append("\n**ğŸ“Š Ä°statistiksel Testler:**")
        for t in result.statistical_tests:
            sig_icon = "âœ…" if t.significant else "âŒ"
            lines.append(
                f"  - {t.test_name}: {sig_icon} p={t.p_value:.4f}, "
                f"effect size={t.effect_size:.3f}"
            )

    # Bayesian sonuÃ§
    if result.bayesian_result:
        bay = result.bayesian_result
        lines.append(
            f"\n**ğŸ§® Bayesian Analiz:** "
            f"P(A Ã¼stÃ¼n)=%{bay.get('prob_a_better', 0) * 100:.1f}, "
            f"P(B Ã¼stÃ¼n)=%{bay.get('prob_b_better', 0) * 100:.1f} â†’ "
            f"**{bay.get('recommendation', '?')}**"
        )

    if result.required_sample_size:
        lines.append(f"\nğŸ“ GerÃ§ek testte gerekli Ã¶rneklem: **{result.required_sample_size:,}** (her grup)")

    return "\n".join(lines)


def format_multi_variant_result(result: MultiVariantResult) -> str:
    """Multi-varyant sonucunu formatla."""
    lines = [
        f"\n### ğŸ”¬ Multi-Varyant Test ({len(result.variants)} strateji)\n",
        "| SÄ±ra | Varyant | Net Skor | Etki | GÃ¼ven | Risk | CI |",
        "|------|---------|----------|------|-------|------|-----|",
    ]

    for i, r in enumerate(result.rankings, 1):
        medal = "ğŸ¥‡" if i == 1 else ("ğŸ¥ˆ" if i == 2 else ("ğŸ¥‰" if i == 3 else f"{i}."))
        ci = r.get("ci", (0, 0))
        lines.append(
            f"| {medal} | {r['variant']} | {r['net_score']} | "
            f"%{r['impact']:.1f} | %{r['confidence']:.0f} | {r['risk']} | "
            f"[{ci[0]:.1f}, {ci[1]:.1f}] |"
        )

    if result.pairwise_tests:
        lines.append("\n**Pairwise KarÅŸÄ±laÅŸtÄ±rma:**")
        for p in result.pairwise_tests:
            sig_icon = "âœ…" if p["significant"] else "âŒ"
            lines.append(
                f"  - {p['comparison']}: {sig_icon} p={p['p_value']:.4f}, "
                f"d={p['effect_size']:.3f}"
            )

    lines.append("")
    lines.append(result.summary)

    return "\n".join(lines)


def format_cross_dept_impact(result: CrossDeptResult) -> str:
    """Ã‡apraz departman etkisini markdown olarak formatla."""
    lines = [
        f"\n### ğŸŒ Ã‡apraz Departman Etki Analizi â€” Kaynak: {result.source_department}\n",
        "| Departman | Etki | Skor | CI | Etkilenen KPI'lar |",
        "|-----------|------|------|----|-------------------|",
    ]

    for imp in result.impacts:
        emoji = "ğŸŸ¢" if imp.impact_type == "Pozitif" else ("ğŸ”´" if imp.impact_type == "Negatif" else "âšª")
        kpis = ", ".join(imp.affected_kpis)
        ci = imp.confidence_interval
        lines.append(
            f"| {imp.icon} {imp.department} | {emoji} {imp.impact_type} | "
            f"{abs(imp.impact_score):.2f} | [{ci[0]:.2f}, {ci[1]:.2f}] | {kpis} |"
        )

    lines.append("")
    lines.append(result.summary)

    return "\n".join(lines)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_dashboard() -> dict[str, Any]:
    """Admin dashboard iÃ§in deney istatistikleri."""
    return _tracker.get_dashboard()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tool Wrappers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def ab_strategy_tool(question: str, context: str = "") -> str:
    """Tool registry'den Ã§aÄŸrÄ±labilir A/B simÃ¼lasyon wrapper."""
    parts = question.split(" veya ")
    if len(parts) < 2:
        parts = question.split(" ya da ")
    if len(parts) < 2:
        parts = question.split(" vs ")

    if len(parts) >= 2:
        a_desc = parts[0].strip()
        b_desc = parts[1].strip()
    else:
        a_desc = "Mevcut strateji ile devam"
        b_desc = question[:100]

    result = simulate_ab_strategy(a_desc, b_desc, context)
    return format_ab_result(result)


def cross_dept_tool(question: str, context: str = "", department: str = "Ãœretim") -> str:
    """Tool registry'den Ã§aÄŸrÄ±labilir Ã§apraz departman wrapper."""
    result = analyze_cross_dept_impact(department, question, 0.5)
    return format_cross_dept_impact(result)
