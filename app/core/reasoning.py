"""Ã‡ok AdÄ±mlÄ± Reasoning Engine â€” ReAct Pattern (v2)

KarmaÅŸÄ±k sorularÄ± adÄ±m adÄ±m Ã§Ã¶zer:
1. Thought (DÃ¼ÅŸÃ¼n) â€” Soruyu analiz et
2. Action (Eylem) â€” AraÃ§ kullan veya bilgi topla
3. Observation (GÃ¶zlem) â€” Sonucu deÄŸerlendir
4. ... (tekrarla)
5. Final Answer â€” Son yanÄ±t

Ã–zellikler:
- Dinamik derinlik (5-10 adÄ±m arasÄ±, soru karmaÅŸÄ±klÄ±ÄŸÄ±na gÃ¶re)
- Backtracking: baÅŸarÄ±sÄ±z adÄ±mlarda alternatif eylem dene
- GÃ¼ven bazlÄ± dallanma: yÃ¼ksek gÃ¼vende erken dur, dÃ¼ÅŸÃ¼kte devam et
- ReasoningTree: dallanmalÄ± akÄ±l yÃ¼rÃ¼tme, en iyi dalÄ± seÃ§
- AdÄ±m meta verileri: sÃ¼re, token, gÃ¼ven
- ReasoningEngine singleton: merkezi yÃ¶netim, dashboard, istatistik
"""

import re
import time
import structlog
from dataclasses import dataclass, field
from typing import Optional

logger = structlog.get_logger()

MAX_REASONING_STEPS = 5

# ---------------------------------------------------------------------------
# YardÄ±mcÄ± veri yapÄ±larÄ±
# ---------------------------------------------------------------------------

@dataclass
class StepMetadata:
    """Her reasoning adÄ±mÄ±na ait meta veri."""
    duration_ms: float = 0.0
    tokens_used: int = 0
    confidence: float = 0.0
    backtracked: bool = False
    alternative_action: Optional[str] = None


class ReasoningStep:
    """Tek bir reasoning adÄ±mÄ±."""
    def __init__(self, step_num: int, thought: str = "", action: str = "",
                 action_input: dict = None, observation: str = ""):
        self.step_num = step_num
        self.thought = thought
        self.action = action
        self.action_input = action_input or {}
        self.observation = observation
        # v2: meta veri alanlarÄ±
        self.metadata = StepMetadata()

    def to_dict(self) -> dict:
        return {
            "step": self.step_num,
            "thought": self.thought,
            "action": self.action,
            "action_input": self.action_input,
            "observation": self.observation,
            "metadata": {
                "duration_ms": self.metadata.duration_ms,
                "tokens_used": self.metadata.tokens_used,
                "confidence": self.metadata.confidence,
                "backtracked": self.metadata.backtracked,
            },
        }


class ReasoningChain:
    """TÃ¼m reasoning zinciri."""
    def __init__(self, question: str, conversation_context: str = ""):
        self.question = question
        self.conversation_context = conversation_context  # Ã–nceki konuÅŸma baÄŸlamÄ±
        self.steps: list[ReasoningStep] = []
        self.final_answer: str = ""
        self.confidence: float = 0.0
        self.tools_used: list[str] = []

    def add_step(self, step: ReasoningStep):
        self.steps.append(step)

    def to_dict(self) -> dict:
        return {
            "question": self.question,
            "steps": [s.to_dict() for s in self.steps],
            "final_answer": self.final_answer,
            "confidence": self.confidence,
            "tools_used": self.tools_used,
            "total_steps": len(self.steps),
            "conversation_aware": bool(self.conversation_context),
        }

    def get_context_for_llm(self) -> str:
        """LLM'e gÃ¶nderilecek reasoning context."""
        text = ""
        # KonuÅŸma baÄŸlamÄ± varsa ekle
        if self.conversation_context:
            text += f"## KonuÅŸma GeÃ§miÅŸi (Ã–nceki BaÄŸlam)\n{self.conversation_context}\n\n"

        text += f"## Reasoning Zinciri\nSoru: {self.question}\n\n"
        for step in self.steps:
            text += f"### AdÄ±m {step.step_num}\n"
            if step.thought:
                text += f"**DÃ¼ÅŸÃ¼nce**: {step.thought}\n"
            if step.action:
                text += f"**Eylem**: {step.action}({step.action_input})\n"
            if step.observation:
                text += f"**GÃ¶zlem**: {step.observation}\n"
            text += "\n"
        return text

    # v2 yardÄ±mcÄ± metotlar --------------------------------------------------
    def get_avg_step_confidence(self) -> float:
        """AdÄ±m gÃ¼venlerinin ortalamasÄ±."""
        confs = [s.metadata.confidence for s in self.steps if s.metadata.confidence > 0]
        return sum(confs) / len(confs) if confs else 0.0

    def total_duration_ms(self) -> float:
        """Toplam zincir sÃ¼resi (ms)."""
        return sum(s.metadata.duration_ms for s in self.steps)

    def backtrack_count(self) -> int:
        """KaÃ§ adÄ±mda backtracking yapÄ±ldÄ±."""
        return sum(1 for s in self.steps if s.metadata.backtracked)


# ---------------------------------------------------------------------------
# ReasoningTree â€” dallanmalÄ± akÄ±l yÃ¼rÃ¼tme
# ---------------------------------------------------------------------------

class ReasoningBranch:
    """Bir reasoning dalÄ± â€” kendi adÄ±m listesi ve skoru var."""
    def __init__(self, branch_id: int, description: str = ""):
        self.branch_id = branch_id
        self.description = description
        self.steps: list[ReasoningStep] = []
        self.score: float = 0.0
        self.final_answer: str = ""

    def add_step(self, step: ReasoningStep):
        self.steps.append(step)

    def compute_score(self) -> float:
        """Dal skorunu hesapla: gÃ¼ven + adÄ±m Ã§eÅŸitliliÄŸi + gÃ¶zlem kalitesi."""
        if not self.steps:
            return 0.0
        conf_avg = sum(s.metadata.confidence for s in self.steps) / len(self.steps)
        obs_quality = sum(1 for s in self.steps if s.observation and len(s.observation) > 20) / len(self.steps)
        backtrack_penalty = sum(1 for s in self.steps if s.metadata.backtracked) * 0.05
        self.score = round(min(1.0, conf_avg * 0.6 + obs_quality * 0.4 - backtrack_penalty), 3)
        return self.score


class ReasoningTree:
    """DallanmalÄ± reasoning aÄŸacÄ± â€” birden fazla yol dener, en iyisini seÃ§er."""
    def __init__(self, question: str):
        self.question = question
        self.branches: list[ReasoningBranch] = []
        self._next_id = 0

    def create_branch(self, description: str = "") -> ReasoningBranch:
        """Yeni dal oluÅŸtur."""
        branch = ReasoningBranch(self._next_id, description)
        self._next_id += 1
        self.branches.append(branch)
        return branch

    def best_branch(self) -> Optional[ReasoningBranch]:
        """En yÃ¼ksek skorlu dalÄ± dÃ¶ndÃ¼r."""
        if not self.branches:
            return None
        for b in self.branches:
            b.compute_score()
        return max(self.branches, key=lambda b: b.score)

    def to_dict(self) -> dict:
        return {
            "question": self.question,
            "branches": [
                {
                    "id": b.branch_id,
                    "desc": b.description,
                    "steps": len(b.steps),
                    "score": b.compute_score(),
                }
                for b in self.branches
            ],
            "best_branch_id": self.best_branch().branch_id if self.branches else None,
        }


# ---------------------------------------------------------------------------
# KarmaÅŸÄ±klÄ±k & planlama fonksiyonlarÄ±
# ---------------------------------------------------------------------------

def _compute_complexity_score(question: str, context: dict) -> int:
    """Sorunun karmaÅŸÄ±klÄ±k puanÄ±nÄ± hesapla (0-15 arasÄ±)."""
    q = question.lower()
    signals = [
        # KarÅŸÄ±laÅŸtÄ±rma
        bool(re.search(r'(karÅŸÄ±laÅŸtÄ±r|kÄ±yasla|versus|vs|fark)', q)),
        # Ã‡ok parÃ§alÄ± soru
        bool(re.search(r'(ve|ayrÄ±ca|bunun\s+yanÄ±nda|hem.*hem)', q)),
        # KoÅŸullu analiz
        bool(re.search(r'(eÄŸer|durumunda|olursa|varsayalÄ±m)', q)),
        # Ã‡ok adÄ±mlÄ± hesaplama
        bool(re.search(r'(hesapla.*ve.*yorumla|analiz.*et.*Ã¶ner)', q)),
        # Neden-sonuÃ§
        bool(re.search(r'(neden.*ve.*ne\s*yapÄ±lmalÄ±|sebep.*Ã§Ã¶zÃ¼m)', q)),
        # Tahmin + analiz
        bool(re.search(r'(tahmin|Ã¶ngÃ¶r|projeksiyon|gelecek)', q)),
        # Risk + aksiyon
        bool(re.search(r'risk.*aksiyon|tehlike.*Ã¶nlem', q)),
        # Soru uzunluÄŸu (30+ kelime genelde karmaÅŸÄ±k)
        len(q.split()) > 30,
        # v2: Veri referansÄ± (tablo, grafik, excel, rapor)
        bool(re.search(r'(tablo|grafik|excel|rapor|veri\s*seti|csv|json)', q)),
        # v2: Ã‡oklu metrik analizi
        bool(re.search(r'(oran.*oran|metrik.*metrik|kpi.*kpi)', q)),
        # v2: Zamansal karÅŸÄ±laÅŸtÄ±rma
        bool(re.search(r'(geÃ§en\s*(ay|yÄ±l|hafta).*bu\s*(ay|yÄ±l|hafta)|dÃ¶nem.*karÅŸÄ±laÅŸtÄ±r)', q)),
        # v2: Ã‡apraz referans (birden fazla kaynak)
        bool(re.search(r'(kaynak.*kaynak|sistem.*sistem|farklÄ±.*veri)', q)),
        # v2: Derinlemesine analiz isteÄŸi
        bool(re.search(r'(detay|derinlemesine|kapsamlÄ±|ayrÄ±ntÄ±lÄ±)', q)),
        # v2: Ã‡oklu soru cÃ¼mlesi (birden fazla soru iÅŸareti)
        q.count('?') >= 2,
        # v2: Optimizasyon / iyileÅŸtirme sorusu
        bool(re.search(r'(optimiz|iyileÅŸtir|verimlilik|minimize|maksimize)', q)),
    ]
    return sum(signals)


def _compute_dynamic_max_steps(complexity_score: int) -> int:
    """KarmaÅŸÄ±klÄ±ÄŸa gÃ¶re dinamik adÄ±m limiti (5-10 arasÄ±)."""
    if complexity_score <= 2:
        return 5
    elif complexity_score <= 4:
        return 6
    elif complexity_score <= 6:
        return 7
    elif complexity_score <= 8:
        return 8
    elif complexity_score <= 10:
        return 9
    else:
        return 10


def needs_multi_step(question: str, context: dict) -> bool:
    """Bu soru Ã§ok adÄ±mlÄ± reasoning gerektiriyor mu?"""
    complexity_score = _compute_complexity_score(question, context)

    # Intent iÅŸ/analiz ise ve 2+ sinyal varsa multi-step
    intent = context.get("intent", "sohbet")
    if intent in ("iÅŸ", "bilgi") and complexity_score >= 2:
        return True
    if complexity_score >= 3:
        return True

    return False


# Eylem tipleri â€” v2 geniÅŸletilmiÅŸ
_ACTION_TYPES = {
    "analyze_question": "Soruyu analiz et",
    "analyze_data": "Veri analizi yap",
    "search_documents": "Bilgi tabanÄ±nda ara",
    "web_search": "GÃ¼ncel bilgi ara (web)",
    "calculate": "Hesaplama yap",
    "interpret": "SonuÃ§larÄ± yorumla",
    "synthesize": "BulgularÄ± sentezle",
    # v2 yeni eylem tipleri
    "verify_result": "Sonucu doÄŸrula / Ã§apraz kontrol",
    "compare": "DeÄŸerleri karÅŸÄ±laÅŸtÄ±r",
    "deep_dive": "Derinlemesine analiz yap",
    "cross_reference": "Ã‡apraz referans kontrolÃ¼",
}

# Alternatif eylemler â€” backtracking sÄ±rasÄ±nda kullanÄ±lÄ±r
_FALLBACK_ACTIONS = {
    "search_documents": "web_search",
    "web_search": "search_documents",
    "calculate": "interpret",
    "analyze_data": "search_documents",
    "verify_result": "interpret",
    "compare": "interpret",
    "deep_dive": "analyze_data",
    "cross_reference": "search_documents",
}


def plan_reasoning_steps(question: str, context: dict) -> list[dict]:
    """Soru iÃ§in reasoning planÄ± oluÅŸtur (v2 zenginleÅŸtirilmiÅŸ)."""
    q = question.lower()
    steps = []

    # KarmaÅŸÄ±klÄ±k & dinamik derinlik
    complexity = _compute_complexity_score(question, context)
    max_steps = _compute_dynamic_max_steps(complexity)

    # AdÄ±m 1: Her zaman â€” soruyu analiz et
    steps.append({
        "thought": "Soruyu analiz ediyorum: Ne soruluyor, hangi veriler gerekli?",
        "action": "analyze_question",
    })

    # AdÄ±m 2: Veri toplama â€” duruma gÃ¶re
    if re.search(r'(veri|dosya|tablo|rapor|excel|csv)', q):
        steps.append({"thought": "Veri analizi gerekiyor", "action": "analyze_data"})

    if re.search(r'(bilgi\s*taban|dokÃ¼man|kaynak)', q):
        steps.append({"thought": "Bilgi tabanÄ±nda aranmalÄ±", "action": "search_documents"})

    if context.get("needs_web"):
        steps.append({"thought": "GÃ¼ncel bilgi gerekiyor", "action": "web_search"})

    # AdÄ±m 3: Hesaplama varsa
    if re.search(r'(hesapla|oran|yÃ¼zde|toplam|ortalama|fire|oee|maliyet)', q):
        steps.append({"thought": "Hesaplama yapÄ±lmalÄ±", "action": "calculate"})

    # v2: KarÅŸÄ±laÅŸtÄ±rma adÄ±mÄ±
    if re.search(r'(karÅŸÄ±laÅŸtÄ±r|kÄ±yasla|versus|vs|fark|benchmark)', q):
        steps.append({"thought": "DeÄŸerlerin karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ± gerekiyor", "action": "compare"})

    # v2: Derinlemesine analiz
    if re.search(r'(detay|derinlemesine|kapsamlÄ±|ayrÄ±ntÄ±lÄ±|neden)', q):
        steps.append({"thought": "Derinlemesine analiz yapÄ±lacak", "action": "deep_dive"})

    # v2: Ã‡apraz referans
    if re.search(r'(kaynak|referans|doÄŸrula|cross|Ã§apraz)', q):
        steps.append({"thought": "Ã‡apraz referans kontrolÃ¼ gerekli", "action": "cross_reference"})

    # AdÄ±m: Yorumlama
    if re.search(r'(yorumla|deÄŸerlendir|analiz|kÄ±yasla)', q):
        steps.append({"thought": "SonuÃ§larÄ± yorumla ve karÅŸÄ±laÅŸtÄ±r", "action": "interpret"})

    # v2: DoÄŸrulama adÄ±mÄ± (karmaÅŸÄ±k sorularda)
    if complexity >= 4:
        steps.append({"thought": "SonuÃ§larÄ± doÄŸrula ve tutarlÄ±lÄ±k kontrolÃ¼ yap", "action": "verify_result"})

    # Son adÄ±m: Her zaman â€” sonuÃ§ ve tavsiye
    steps.append({
        "thought": "TÃ¼m bulgularÄ± birleÅŸtir, tavsiye oluÅŸtur",
        "action": "synthesize",
    })

    return steps[:max_steps]


def build_reasoning_prompt(question: str, chain: ReasoningChain, step_plan: dict) -> str:
    """Reasoning adÄ±mÄ± iÃ§in LLM prompt'u oluÅŸtur."""
    dynamic_max = _compute_dynamic_max_steps(
        _compute_complexity_score(question, {})
    )
    prompt = f"""## Ã‡ok AdÄ±mlÄ± Analiz â€” AdÄ±m {len(chain.steps) + 1}/{dynamic_max}

### Soru: {question}

### Åimdiye Kadar:
{chain.get_context_for_llm()}

### Bu AdÄ±mda:
**DÃ¼ÅŸÃ¼nce**: {step_plan.get('thought', '')}
**Eylem**: {step_plan.get('action', '')}

LÃ¼tfen bu adÄ±mÄ± tamamla ve bulgularÄ±nÄ± yaz. KÄ±sa ve somut ol."""

    return prompt


def format_reasoning_result(chain: ReasoningChain) -> str:
    """Reasoning zincirini kullanÄ±cÄ±ya gÃ¶sterilecek formata Ã§evir."""
    output = ""

    # AdÄ±mlarÄ± gÃ¶ster (kÄ±sa)
    if len(chain.steps) > 1:
        output += "### ğŸ§  Analiz SÃ¼reci\n"
        for step in chain.steps:
            if step.thought:
                bt_flag = " â†©ï¸" if step.metadata.backtracked else ""
                output += f"**{step.step_num}.** {step.thought}{bt_flag}\n"
            if step.observation:
                output += f"   â†’ {step.observation[:200]}\n"
        output += "\n---\n\n"

    # Son yanÄ±t
    output += chain.final_answer

    return output


def summarize_reasoning(chain: ReasoningChain) -> str:
    """Reasoning zincirinin kompakt Ã¶zeti â€” dashboard ve log iÃ§in."""
    total_steps = len(chain.steps)
    bt = chain.backtrack_count()
    dur = chain.total_duration_ms()
    avg_conf = chain.get_avg_step_confidence()
    tools = ", ".join(chain.tools_used) if chain.tools_used else "yok"
    lines = [
        f"ğŸ“Š Reasoning Ã–zeti",
        f"  Soru     : {chain.question[:80]}",
        f"  AdÄ±m     : {total_steps}",
        f"  SÃ¼re     : {dur:.0f} ms",
        f"  Ort.GÃ¼ven: {avg_conf:.2f}",
        f"  Backtrack: {bt}",
        f"  AraÃ§lar  : {tools}",
        f"  SonuÃ§    : {chain.final_answer[:120] if chain.final_answer else '-'}",
    ]
    return "\n".join(lines)


# ---------------------------------------------------------------------------
# Backtracking yardÄ±mcÄ±larÄ±
# ---------------------------------------------------------------------------

def _is_step_failed(step: ReasoningStep) -> bool:
    """AdÄ±m baÅŸarÄ±sÄ±z mÄ±? (boÅŸ veya hata gÃ¶zlemi)"""
    obs = step.observation.strip() if step.observation else ""
    if not obs:
        return True
    fail_patterns = ["hata", "error", "bulunamadÄ±", "sonuÃ§ vermedi", "baÅŸarÄ±sÄ±z", "timeout"]
    return any(p in obs.lower() for p in fail_patterns)


def _get_fallback_action(action: str) -> Optional[str]:
    """BaÅŸarÄ±sÄ±z eylem iÃ§in alternatif eylem dÃ¶ndÃ¼r."""
    return _FALLBACK_ACTIONS.get(action)


# ---------------------------------------------------------------------------
# GÃ¼ven hesaplama
# ---------------------------------------------------------------------------

def _estimate_step_confidence(step: ReasoningStep) -> float:
    """Bir adÄ±mÄ±n gÃ¼ven skorunu tahmin et (0-1)."""
    if not step.observation:
        return 0.1
    obs = step.observation
    length_score = min(1.0, len(obs) / 300)  # Daha uzun gÃ¶zlem â†’ daha fazla bilgi
    # SayÄ±sal veri iÃ§eriyorsa gÃ¼ven artar
    has_numbers = bool(re.search(r'\d+[\.,]\d+|\d{2,}', obs))
    numeric_bonus = 0.1 if has_numbers else 0.0
    # Hata/olumsuz ifade varsa gÃ¼ven dÃ¼ÅŸer
    negative = bool(re.search(r'(hata|error|bulunamadÄ±|yok|baÅŸarÄ±sÄ±z)', obs.lower()))
    negative_penalty = 0.2 if negative else 0.0
    # Kaynak referansÄ± varsa gÃ¼ven artar
    has_source = bool(re.search(r'(kaynak|dokÃ¼man|tablo|rapor)', obs.lower()))
    source_bonus = 0.1 if has_source else 0.0

    conf = 0.3 + length_score * 0.4 + numeric_bonus + source_bonus - negative_penalty
    return round(max(0.05, min(1.0, conf)), 3)


def _should_stop_early(chain: ReasoningChain) -> bool:
    """GÃ¼ven yeterince yÃ¼ksekse erken durma kararÄ±."""
    if len(chain.steps) < 2:
        return False
    avg = chain.get_avg_step_confidence()
    # Son adÄ±mÄ±n gÃ¼veni Ã§ok yÃ¼ksekse ve ortalama da iyiyse dur
    last_conf = chain.steps[-1].metadata.confidence if chain.steps else 0
    return avg > 0.85 and last_conf > 0.9


def _should_add_extra_steps(chain: ReasoningChain) -> bool:
    """GÃ¼ven dÃ¼ÅŸÃ¼kse ek adÄ±m eklensin mi?"""
    if len(chain.steps) < 2:
        return False
    avg = chain.get_avg_step_confidence()
    return avg < 0.5


# ---------------------------------------------------------------------------
# Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu
# ---------------------------------------------------------------------------

async def execute_reasoning_chain(
    question: str,
    context: dict,
    llm_generate,
    tool_execute=None,
    rag_search=None,
    web_search=None,
    session_history: list = None,
) -> dict:
    """
    Tam reasoning zincirini Ã§alÄ±ÅŸtÄ±r.

    Args:
        question: KullanÄ±cÄ± sorusu
        context: Router context
        llm_generate: LLM generate fonksiyonu (async)
        tool_execute: Tool registry execute fonksiyonu (async, optional)
        rag_search: RAG arama fonksiyonu (optional)
        web_search: Web arama fonksiyonu (optional)
        session_history: Ã–nceki konuÅŸma geÃ§miÅŸi (conversation-aware reasoning)

    Returns:
        {"answer": str, "reasoning": ReasoningChain, "tools_used": list}
    """
    chain_start = time.time()

    # Conversation-aware: Ã–nceki soru-cevaplarÄ± reasoning baÄŸlamÄ±na ekle
    conv_context = ""
    if session_history:
        recent = session_history[-3:]  # Son 3 mesaj
        for msg in recent:
            role = msg.get("role", "user")
            content = msg.get("content", "")[:300]
            if role == "user":
                conv_context += f"**KullanÄ±cÄ±**: {content}\n"
            elif role == "assistant":
                conv_context += f"**AI**: {content}\n"

    chain = ReasoningChain(question, conversation_context=conv_context)
    step_plans = plan_reasoning_steps(question, context)

    # Dinamik derinlik
    complexity = _compute_complexity_score(question, context)
    dynamic_max = _compute_dynamic_max_steps(complexity)

    logger.info("reasoning_chain_started",
                question=question[:80],
                planned_steps=len(step_plans),
                dynamic_max=dynamic_max,
                complexity=complexity)

    # Step-to-step data chaining: Ã¶nceki adÄ±m Ã§Ä±ktÄ±larÄ± bir sonrakine aktarÄ±lÄ±r
    accumulated_context = ""

    for i, step_plan in enumerate(step_plans):
        # GÃ¼ven bazlÄ± erken durma
        if _should_stop_early(chain):
            logger.info("reasoning_early_stop", step=i, avg_conf=chain.get_avg_step_confidence())
            break

        step_start = time.time()
        step = ReasoningStep(step_num=i + 1, thought=step_plan.get("thought", ""))
        action = step_plan.get("action", "")
        step.action = action

        try:
            await _execute_step_action(
                step, action, question, chain, accumulated_context,
                llm_generate, tool_execute, rag_search, web_search,
            )
        except Exception as e:
            step.observation = f"Hata: {str(e)}"
            logger.warning("reasoning_step_error", step=i + 1, error=str(e))

        # AdÄ±m sÃ¼resini kaydet
        step.metadata.duration_ms = round((time.time() - step_start) * 1000, 1)

        # GÃ¼ven skoru hesapla
        step.metadata.confidence = _estimate_step_confidence(step)

        # Backtracking: adÄ±m baÅŸarÄ±sÄ±zsa alternatif dene
        if _is_step_failed(step):
            fallback = _get_fallback_action(action)
            if fallback:
                logger.info("reasoning_backtrack", original=action, fallback=fallback)
                step.metadata.backtracked = True
                step.metadata.alternative_action = fallback
                bt_start = time.time()
                try:
                    await _execute_step_action(
                        step, fallback, question, chain, accumulated_context,
                        llm_generate, tool_execute, rag_search, web_search,
                    )
                except Exception as e:
                    step.observation = f"Backtrack hatasÄ± ({fallback}): {str(e)}"
                step.metadata.duration_ms += round((time.time() - bt_start) * 1000, 1)
                step.metadata.confidence = _estimate_step_confidence(step)

        # Step-to-step data chaining â€” observation'Ä± bir sonraki adÄ±ma aktar
        if step.observation and not step.observation.startswith(f"AdÄ±m tamamlandÄ±"):
            accumulated_context += f"\n### AdÄ±m {step.step_num} ({action}):\n{step.observation[:500]}\n"

        chain.add_step(step)

    # GÃ¼ven dÃ¼ÅŸÃ¼kse ve plan limiti dolmadÄ±ysa ek adÄ±m ekle
    if _should_add_extra_steps(chain) and len(chain.steps) < dynamic_max:
        extra_step = ReasoningStep(
            step_num=len(chain.steps) + 1,
            thought="GÃ¼ven dÃ¼ÅŸÃ¼k â€” ek doÄŸrulama ve sentez yapÄ±lÄ±yor",
        )
        extra_step.action = "verify_result"
        es_start = time.time()
        try:
            await _execute_step_action(
                extra_step, "verify_result", question, chain, accumulated_context,
                llm_generate, tool_execute, rag_search, web_search,
            )
        except Exception as e:
            extra_step.observation = f"Ek doÄŸrulama hatasÄ±: {str(e)}"
        extra_step.metadata.duration_ms = round((time.time() - es_start) * 1000, 1)
        extra_step.metadata.confidence = _estimate_step_confidence(extra_step)
        chain.add_step(extra_step)

    # Confidence hesapla
    base_conf = 0.6
    tool_bonus = len(chain.tools_used) * 0.05
    step_bonus = len(chain.steps) * 0.05
    avg_step_conf = chain.get_avg_step_confidence()
    chain.confidence = min(0.95, base_conf + tool_bonus + step_bonus + avg_step_conf * 0.1)

    total_ms = round((time.time() - chain_start) * 1000, 1)

    logger.info("reasoning_chain_completed",
                steps=len(chain.steps),
                tools_used=chain.tools_used,
                confidence=chain.confidence,
                duration_ms=total_ms,
                backtracks=chain.backtrack_count())

    # Singleton istatistik gÃ¼ncelle
    ReasoningEngine.instance()._record_chain(chain, total_ms)

    return {
        "reasoning_chain": chain,
        "reasoning_context": chain.get_context_for_llm(),
        "tools_used": chain.tools_used,
        "confidence": chain.confidence,
    }


# ---------------------------------------------------------------------------
# AdÄ±m eylem Ã§alÄ±ÅŸtÄ±rÄ±cÄ± (tekrarlanan kodu merkezileÅŸtir)
# ---------------------------------------------------------------------------

async def _execute_step_action(
    step: ReasoningStep,
    action: str,
    question: str,
    chain: ReasoningChain,
    accumulated_context: str,
    llm_generate,
    tool_execute,
    rag_search,
    web_search,
):
    """Tek bir reasoning adÄ±mÄ±nÄ±n eylemini Ã§alÄ±ÅŸtÄ±r."""
    if action == "search_documents" and rag_search:
        docs = rag_search(question, n_results=5)
        if docs:
            step.observation = f"{len(docs)} ilgili dokÃ¼man bulundu"
            for doc in docs[:3]:
                step.observation += f"\n- {doc.get('source', '?')}: {doc.get('content', '')[:200]}"
        else:
            step.observation = "Bilgi tabanÄ±nda ilgili dokÃ¼man bulunamadÄ±"

    elif action == "web_search" and web_search:
        web_result, _ = await web_search(question)
        step.observation = web_result[:500] if web_result else "Web aramasÄ± sonuÃ§ vermedi"

    elif action == "calculate" and tool_execute:
        from app.core.tool_registry import detect_tool_calls
        calls = detect_tool_calls(question)
        for call in calls:
            result = await tool_execute(call["tool"], call["params"])
            if result.get("success"):
                step.observation += f"\n{call['tool']}: {result['result']}"
                chain.tools_used.append(call["tool"])
        if not step.observation:
            step.observation = "Otomatik hesaplama yapÄ±lamadÄ±, LLM ile devam ediliyor"

    elif action == "analyze_data":
        step.observation = "Veri analizi modu aktif â€” yÃ¼klenen veriler incelenecek"

    elif action == "analyze_question" and llm_generate:
        analysis_prompt = (
            f"AÅŸaÄŸÄ±daki soruyu analiz et. Ne soruluyor, hangi veriler gerekli, "
            f"hangi metrikler hesaplanmalÄ±? KÄ±sa ve yapÄ±sal cevap ver.\n\n"
            f"Soru: {question}"
        )
        if accumulated_context:
            analysis_prompt += f"\n\nÃ–nceki adÄ±m bulgularÄ±:\n{accumulated_context}"
        try:
            result = await llm_generate(
                prompt=analysis_prompt,
                system_prompt="KÄ±sa ve somut analiz yap. Madde madde yaz.",
                temperature=0.2,
                max_tokens=400,
            )
            step.observation = result if result else "Soru analizi tamamlandÄ±"
            step.metadata.tokens_used = len(result.split()) * 2 if result else 0
        except Exception:
            step.observation = "Soru analiz edildi â€” devam ediliyor"

    elif action == "interpret" and llm_generate:
        interpret_prompt = (
            f"AÅŸaÄŸÄ±daki analiz sonuÃ§larÄ±nÄ± TÃ¼rkÃ§e olarak yorumla. "
            f"SayÄ±sal veriler varsa trend ve anomali belirt. "
            f"Somut ve kÄ±sa yaz.\n\n"
            f"Soru: {question}\n\n"
            f"Åimdiye kadar toplanan bulgular:\n{accumulated_context}\n\n"
            f"Reasoning sÃ¼reci:\n{chain.get_context_for_llm()}"
        )
        try:
            result = await llm_generate(
                prompt=interpret_prompt,
                system_prompt="Veri yorumlama uzmanÄ±sÄ±n. Benchmarkla karÅŸÄ±laÅŸtÄ±r, risk belirt.",
                temperature=0.3,
                max_tokens=500,
            )
            step.observation = result if result else "Yorumlama tamamlandÄ±"
            step.metadata.tokens_used = len(result.split()) * 2 if result else 0
        except Exception as e:
            step.observation = f"Yorumlama hatasÄ±: {str(e)}"

    elif action == "synthesize" and llm_generate:
        synthesize_prompt = (
            f"AÅŸaÄŸÄ±daki Ã§ok adÄ±mlÄ± analizin tÃ¼m bulgularÄ±nÄ± sentezle. "
            f"Nihai sonuÃ§ ve tavsiyeni yaz. Somut aksiyon Ã¶ner.\n\n"
            f"Orijinal soru: {question}\n\n"
            f"TÃ¼m adÄ±m bulgularÄ±:\n{accumulated_context}\n\n"
            f"Tam reasoning zinciri:\n{chain.get_context_for_llm()}"
        )
        try:
            result = await llm_generate(
                prompt=synthesize_prompt,
                system_prompt="Sentez uzmanÄ±sÄ±n. TÃ¼m bulgularÄ± birleÅŸtir, Ã§eliÅŸkileri Ã§Ã¶z, net tavsiye ver.",
                temperature=0.3,
                max_tokens=600,
            )
            step.observation = result if result else "Sentez tamamlandÄ±"
            chain.final_answer = result or chain.final_answer
            step.metadata.tokens_used = len(result.split()) * 2 if result else 0
        except Exception as e:
            step.observation = f"Sentez hatasÄ±: {str(e)}"

    elif action == "verify_result" and llm_generate:
        verify_prompt = (
            f"AÅŸaÄŸÄ±daki analiz bulgularÄ±nÄ± doÄŸrula. TutarsÄ±zlÄ±k var mÄ±? "
            f"SayÄ±sal veriler mantÄ±klÄ± mÄ±? SonuÃ§lar gÃ¼venilir mi?\n\n"
            f"Soru: {question}\n\nBulgular:\n{accumulated_context}"
        )
        try:
            result = await llm_generate(
                prompt=verify_prompt,
                system_prompt="Kalite kontrol uzmanÄ±sÄ±n. Veri tutarlÄ±lÄ±ÄŸÄ±nÄ± kontrol et.",
                temperature=0.2,
                max_tokens=400,
            )
            step.observation = result if result else "DoÄŸrulama tamamlandÄ±"
            step.metadata.tokens_used = len(result.split()) * 2 if result else 0
        except Exception as e:
            step.observation = f"DoÄŸrulama hatasÄ±: {str(e)}"

    elif action == "compare" and llm_generate:
        compare_prompt = (
            f"AÅŸaÄŸÄ±daki verileri karÅŸÄ±laÅŸtÄ±r. Temel farklarÄ±, avantaj/dezavantajlarÄ± "
            f"ve Ã¶nemli deÄŸiÅŸimleri vurgula.\n\n"
            f"Soru: {question}\n\nMevcut veriler:\n{accumulated_context}"
        )
        try:
            result = await llm_generate(
                prompt=compare_prompt,
                system_prompt="KarÅŸÄ±laÅŸtÄ±rma uzmanÄ±sÄ±n. Tablo formatÄ±nda, net farklarÄ± gÃ¶ster.",
                temperature=0.3,
                max_tokens=500,
            )
            step.observation = result if result else "KarÅŸÄ±laÅŸtÄ±rma tamamlandÄ±"
            step.metadata.tokens_used = len(result.split()) * 2 if result else 0
        except Exception as e:
            step.observation = f"KarÅŸÄ±laÅŸtÄ±rma hatasÄ±: {str(e)}"

    elif action == "deep_dive" and llm_generate:
        deep_prompt = (
            f"AÅŸaÄŸÄ±daki konuyu derinlemesine analiz et. KÃ¶k nedenleri bul, "
            f"gizli kalÄ±plarÄ± tespit et, detaylÄ± iÃ§gÃ¶rÃ¼ sun.\n\n"
            f"Soru: {question}\n\nMevcut bulgular:\n{accumulated_context}"
        )
        try:
            result = await llm_generate(
                prompt=deep_prompt,
                system_prompt="Derin analiz uzmanÄ±sÄ±n. YÃ¼zeyin altÄ±na in, kÃ¶k nedenleri bul.",
                temperature=0.4,
                max_tokens=600,
            )
            step.observation = result if result else "Derin analiz tamamlandÄ±"
            step.metadata.tokens_used = len(result.split()) * 2 if result else 0
        except Exception as e:
            step.observation = f"Derin analiz hatasÄ±: {str(e)}"

    elif action == "cross_reference" and llm_generate:
        xref_prompt = (
            f"FarklÄ± kaynaklardan gelen bilgileri Ã§apraz kontrol et. "
            f"TutarlÄ±lÄ±k ve Ã§eliÅŸkileri belirle.\n\n"
            f"Soru: {question}\n\nKaynaklar:\n{accumulated_context}"
        )
        try:
            result = await llm_generate(
                prompt=xref_prompt,
                system_prompt="Ã‡apraz referans uzmanÄ±sÄ±n. FarklÄ± kaynaklarÄ± karÅŸÄ±laÅŸtÄ±r, tutarsÄ±zlÄ±klarÄ± bul.",
                temperature=0.2,
                max_tokens=400,
            )
            step.observation = result if result else "Ã‡apraz kontrol tamamlandÄ±"
            step.metadata.tokens_used = len(result.split()) * 2 if result else 0
        except Exception as e:
            step.observation = f"Ã‡apraz kontrol hatasÄ±: {str(e)}"

    else:
        step.observation = f"AdÄ±m tamamlandÄ±: {action}"


# ---------------------------------------------------------------------------
# ReasoningEngine â€” Singleton
# ---------------------------------------------------------------------------

class ReasoningEngine:
    """Merkezi reasoning yÃ¶neticisi â€” singleton pattern.

    TÃ¼m reasoning iÅŸlemlerini yÃ¶netir, istatistik toplar ve dashboard sunar.
    """
    _instance: Optional["ReasoningEngine"] = None

    def __init__(self):
        """DoÄŸrudan kullanmayÄ±n, ReasoningEngine.instance() ile eriÅŸin."""
        self._stats: dict = {
            "total_chains": 0,
            "total_steps": 0,
            "avg_steps": 0.0,
            "avg_confidence": 0.0,
            "avg_duration_ms": 0.0,
            "tool_calls": 0,
            "backtrack_count": 0,
            "early_stops": 0,
            "extra_steps_added": 0,
            "action_counts": {},        # eylem tipi â†’ sayÄ±
            "confidence_history": [],    # son 50 zincir gÃ¼veni
            "duration_history": [],      # son 50 zincir sÃ¼resi
        }
        self._history: list[dict] = []  # son N zincir Ã¶zeti

    @classmethod
    def instance(cls) -> "ReasoningEngine":
        """Singleton eriÅŸim noktasÄ±."""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    @classmethod
    def reset(cls):
        """Test amaÃ§lÄ± â€” singleton'Ä± sÄ±fÄ±rla."""
        cls._instance = None

    # --- KayÄ±t & istatistik ------------------------------------------------

    def _record_chain(self, chain: ReasoningChain, total_ms: float):
        """Tamamlanan zinciri istatistiklere kaydet."""
        s = self._stats
        s["total_chains"] += 1
        s["total_steps"] += len(chain.steps)
        s["tool_calls"] += len(chain.tools_used)
        s["backtrack_count"] += chain.backtrack_count()

        # Ã‡alÄ±ÅŸan ortalama
        n = s["total_chains"]
        s["avg_steps"] = round(((s["avg_steps"] * (n - 1)) + len(chain.steps)) / n, 2)
        s["avg_confidence"] = round(((s["avg_confidence"] * (n - 1)) + chain.confidence) / n, 3)
        s["avg_duration_ms"] = round(((s["avg_duration_ms"] * (n - 1)) + total_ms) / n, 1)

        # Eylem daÄŸÄ±lÄ±mÄ±
        for step in chain.steps:
            act = step.action
            s["action_counts"][act] = s["action_counts"].get(act, 0) + 1

        # Son N geÃ§miÅŸ (max 50)
        s["confidence_history"].append(chain.confidence)
        s["duration_history"].append(total_ms)
        if len(s["confidence_history"]) > 50:
            s["confidence_history"] = s["confidence_history"][-50:]
            s["duration_history"] = s["duration_history"][-50:]

        # Ã–zet geÃ§miÅŸi (max 20)
        self._history.append({
            "question": chain.question[:80],
            "steps": len(chain.steps),
            "confidence": chain.confidence,
            "tools": chain.tools_used[:],
            "backtracks": chain.backtrack_count(),
            "duration_ms": total_ms,
        })
        if len(self._history) > 20:
            self._history = self._history[-20:]

    # --- Dashboard ---------------------------------------------------------

    def get_dashboard(self) -> dict:
        """Reasoning istatistik dashboard'u dÃ¶ndÃ¼r."""
        s = self._stats
        return {
            "total_chains": s["total_chains"],
            "total_steps": s["total_steps"],
            "avg_steps_per_chain": s["avg_steps"],
            "avg_confidence": s["avg_confidence"],
            "avg_duration_ms": s["avg_duration_ms"],
            "tool_calls": s["tool_calls"],
            "backtrack_count": s["backtrack_count"],
            "action_distribution": dict(s["action_counts"]),
            "recent_confidence": s["confidence_history"][-10:],
            "recent_duration_ms": s["duration_history"][-10:],
            "recent_chains": self._history[-5:],
        }

    def get_stats(self) -> dict:
        """Ham istatistik sÃ¶zlÃ¼ÄŸÃ¼."""
        return dict(self._stats)

    # --- YÃ¼ksek seviye API -------------------------------------------------

    async def run(
        self,
        question: str,
        context: dict,
        llm_generate,
        **kwargs,
    ) -> dict:
        """execute_reasoning_chain'i sarmalayan yÃ¼ksek seviye API."""
        return await execute_reasoning_chain(
            question=question,
            context=context,
            llm_generate=llm_generate,
            **kwargs,
        )

    def should_reason(self, question: str, context: dict) -> bool:
        """needs_multi_step'i sarmalayan yÃ¼ksek seviye API."""
        return needs_multi_step(question, context)

    def get_complexity(self, question: str, context: dict = None) -> dict:
        """Soru karmaÅŸÄ±klÄ±k analizi dÃ¶ndÃ¼r."""
        ctx = context or {}
        score = _compute_complexity_score(question, ctx)
        return {
            "score": score,
            "max_steps": _compute_dynamic_max_steps(score),
            "needs_multi_step": needs_multi_step(question, ctx),
            "level": "basit" if score <= 2 else "orta" if score <= 5 else "karmaÅŸÄ±k",
        }
