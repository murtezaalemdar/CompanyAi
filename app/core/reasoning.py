"""Ã‡ok AdÄ±mlÄ± Reasoning Engine â€” ReAct Pattern

KarmaÅŸÄ±k sorularÄ± adÄ±m adÄ±m Ã§Ã¶zer:
1. Thought (DÃ¼ÅŸÃ¼n) â€” Soruyu analiz et
2. Action (Eylem) â€” AraÃ§ kullan veya bilgi topla
3. Observation (GÃ¶zlem) â€” Sonucu deÄŸerlendir
4. ... (tekrarla)
5. Final Answer â€” Son yanÄ±t

Max 5 adÄ±m ile dÃ¶ngÃ¼sel reasoning.
"""

import re
import structlog
from typing import Optional

logger = structlog.get_logger()

MAX_REASONING_STEPS = 5


class ReasoningStep:
    """Tek bir reasoning adÄ±mÄ±."""
    def __init__(self, step_num: int, thought: str = "", action: str = "",
                 action_input: dict = None, observation: str = ""):
        self.step_num = step_num
        self.thought = thought
        self.action = action
        self.action_input = action_input or {}
        self.observation = observation
    
    def to_dict(self) -> dict:
        return {
            "step": self.step_num,
            "thought": self.thought,
            "action": self.action,
            "action_input": self.action_input,
            "observation": self.observation,
        }


class ReasoningChain:
    """TÃ¼m reasoning zinciri."""
    def __init__(self, question: str):
        self.question = question
        self.steps: list[ReasoningStep] = []
        self.final_answer: str = ""
        self.confidence: float = 0.0
        self.tools_used: list[str] = []
    
    def add_step(self, step: ReasoningStep):
        self.steps.append(step)
    
    def to_dict(self) -> dict:
        return {
            "question": self.question,
            "steps": [s.to_dict() for s in self.steps],
            "final_answer": self.final_answer,
            "confidence": self.confidence,
            "tools_used": self.tools_used,
            "total_steps": len(self.steps),
        }
    
    def get_context_for_llm(self) -> str:
        """LLM'e gÃ¶nderilecek reasoning context."""
        text = f"## Reasoning Zinciri\nSoru: {self.question}\n\n"
        for step in self.steps:
            text += f"### AdÄ±m {step.step_num}\n"
            if step.thought:
                text += f"**DÃ¼ÅŸÃ¼nce**: {step.thought}\n"
            if step.action:
                text += f"**Eylem**: {step.action}({step.action_input})\n"
            if step.observation:
                text += f"**GÃ¶zlem**: {step.observation}\n"
            text += "\n"
        return text


def needs_multi_step(question: str, context: dict) -> bool:
    """Bu soru Ã§ok adÄ±mlÄ± reasoning gerektiriyor mu?"""
    q = question.lower()
    
    # KarmaÅŸÄ±klÄ±k gÃ¶stergeleri
    complexity_signals = [
        # Birden fazla hesaplama/karÅŸÄ±laÅŸtÄ±rma
        bool(re.search(r'(karÅŸÄ±laÅŸtÄ±r|kÄ±yasla|versus|vs|fark)', q)),
        # Ã‡ok parÃ§alÄ± soru
        bool(re.search(r'(ve|ayrÄ±ca|bunun\s+yanÄ±nda|hem.*hem)', q)),
        # KoÅŸullu analiz
        bool(re.search(r'(eÄŸer|durumunda|olursa|varsayalÄ±m)', q)),
        # Ã‡ok adÄ±mlÄ± hesaplama
        bool(re.search(r'(hesapla.*ve.*yorumla|analiz.*et.*Ã¶ner)', q)),
        # Neden-sonuÃ§
        bool(re.search(r'(neden.*ve.*ne\s*yapÄ±lmalÄ±|sebep.*Ã§Ã¶zÃ¼m)', q)),
        # Tahmin + analiz
        bool(re.search(r'(tahmin|Ã¶ngÃ¶r|projeksiyon|gelecek)', q)),
        # Risk + aksiyon
        bool(re.search(r'risk.*aksiyon|tehlike.*Ã¶nlem', q)),
        # Soru uzunluÄŸu (30+ kelime genelde karmaÅŸÄ±k)
        len(q.split()) > 30,
    ]
    
    complexity_score = sum(complexity_signals)
    
    # Intent iÅŸ/analiz ise ve 2+ sinyal varsa multi-step
    intent = context.get("intent", "sohbet")
    if intent in ("iÅŸ", "bilgi") and complexity_score >= 2:
        return True
    if complexity_score >= 3:
        return True
    
    return False


def plan_reasoning_steps(question: str, context: dict) -> list[dict]:
    """Soru iÃ§in reasoning planÄ± oluÅŸtur."""
    q = question.lower()
    steps = []
    
    # AdÄ±m 1: Her zaman â€” soruyu analiz et
    steps.append({
        "thought": "Soruyu analiz ediyorum: Ne soruluyor, hangi veriler gerekli?",
        "action": "analyze_question",
    })
    
    # AdÄ±m 2: Veri toplama â€” duruma gÃ¶re
    if re.search(r'(veri|dosya|tablo|rapor|excel)', q):
        steps.append({"thought": "Veri analizi gerekiyor", "action": "analyze_data"})
    
    if re.search(r'(bilgi\s*taban|dokÃ¼man|kaynak)', q):
        steps.append({"thought": "Bilgi tabanÄ±nda aranmalÄ±", "action": "search_documents"})
    
    if context.get("needs_web"):
        steps.append({"thought": "GÃ¼ncel bilgi gerekiyor", "action": "web_search"})
    
    # AdÄ±m 3: Hesaplama varsa
    if re.search(r'(hesapla|oran|yÃ¼zde|toplam|ortalama|fire|oee|maliyet)', q):
        steps.append({"thought": "Hesaplama yapÄ±lmalÄ±", "action": "calculate"})
    
    # AdÄ±m 4: Yorumlama
    if re.search(r'(yorumla|deÄŸerlendir|analiz|kÄ±yasla)', q):
        steps.append({"thought": "SonuÃ§larÄ± yorumla ve karÅŸÄ±laÅŸtÄ±r", "action": "interpret"})
    
    # AdÄ±m 5: Her zaman â€” sonuÃ§ ve tavsiye
    steps.append({
        "thought": "TÃ¼m bulgularÄ± birleÅŸtir, tavsiye oluÅŸtur",
        "action": "synthesize",
    })
    
    return steps[:MAX_REASONING_STEPS]


def build_reasoning_prompt(question: str, chain: ReasoningChain, step_plan: dict) -> str:
    """Reasoning adÄ±mÄ± iÃ§in LLM prompt'u oluÅŸtur."""
    prompt = f"""## Ã‡ok AdÄ±mlÄ± Analiz â€” AdÄ±m {len(chain.steps) + 1}/{MAX_REASONING_STEPS}

### Soru: {question}

### Åžimdiye Kadar:
{chain.get_context_for_llm()}

### Bu AdÄ±mda:
**DÃ¼ÅŸÃ¼nce**: {step_plan.get('thought', '')}
**Eylem**: {step_plan.get('action', '')}

LÃ¼tfen bu adÄ±mÄ± tamamla ve bulgularÄ±nÄ± yaz. KÄ±sa ve somut ol."""
    
    return prompt


def format_reasoning_result(chain: ReasoningChain) -> str:
    """Reasoning zincirini kullanÄ±cÄ±ya gÃ¶sterilecek formata Ã§evir."""
    output = ""
    
    # AdÄ±mlarÄ± gÃ¶ster (kÄ±sa)
    if len(chain.steps) > 1:
        output += "### ðŸ§  Analiz SÃ¼reci\n"
        for step in chain.steps:
            if step.thought:
                output += f"**{step.step_num}.** {step.thought}\n"
            if step.observation:
                output += f"   â†’ {step.observation[:200]}\n"
        output += "\n---\n\n"
    
    # Son yanÄ±t
    output += chain.final_answer
    
    return output


async def execute_reasoning_chain(
    question: str,
    context: dict,
    llm_generate,
    tool_execute=None,
    rag_search=None,
    web_search=None,
) -> dict:
    """
    Tam reasoning zincirini Ã§alÄ±ÅŸtÄ±r.
    
    Args:
        question: KullanÄ±cÄ± sorusu
        context: Router context
        llm_generate: LLM generate fonksiyonu (async)
        tool_execute: Tool registry execute fonksiyonu (async, optional)
        rag_search: RAG arama fonksiyonu (optional)
        web_search: Web arama fonksiyonu (optional)
    
    Returns:
        {"answer": str, "reasoning": ReasoningChain, "tools_used": list}
    """
    chain = ReasoningChain(question)
    step_plans = plan_reasoning_steps(question, context)
    
    logger.info("reasoning_chain_started", question=question[:80], planned_steps=len(step_plans))
    
    for i, step_plan in enumerate(step_plans):
        step = ReasoningStep(step_num=i + 1, thought=step_plan.get("thought", ""))
        action = step_plan.get("action", "")
        step.action = action
        
        try:
            # Aksiyona gÃ¶re iÅŸlem yap
            if action == "search_documents" and rag_search:
                docs = rag_search(question, n_results=5)
                if docs:
                    step.observation = f"{len(docs)} ilgili dokÃ¼man bulundu"
                    # DokÃ¼man iÃ§eriklerini context'e ekle
                    for doc in docs[:3]:
                        step.observation += f"\n- {doc.get('source', '?')}: {doc.get('content', '')[:200]}"
                else:
                    step.observation = "Bilgi tabanÄ±nda ilgili dokÃ¼man bulunamadÄ±"
            
            elif action == "web_search" and web_search:
                web_result, _ = await web_search(question)
                step.observation = web_result[:500] if web_result else "Web aramasÄ± sonuÃ§ vermedi"
            
            elif action == "calculate" and tool_execute:
                # Hesaplama gereken metrikleri algÄ±la
                from app.core.tool_registry import detect_tool_calls
                calls = detect_tool_calls(question)
                for call in calls:
                    result = await tool_execute(call["tool"], call["params"])
                    if result.get("success"):
                        step.observation += f"\n{call['tool']}: {result['result']}"
                        chain.tools_used.append(call["tool"])
                if not step.observation:
                    step.observation = "Otomatik hesaplama yapÄ±lamadÄ±, LLM ile devam ediliyor"
            
            elif action == "analyze_data":
                step.observation = "Veri analizi modu aktif â€” yÃ¼klenen veriler incelenecek"
            
            elif action in ("interpret", "synthesize", "analyze_question"):
                # Bu adÄ±mlar LLM tarafÄ±ndan yanÄ±tlanÄ±r
                reasoning_prompt = build_reasoning_prompt(question, chain, step_plan)
                step.observation = "LLM tarafÄ±ndan iÅŸlenecek"
            
            else:
                step.observation = f"AdÄ±m tamamlandÄ±: {action}"
        
        except Exception as e:
            step.observation = f"Hata: {str(e)}"
            logger.warning("reasoning_step_error", step=i+1, error=str(e))
        
        chain.add_step(step)
    
    # Confidence hesapla
    chain.confidence = min(0.95, 0.6 + (len(chain.tools_used) * 0.05) + (len(chain.steps) * 0.05))
    
    logger.info("reasoning_chain_completed", 
                steps=len(chain.steps), 
                tools_used=chain.tools_used,
                confidence=chain.confidence)
    
    return {
        "reasoning_chain": chain,
        "reasoning_context": chain.get_context_for_llm(),
        "tools_used": chain.tools_used,
        "confidence": chain.confidence,
    }
