"""Web Arama ModÃ¼lÃ¼ â€” SerpAPI (Google) + DuckDuckGo Fallback

Kurumsal AI asistanÄ±n bilmediÄŸi konularda internetten gÃ¼ncel bilgi
bulmasÄ±nÄ± saÄŸlar.

Ã–ncelik sÄ±rasÄ±:
1. SerpAPI â€” Google sonuÃ§larÄ± (SERPAPI_KEY varsa, Ã¼cretsiz 100/ay)
2. Google Custom Search API (GOOGLE_API_KEY + GOOGLE_CSE_ID varsa)
3. DuckDuckGo Instant Answer API (Ã¼cretsiz fallback)
4. DuckDuckGo HTML scraping (son Ã§are)
"""

import httpx
import structlog
import re
from typing import List, Dict, Optional, Tuple

from app.config import settings

logger = structlog.get_logger()

# SerpAPI (Google sonuÃ§larÄ± â€” Ã¼cretsiz 100 arama/ay)
SERPAPI_URL = "https://serpapi.com/search.json"

# Google Custom Search API (yedek)
GOOGLE_SEARCH_URL = "https://www.googleapis.com/customsearch/v1"

# DuckDuckGo (fallback)
DDG_API_URL = "https://api.duckduckgo.com/"
DDG_HTML_URL = "https://html.duckduckgo.com/html/"


def _serpapi_configured() -> bool:
    """SerpAPI anahtarÄ± yapÄ±landÄ±rÄ±lmÄ±ÅŸ mÄ±?"""
    return bool(settings.SERPAPI_KEY)


def _google_configured() -> bool:
    """Google API anahtarlarÄ± yapÄ±landÄ±rÄ±lmÄ±ÅŸ mÄ±?"""
    return bool(settings.GOOGLE_API_KEY and settings.GOOGLE_CSE_ID)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SerpAPI â€” Google Arama SonuÃ§larÄ±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _search_serpapi(query: str, max_results: int = 5) -> Tuple[List[Dict[str, str]], Optional[List[Dict]]]:
    """
    SerpAPI ile Google arama sonuÃ§larÄ±nÄ± Ã§eker.
    
    Ãœcretsiz plan: 250 arama/ay, kredi kartÄ± gerektirmez.
    DÃ¶kÃ¼man: https://serpapi.com/search-api
    
    Returns:
        (results, rich_data) â€” rich_data hava durumu, gÃ¶rseller gibi yapÄ±sal veri listesi iÃ§erir
    """
    results = []
    rich_data = []
    
    try:
        async with httpx.AsyncClient(timeout=15.0, trust_env=False) as client:
            response = await client.get(
                SERPAPI_URL,
                params={
                    "api_key": settings.SERPAPI_KEY,
                    "engine": "google",
                    "q": query,
                    "num": min(max_results, 10),
                    "hl": "tr",  # TÃ¼rkÃ§e arayÃ¼z
                    "gl": "tr",  # TÃ¼rkiye bÃ¶lgesi
                    "safe": "active",
                    "no_cache": "false",  # Cache kullan (kota tasarrufu)
                },
            )
            response.raise_for_status()
            data = response.json()
        
        # Organik sonuÃ§lar
        for item in data.get("organic_results", [])[:max_results]:
            results.append({
                "title": item.get("title", ""),
                "snippet": item.get("snippet", ""),
                "url": item.get("link", ""),
                "source": "Google (SerpAPI)",
            })
        
        # Answer box â€” yapÄ±sal veri Ã§Ä±karma
        answer_box = data.get("answer_box", {})
        if answer_box:
            # Hava durumu sonucu
            ab_type = answer_box.get("type", "")
            if ab_type == "weather_result" or "temperature" in answer_box:
                weather = _extract_weather_data(answer_box, data)
                rich_data.append(weather)
                logger.info("serpapi_weather_detected", location=weather.get("location", ""))
            
            # Metin answer box
            if answer_box.get("snippet") or answer_box.get("answer"):
                answer_text = answer_box.get("snippet") or answer_box.get("answer", "")
                results.insert(0, {
                    "title": answer_box.get("title", "Google YanÄ±t"),
                    "snippet": answer_text[:500],
                    "url": answer_box.get("link", ""),
                    "source": "Google Answer Box",
                })
        
        # Knowledge graph varsa ekle
        knowledge = data.get("knowledge_graph", {})
        if knowledge and knowledge.get("description"):
            results.append({
                "title": knowledge.get("title", ""),
                "snippet": knowledge.get("description", "")[:500],
                "url": knowledge.get("source", {}).get("link", ""),
                "source": "Google Knowledge Graph",
            })
        
        logger.info("serpapi_search_ok", query=query[:80], results=len(results),
                    has_rich_data=len(rich_data) > 0)
        
        # Inline gÃ¶rseller â€” Google gÃ¶rsel sonuÃ§larÄ± (normal aramada varsa)
        inline_images = data.get("inline_images", [])
        if inline_images:
            images_card = _extract_image_results(inline_images, query)
            if images_card:
                rich_data.append(images_card)
                logger.info("serpapi_images_detected", count=len(images_card.get("images", [])))
        
        # EÄŸer inline gÃ¶rseller yoksa ve sorgu gÃ¶rsele uygunsa, Google Images engine dene
        if not inline_images and _query_needs_images(query):
            try:
                images_card = await _search_serpapi_images(query)
                if images_card:
                    rich_data.append(images_card)
                    logger.info("serpapi_images_secondary", count=len(images_card.get("images", [])))
            except Exception as img_err:
                logger.warning("serpapi_images_fallback_error", error=str(img_err))
        
    except httpx.HTTPStatusError as e:
        status = e.response.status_code
        if status == 429:
            logger.warning("serpapi_quota_exceeded", query=query[:60])
        elif status == 401:
            logger.error("serpapi_key_invalid")
        else:
            logger.error("serpapi_http_error", status=status)
        
    except Exception as e:
        logger.error("serpapi_error", error=str(e))
    
    return results, rich_data if rich_data else None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GÃ¶rsel Arama YardÄ±mcÄ±larÄ±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# GÃ¶rsel arama tetikleyen kelimeler
_IMAGE_KEYWORDS = {
    "Ã¶rnek", "Ã¶rnekleri", "Ã¶rneÄŸi", "gÃ¶rseli", "gÃ¶rselleri", "gÃ¶rsel",
    "resim", "resimleri", "fotoÄŸraf", "fotoÄŸraflarÄ±", "model", "modelleri",
    "desen", "desenleri", "kalÄ±p", "kalÄ±plarÄ±", "Ã§izim", "Ã§izimleri",
    "tasarÄ±m", "tasarÄ±mlarÄ±", "numune", "numuneleri", "katalog", "kataloÄŸu",
    "renk", "renkleri", "baskÄ±", "baskÄ±larÄ±", "kumaÅŸ", "kumaÅŸlarÄ±",
    "nasÄ±l gÃ¶rÃ¼nÃ¼r", "gÃ¶ster", "nedir", "ÅŸekil", "ÅŸekilleri",
}


def _query_needs_images(query: str) -> bool:
    """Sorgunun gÃ¶rsel sonuÃ§lara ihtiyaÃ§ duyup duymadÄ±ÄŸÄ±nÄ± belirler."""
    query_lower = query.lower()
    # Hava durumu sorgularÄ±nda gÃ¶rsele gerek yok
    weather_words = {"hava", "sÄ±caklÄ±k", "derece", "yaÄŸmur", "kar", "rÃ¼zgar"}
    if any(w in query_lower for w in weather_words):
        return False
    # GÃ¶rsel tetikleyici kelimeler
    return any(kw in query_lower for kw in _IMAGE_KEYWORDS)


async def _search_serpapi_images(query: str, max_images: int = 12) -> Optional[Dict]:
    """SerpAPI Google Images engine ile gÃ¶rsel arama yapar.
    
    Bu fonksiyon ayrÄ± bir API Ã§aÄŸrÄ±sÄ± yapar (kota kullanÄ±r).
    Sadece sorgu gÃ¶rsele uygun olduÄŸunda Ã§aÄŸrÄ±lmalÄ±.
    """
    try:
        async with httpx.AsyncClient(timeout=15.0, trust_env=False) as client:
            response = await client.get(
                SERPAPI_URL,
                params={
                    "api_key": settings.SERPAPI_KEY,
                    "engine": "google_images",
                    "q": query,
                    "num": max_images,
                    "hl": "tr",
                    "gl": "tr",
                    "safe": "active",
                    "no_cache": "false",
                },
            )
            response.raise_for_status()
            data = response.json()
        
        images_results = data.get("images_results", [])
        if not images_results:
            return None
        
        images = []
        for img in images_results[:max_images]:
            src = img.get("original") or img.get("thumbnail", "")
            if not src:
                continue
            images.append({
                "src": src,
                "thumbnail": img.get("thumbnail", src),
                "title": img.get("title", ""),
                "source": img.get("source", ""),
                "link": img.get("link", ""),
            })
        
        if not images:
            return None
        
        logger.info("serpapi_google_images_ok", query=query[:60], count=len(images))
        return {
            "type": "images",
            "query": query,
            "images": images,
            "source": "Google GÃ¶rseller",
        }
    except Exception as e:
        logger.warning("serpapi_google_images_error", error=str(e))
        return None


def _extract_image_results(inline_images: list, query: str) -> Optional[Dict]:
    """SerpAPI inline_images verilerinden gÃ¶rsel kart verisi Ã§Ä±kar."""
    images = []
    for img in inline_images[:12]:  # Max 12 gÃ¶rsel
        src = img.get("original") or img.get("thumbnail", "")
        if not src:
            continue
        images.append({
            "src": src,
            "thumbnail": img.get("thumbnail", src),
            "title": img.get("title", ""),
            "source": img.get("source", ""),
            "link": img.get("link", ""),
        })
    
    if not images:
        return None
    
    return {
        "type": "images",
        "query": query,
        "images": images,
        "source": "Google GÃ¶rseller",
    }


def _extract_weather_data(answer_box: dict, full_data: dict) -> Dict:
    """SerpAPI answer_box'tan hava durumu verisini yapÄ±sal olarak Ã§Ä±kar."""
    # Hava durumu koÅŸulunu TÃ¼rkÃ§eye Ã§evir
    WEATHER_TR = {
        "Sunny": "GÃ¼neÅŸli", "Clear": "AÃ§Ä±k", "Partly cloudy": "ParÃ§alÄ± Bulutlu",
        "Cloudy": "Bulutlu", "Overcast": "KapalÄ±", "Rainy": "YaÄŸmurlu",
        "Light rain": "Hafif YaÄŸmur", "Heavy rain": "Åiddetli YaÄŸmur",
        "Thunderstorm": "GÃ¶k GÃ¼rÃ¼ltÃ¼lÃ¼ FÄ±rtÄ±na", "Snowy": "KarlÄ±",
        "Light snow": "Hafif Kar", "Heavy snow": "YoÄŸun Kar",
        "Foggy": "Sisli", "Windy": "RÃ¼zgarlÄ±", "Haze": "Puslu",
        "Mist": "Sisli", "Drizzle": "Ã‡isenti", "Sleet": "Sulu Kar",
        # TÃ¼rkÃ§e gelen deÄŸerler (SerpAPI hl=tr)
        "GÃ¼neÅŸli": "GÃ¼neÅŸli", "AÃ§Ä±k": "AÃ§Ä±k", "ParÃ§alÄ± bulutlu": "ParÃ§alÄ± Bulutlu",
        "Bulutlu": "Bulutlu", "KapalÄ±": "KapalÄ±", "YaÄŸmurlu": "YaÄŸmurlu",
        "Hafif yaÄŸmurlu": "Hafif YaÄŸmurlu", "Ã‡ok bulutlu": "Ã‡ok Bulutlu",
        "Åiddetli yaÄŸmurlu": "Åiddetli YaÄŸmur", "GÃ¶k gÃ¼rÃ¼ltÃ¼lÃ¼ fÄ±rtÄ±na": "GÃ¶k GÃ¼rÃ¼ltÃ¼lÃ¼ FÄ±rtÄ±na",
        "KarlÄ±": "KarlÄ±", "Hafif kar": "Hafif Kar", "YoÄŸun kar": "YoÄŸun Kar",
        "Sisli": "Sisli", "RÃ¼zgarlÄ±": "RÃ¼zgarlÄ±", "Puslu": "Puslu",
        "Ã‡isenti": "Ã‡isenti", "Sulu kar": "Sulu Kar",
    }
    
    # Hava durumu ikonunu belirle
    WEATHER_ICONS = {
        "Sunny": "â˜€ï¸", "Clear": "â˜€ï¸", "Partly cloudy": "â›…",
        "Cloudy": "â˜ï¸", "Overcast": "â˜ï¸", "Rainy": "ğŸŒ§ï¸",
        "Light rain": "ğŸŒ¦ï¸", "Heavy rain": "ğŸŒ§ï¸", "Thunderstorm": "â›ˆï¸",
        "Snowy": "ğŸŒ¨ï¸", "Light snow": "ğŸŒ¨ï¸", "Heavy snow": "â„ï¸",
        "Foggy": "ğŸŒ«ï¸", "Windy": "ğŸ’¨", "Haze": "ğŸŒ«ï¸",
        "Mist": "ğŸŒ«ï¸", "Drizzle": "ğŸŒ¦ï¸", "Sleet": "ğŸŒ¨ï¸",
        # TÃ¼rkÃ§e gelen deÄŸerler (SerpAPI hl=tr)
        "GÃ¼neÅŸli": "â˜€ï¸", "AÃ§Ä±k": "â˜€ï¸", "ParÃ§alÄ± bulutlu": "â›…",
        "Bulutlu": "â˜ï¸", "KapalÄ±": "â˜ï¸", "YaÄŸmurlu": "ğŸŒ§ï¸",
        "Hafif yaÄŸmurlu": "ğŸŒ¦ï¸", "Ã‡ok bulutlu": "â˜ï¸",
        "Åiddetli yaÄŸmurlu": "ğŸŒ§ï¸", "GÃ¶k gÃ¼rÃ¼ltÃ¼lÃ¼ fÄ±rtÄ±na": "â›ˆï¸",
        "KarlÄ±": "ğŸŒ¨ï¸", "Hafif kar": "ğŸŒ¨ï¸", "YoÄŸun kar": "â„ï¸",
        "Sisli": "ğŸŒ«ï¸", "RÃ¼zgarlÄ±": "ğŸ’¨", "Puslu": "ğŸŒ«ï¸",
        "Ã‡isenti": "ğŸŒ¦ï¸", "Sulu kar": "ğŸŒ¨ï¸",
    }
    
    weather_en = answer_box.get("weather", "")
    condition_tr = WEATHER_TR.get(weather_en, weather_en)
    icon = WEATHER_ICONS.get(weather_en, "ğŸŒ¡ï¸")
    
    # HaftalÄ±k tahmin
    forecast = []
    for day in answer_box.get("forecast", []):
        day_weather = day.get("weather", "")
        forecast.append({
            "day": day.get("day", ""),
            "high": day.get("temperature", {}).get("high", day.get("high", "")),
            "low": day.get("temperature", {}).get("low", day.get("low", "")),
            "condition": WEATHER_TR.get(day_weather, day_weather),
            "icon": WEATHER_ICONS.get(day_weather, "ğŸŒ¡ï¸"),
        })
    
    return {
        "type": "weather",
        "location": answer_box.get("location", ""),
        "temperature": answer_box.get("temperature", ""),
        "unit": answer_box.get("unit", "Celsius"),
        "condition": condition_tr,
        "condition_icon": icon,
        "precipitation": answer_box.get("precipitation", ""),
        "humidity": answer_box.get("humidity", ""),
        "wind": answer_box.get("wind", ""),
        "date": answer_box.get("date", ""),
        "forecast": forecast,
        "source": "Google Hava Durumu",
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Google Custom Search API (yedek)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _search_google(query: str, max_results: int = 5) -> List[Dict[str, str]]:
    """
    Google Custom Search JSON API ile arama yapar.
    Billing hesabÄ± gerektirir.
    """
    results = []
    
    try:
        async with httpx.AsyncClient(timeout=15.0, trust_env=False) as client:
            response = await client.get(
                GOOGLE_SEARCH_URL,
                params={
                    "key": settings.GOOGLE_API_KEY,
                    "cx": settings.GOOGLE_CSE_ID,
                    "q": query,
                    "num": min(max_results, 10),
                    "lr": "lang_tr",
                    "gl": "tr",
                    "safe": "active",
                },
            )
            response.raise_for_status()
            data = response.json()
        
        for item in data.get("items", []):
            results.append({
                "title": item.get("title", ""),
                "snippet": item.get("snippet", ""),
                "url": item.get("link", ""),
                "source": "Google",
            })
        
        logger.info("google_search_ok", query=query[:80], results=len(results))
        
    except httpx.HTTPStatusError as e:
        status = e.response.status_code
        if status == 429:
            logger.warning("google_quota_exceeded", query=query[:60])
        elif status == 403:
            logger.error("google_api_key_invalid")
        else:
            logger.error("google_search_http_error", status=status)
        
    except Exception as e:
        logger.error("google_search_error", error=str(e))
    
    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DuckDuckGo (Fallback)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _search_ddg_instant(query: str) -> List[Dict[str, str]]:
    """DuckDuckGo Instant Answer API ile arama"""
    results = []
    
    async with httpx.AsyncClient(timeout=10.0, trust_env=False) as client:
        response = await client.get(
            DDG_API_URL,
            params={
                "q": query,
                "format": "json",
                "no_html": 1,
                "skip_disambig": 1,
            },
            headers={"User-Agent": "CompanyAI/1.0"}
        )
        response.raise_for_status()
        data = response.json()
    
    # Abstract (Wikipedia vb.)
    if data.get("Abstract"):
        results.append({
            "title": data.get("Heading", "SonuÃ§"),
            "snippet": data["Abstract"][:500],
            "url": data.get("AbstractURL", ""),
            "source": data.get("AbstractSource", "Web"),
        })
    
    # Related Topics
    for topic in data.get("RelatedTopics", [])[:3]:
        if isinstance(topic, dict) and topic.get("Text"):
            results.append({
                "title": topic.get("Text", "")[:80],
                "snippet": topic.get("Text", "")[:300],
                "url": topic.get("FirstURL", ""),
                "source": "DuckDuckGo",
            })
    
    return results


async def _search_ddg_html(query: str, max_results: int = 3) -> List[Dict[str, str]]:
    """DuckDuckGo HTML aramasÄ± ile sonuÃ§ Ã§ekme"""
    results = []
    
    try:
        async with httpx.AsyncClient(timeout=10.0, trust_env=False, follow_redirects=True) as client:
            response = await client.post(
                DDG_HTML_URL,
                data={"q": query},
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                }
            )
            response.raise_for_status()
            html = response.text
        
        result_blocks = re.findall(
            r'class="result__a"[^>]*href="([^"]*)"[^>]*>(.*?)</a>.*?'
            r'class="result__snippet"[^>]*>(.*?)</(?:a|span|div)',
            html, re.DOTALL
        )
        
        for url, title, snippet in result_blocks[:max_results]:
            title_clean = re.sub(r'<[^>]+>', '', title).strip()
            snippet_clean = re.sub(r'<[^>]+>', '', snippet).strip()
            
            if title_clean and snippet_clean:
                results.append({
                    "title": title_clean[:100],
                    "snippet": snippet_clean[:300],
                    "url": url,
                    "source": "Web",
                })
    
    except Exception as e:
        logger.warning("ddg_html_parse_error", error=str(e))
    
    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ana Arama Fonksiyonu
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def search_web(query: str, max_results: int = 5) -> Tuple[List[Dict[str, str]], Optional[List[Dict]]]:
    """
    Web aramasÄ± yapar. Ã–ncelik sÄ±rasÄ±na gÃ¶re dener:
    
    1. SerpAPI (Google sonuÃ§larÄ± â€” Ã¼cretsiz 250/ay)
    2. Google Custom Search API (billing gerektirir)
    3. DuckDuckGo Instant API (Ã¼cretsiz fallback)
    4. DuckDuckGo HTML scraping (son Ã§are)
    
    Returns:
        (results, rich_data) â€” rich_data hava durumu/gÃ¶rseller gibi gÃ¶rsel kart verisi listesi
    """
    results = []
    rich_data = None
    search_engine = "none"
    
    # 1) SerpAPI â€” Google sonuÃ§larÄ± (en kaliteli)
    if _serpapi_configured():
        results, rich_data = await _search_serpapi(query, max_results)
        if results:
            search_engine = "serpapi"
    
    # 2) Google Custom Search (yedek â€” billing gerektiriyor)
    if not results and _google_configured():
        results = await _search_google(query, max_results)
        if results:
            search_engine = "google"
    
    # 3) DuckDuckGo Instant (fallback)
    if not results:
        try:
            results = await _search_ddg_instant(query)
            if results:
                search_engine = "duckduckgo_instant"
        except Exception as e:
            logger.warning("ddg_instant_failed", error=str(e))
    
    # 4) DuckDuckGo HTML (son Ã§are)
    if len(results) < 2:
        try:
            html_results = await _search_ddg_html(query, max_results - len(results))
            results.extend(html_results)
            if html_results and search_engine == "none":
                search_engine = "duckduckgo_html"
        except Exception as e:
            logger.warning("ddg_html_failed", error=str(e))
    
    logger.info("web_search_complete", 
                query=query[:80], 
                engine=search_engine,
                results_count=len(results),
                has_rich_data=rich_data is not None)
    
    return results[:max_results], rich_data


async def search_and_summarize(query: str) -> Tuple[Optional[str], Optional[List[Dict]]]:
    """
    Arama yap ve sonuÃ§larÄ± LLM prompt'una eklenecek formatta dÃ¶ndÃ¼r.
    
    Returns:
        (text_summary, rich_data) â€” rich_data gÃ¶rsel kart verisi listesi
    """
    results, rich_data = await search_web(query, max_results=5)
    
    if not results:
        return None, rich_data
    
    # Hangi motor kullanÄ±ldÄ±?
    engine = results[0].get("source", "Web")
    
    text = f"\n## ğŸŒ Ä°nternet AramasÄ± SonuÃ§larÄ± ({engine}):\n"
    for i, r in enumerate(results, 1):
        text += f"**{i}. {r['title']}**\n"
        text += f"{r['snippet']}\n"
        if r.get('url'):
            text += f"Kaynak: {r['url']}\n"
        text += "\n"
    
    text += "Bu bilgileri kullanarak yanÄ±t ver. KaynaÄŸÄ±n internetten geldiÄŸini belirt.\n"
    
    return text, rich_data
