"""Embedding Model Fine-Tune Pipeline â€” Tekstil SektÃ¶rÃ¼

Mevcut paraphrase-multilingual-mpnet-base-v2 modelini
tekstil terminolojisi ve ÅŸirket dokÃ¼manlarÄ±yla fine-tune eder.

KullanÄ±m:
    python -m app.scripts.finetune_embeddings --generate   # EÄŸitim verisini oluÅŸtur
    python -m app.scripts.finetune_embeddings --train       # Fine-tune baÅŸlat
    python -m app.scripts.finetune_embeddings --evaluate    # DeÄŸerlendir

Gereksinimler:
    pip install sentence-transformers datasets
"""

import os
import json
import argparse
import random
from pathlib import Path
from typing import List, Tuple

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. TEKSTÄ°L TERMÄ°NOLOJÄ° VERÄ° SETÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Anchor-Positive Ã§iftleri: Anlam olarak benzer olmasÄ± gereken cÃ¼mle Ã§iftleri
# Fine-tune sonrasÄ± bu Ã§iftlerin embedding'leri daha yakÄ±n olmalÄ±
TEXTILE_PAIRS = [
    # Fire / AtÄ±k
    ("Fire oranÄ± nedir?", "Ãœretim atÄ±k yÃ¼zdesi nedir?"),
    ("Fire oranÄ±mÄ±z %3.5", "AtÄ±k oranÄ±mÄ±z yÃ¼zde 3.5 seviyesinde"),
    ("KumaÅŸ firesi dÃ¼ÅŸÃ¼rme yollarÄ±", "Tekstil atÄ±k azaltma stratejileri"),
    ("Fire maliyeti nasÄ±l hesaplanÄ±r?", "Ãœretim kayÄ±p maliyeti formÃ¼lÃ¼"),
    ("Kesim kayÄ±plarÄ± Ã§ok yÃ¼ksek", "Kesim sÃ¼recinde fire oranÄ± fazla"),
    
    # OEE / Verimlilik
    ("OEE nasÄ±l hesaplanÄ±r?", "Overall Equipment Effectiveness formÃ¼lÃ¼"),
    ("OEE %72 iyi mi?", "Genel Ekipman VerimliliÄŸi yÃ¼zde 72 yeterli mi?"),
    ("Makine verimliliÄŸi dÃ¼ÅŸÃ¼k", "Ekipman etkinliÄŸi yetersiz"),
    ("Ãœretim hattÄ± performansÄ±", "Hat bazlÄ± Ã§Ä±ktÄ± verimliliÄŸi"),
    ("DuruÅŸ sÃ¼resi analizi", "ArÄ±za kaynaklÄ± Ã¼retim kaybÄ±"),
    
    # Kalite
    ("2. kalite oran artÄ±ÅŸÄ±", "B-grade Ã¼rÃ¼n oranÄ± yÃ¼kseldi"),
    ("Kalite kontrol sonuÃ§larÄ±", "KK test raporlarÄ±"),
    ("KumaÅŸ hata tipleri", "Tekstil kusur sÄ±nÄ±flandÄ±rmasÄ±"),
    ("Gramaj sapmasÄ±", "KumaÅŸ aÄŸÄ±rlÄ±k tolerans aÅŸÄ±mÄ±"),
    ("Ã‡ekmezlik testi", "Shrinkage test sonuÃ§larÄ±"),
    ("HaslÄ±k deÄŸerleri", "Renk haslÄ±k test sonuÃ§larÄ±"),
    ("Pilling testi", "Boncuklanma dayanÄ±m skoru"),
    
    # Ãœretim
    ("Ãœretim planÄ±", "AylÄ±k imalat programÄ±"),
    ("SipariÅŸ teslimat gecikmeleri", "MÃ¼ÅŸteri sipariÅŸi termin aÅŸÄ±mÄ±"),
    ("Lot takibi", "Parti izlenebilirlik"),
    ("Makinede duruÅŸ", "Ãœretim hattÄ± duraÄŸanlÄ±ÄŸÄ±"),
    ("Dokuma hÄ±zÄ± ayarÄ±", "Tezgah Ã§alÄ±ÅŸma hÄ±zÄ± optimizasyonu"),
    ("Boyahane kapasitesi", "Boya tesisi Ã¼retim yeteneÄŸi"),
    
    # Maliyet / Finans
    ("Hammadde maliyeti arttÄ±", "Ä°plik fiyatlarÄ± yÃ¼kseldi"),
    ("BrÃ¼t kÃ¢r marjÄ±", "BrÃ¼t kar marjÄ± oranÄ±"),
    ("Ä°ÅŸÃ§ilik maliyeti", "Personel giderleri"),
    ("Enerji giderleri analizi", "Elektrik ve doÄŸalgaz maliyet incelemesi"),
    ("YatÄ±rÄ±m geri dÃ¶nÃ¼ÅŸ sÃ¼resi", "ROI hesaplamasÄ±"),
    
    # Risk / Strateji
    ("Tedarik zinciri riski", "TedarikÃ§i kaynaklÄ± risk faktÃ¶rleri"),
    ("Pazar riski analizi", "Piyasa dalgalanma risk deÄŸerlendirmesi"),
    ("Rekabet analizi", "Rakip firma kÄ±yaslamasÄ±"),
    ("SWOT analizi yapÄ±lmalÄ±", "GÃ¼Ã§lÃ¼/zayÄ±f yÃ¶nler ve fÄ±rsat/tehdit deÄŸerlendirmesi"),
    
    # Departman
    ("Ä°K departmanÄ± raporu", "Ä°nsan kaynaklarÄ± bÃ¶lÃ¼m raporu"),
    ("SatÄ±ÅŸ hedefi tutturma oranÄ±", "Ciro hedef gerÃ§ekleÅŸtirme yÃ¼zdesi"),
    ("Proses mÃ¼hendisliÄŸi", "SÃ¼reÃ§ iyileÅŸtirme mÃ¼hendisliÄŸi"),
    
    # SektÃ¶r Spesifik
    ("Ne/dtex deÄŸeri", "Numaralama iplik inceliÄŸi"),
    ("Lif kompozisyon oranÄ±", "KarÄ±ÅŸÄ±m oranÄ± yÃ¼zdesi"),
    ("Merserize sÃ¼recinde sorun", "Merserizasyon prosesinde problem"),
    ("Apre iÅŸlemi maliyeti", "Terbiye sonlandÄ±rma gideri"),
    ("Ã‡Ã¶zgÃ¼ kopmasÄ±", "Warp breakage oranÄ±"),
    ("AtkÄ± sÄ±klÄ±ÄŸÄ±", "Weft density ayarÄ±"),
]

# Negatif Ã¶rnekler: Bunlar birbirine benzer OLMAMALI
HARD_NEGATIVES = [
    ("Fire oranÄ± nedir?", "BugÃ¼n hava nasÄ±l?"),
    ("OEE hesaplama", "Yemek tarifi Ã¶nerisi"),
    ("KumaÅŸ kalite kontrolÃ¼", "Futbol maÃ§Ä± sonucu"),
    ("Ãœretim planÄ±", "Tatil rezervasyonu"),
    ("Maliyet analizi", "Film Ã¶nerisi"),
]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. EÄÄ°TÄ°M VERÄ°SÄ° OLUÅTUR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_training_data(output_dir: str = "data/embedding_training") -> str:
    """EÄŸitim verisini oluÅŸtur ve dosyaya yaz."""
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Triplet format: (anchor, positive, negative)
    triplets = []
    
    for anchor, positive in TEXTILE_PAIRS:
        # Her pozitif Ã§ift iÃ§in rastgele bir negatif seÃ§
        neg_candidates = [p for a, p in TEXTILE_PAIRS if a != anchor]
        neg_candidates += [n for _, n in HARD_NEGATIVES]
        negative = random.choice(neg_candidates)
        
        triplets.append({
            "anchor": anchor,
            "positive": positive,
            "negative": negative,
        })
        
        # Ters yÃ¶nÃ¼ de ekle (positive â†’ anchor, farklÄ± negatif)
        negative2 = random.choice(neg_candidates)
        triplets.append({
            "anchor": positive,
            "positive": anchor,
            "negative": negative2,
        })
    
    # ChromaDB'deki mevcut dokÃ¼manlardan ek Ã§iftler
    try:
        from app.rag.vector_store import get_collection
        collection = get_collection()
        if collection and collection.count() > 0:
            all_docs = collection.get(include=["documents", "metadatas"])
            if all_docs and all_docs.get("documents"):
                docs = all_docs["documents"]
                # AynÄ± source'tan gelen chunk'lar benzer olmalÄ±
                source_groups = {}
                for i, doc in enumerate(docs):
                    meta = all_docs["metadatas"][i] if all_docs.get("metadatas") else {}
                    src = meta.get("source", f"unk_{i}")
                    if src not in source_groups:
                        source_groups[src] = []
                    source_groups[src].append(doc[:200])
                
                for src, chunks in source_groups.items():
                    if len(chunks) >= 2:
                        for j in range(min(3, len(chunks) - 1)):
                            neg_src = random.choice([c for s, cs in source_groups.items() 
                                                    if s != src for c in cs] or ["AlakasÄ±z metin."])
                            triplets.append({
                                "anchor": chunks[j][:200],
                                "positive": chunks[j+1][:200],
                                "negative": neg_src[:200],
                            })
                print(f"  ChromaDB'den {len(source_groups)} kaynak grubundan ek Ã§iftler eklendi")
    except Exception as e:
        print(f"  ChromaDB okunamadÄ± (normal â€” sunucuda Ã§alÄ±ÅŸtÄ±rÄ±lmalÄ±): {e}")
    
    # Shuffle
    random.shuffle(triplets)
    
    # Train/Eval split
    split_idx = int(len(triplets) * 0.85)
    train_data = triplets[:split_idx]
    eval_data = triplets[split_idx:]
    
    # Dosyalara yaz
    train_path = os.path.join(output_dir, "train_triplets.json")
    eval_path = os.path.join(output_dir, "eval_triplets.json")
    
    with open(train_path, "w", encoding="utf-8") as f:
        json.dump(train_data, f, ensure_ascii=False, indent=2)
    
    with open(eval_path, "w", encoding="utf-8") as f:
        json.dump(eval_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… EÄŸitim verisi oluÅŸturuldu:")
    print(f"   Train: {len(train_data)} triplet â†’ {train_path}")
    print(f"   Eval:  {len(eval_data)} triplet â†’ {eval_path}")
    
    return train_path


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. FINE-TUNE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_embedding_model(
    train_path: str = "data/embedding_training/train_triplets.json",
    eval_path: str = "data/embedding_training/eval_triplets.json",
    output_model: str = "models/textile-mpnet-v1",
    epochs: int = 3,
    batch_size: int = 16,
):
    """SentenceTransformer fine-tune.
    
    TripletLoss ile eÄŸitir:
    - Pozitif Ã§iftlerin embedding'lerini yakÄ±nlaÅŸtÄ±rÄ±r
    - Negatif Ã§iftlerin embedding'lerini uzaklaÅŸtÄ±rÄ±r
    """
    from sentence_transformers import SentenceTransformer, InputExample, losses
    from torch.utils.data import DataLoader
    
    # Base model yÃ¼kle
    base_model = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    print(f"ğŸ“¦ Base model yÃ¼kleniyor: {base_model}")
    model = SentenceTransformer(base_model)
    
    # EÄŸitim verisini yÃ¼kle
    with open(train_path, "r", encoding="utf-8") as f:
        train_data = json.load(f)
    
    # InputExample formatÄ±na Ã§evir
    train_examples = []
    for item in train_data:
        train_examples.append(InputExample(
            texts=[item["anchor"], item["positive"], item["negative"]]
        ))
    
    # DataLoader
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)
    
    # TripletLoss â€” margin=0.5 ile benzer ve farklÄ± ifadeleri ayÄ±r
    train_loss = losses.TripletLoss(model=model, distance_metric=losses.TripletDistanceMetric.COSINE, triplet_margin=0.5)
    
    # Evaluation (opsiyonel)
    evaluator = None
    if os.path.exists(eval_path):
        from sentence_transformers.evaluation import TripletEvaluator
        with open(eval_path, "r", encoding="utf-8") as f:
            eval_data = json.load(f)
        
        anchors = [d["anchor"] for d in eval_data]
        positives = [d["positive"] for d in eval_data]
        negatives = [d["negative"] for d in eval_data]
        evaluator = TripletEvaluator(anchors, positives, negatives, name="textile_eval")
    
    # Fine-tune
    print(f"ğŸš€ Fine-tune baÅŸlatÄ±lÄ±yor...")
    print(f"   Epochs: {epochs}, Batch: {batch_size}, Samples: {len(train_examples)}")
    
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=epochs,
        warmup_steps=int(len(train_dataloader) * 0.1),
        evaluator=evaluator,
        evaluation_steps=50,
        output_path=output_model,
        show_progress_bar=True,
    )
    
    print(f"âœ… Fine-tuned model kaydedildi: {output_model}")
    print(f"   âš¡ Kullanmak iÃ§in vector_store.py'deki EMBEDDING_MODEL'i deÄŸiÅŸtir:")
    print(f'   EMBEDDING_MODEL = "{output_model}"')
    
    return output_model


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. DEÄERLENDÄ°RME
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def evaluate_model(model_path: str = None):
    """Fine-tuned model vs base model karÅŸÄ±laÅŸtÄ±rmasÄ±."""
    from sentence_transformers import SentenceTransformer
    import numpy as np
    
    base_model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    fine_tuned_path = model_path or "models/textile-mpnet-v1"
    
    print("ğŸ“Š Model deÄŸerlendirmesi...")
    
    # Her iki modeli de yÃ¼kle
    base_model = SentenceTransformer(base_model_name)
    fine_model = SentenceTransformer(fine_tuned_path)
    
    # Test Ã§iftleri
    test_pairs = TEXTILE_PAIRS[:15]  # Ä°lk 15 Ã§ift
    
    print(f"\n{'Ã‡ift':<50} {'Base':>8} {'FT':>8} {'Î”':>8}")
    print("-" * 80)
    
    base_scores = []
    ft_scores = []
    
    for anchor, positive in test_pairs:
        # Base model similarity
        base_emb = base_model.encode([anchor, positive])
        base_sim = float(np.dot(base_emb[0], base_emb[1]) / 
                        (np.linalg.norm(base_emb[0]) * np.linalg.norm(base_emb[1])))
        
        # Fine-tuned model similarity
        ft_emb = fine_model.encode([anchor, positive])
        ft_sim = float(np.dot(ft_emb[0], ft_emb[1]) / 
                       (np.linalg.norm(ft_emb[0]) * np.linalg.norm(ft_emb[1])))
        
        delta = ft_sim - base_sim
        base_scores.append(base_sim)
        ft_scores.append(ft_sim)
        
        arrow = "â†‘" if delta > 0 else "â†“"
        print(f"{anchor[:25]} â†” {positive[:20]:<20} {base_sim:>7.4f} {ft_sim:>7.4f} {arrow}{abs(delta):>6.4f}")
    
    avg_base = sum(base_scores) / len(base_scores)
    avg_ft = sum(ft_scores) / len(ft_scores)
    improvement = ((avg_ft - avg_base) / avg_base) * 100
    
    print(f"\n{'Ortalama':<50} {avg_base:>7.4f} {avg_ft:>7.4f} {'â†‘' if improvement > 0 else 'â†“'}{abs(improvement):.1f}%")
    print(f"\n{'âœ… Fine-tuned model daha iyi!' if improvement > 0 else 'âš ï¸ Base model daha iyi â€” daha fazla veri gerekli.'}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. CLI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Embedding Model Fine-Tune Pipeline")
    parser.add_argument("--generate", action="store_true", help="EÄŸitim verisini oluÅŸtur")
    parser.add_argument("--train", action="store_true", help="Fine-tune baÅŸlat")
    parser.add_argument("--evaluate", action="store_true", help="Model deÄŸerlendirmesi")
    parser.add_argument("--epochs", type=int, default=3, help="EÄŸitim epoch sayÄ±sÄ±")
    parser.add_argument("--batch-size", type=int, default=16, help="Batch boyutu")
    parser.add_argument("--model-path", type=str, default="models/textile-mpnet-v1", help="Model kayÄ±t yolu")
    
    args = parser.parse_args()
    
    if args.generate:
        generate_training_data()
    elif args.train:
        train_path = "data/embedding_training/train_triplets.json"
        if not os.path.exists(train_path):
            print("âš ï¸ EÄŸitim verisi bulunamadÄ±. Ã–nce --generate Ã§alÄ±ÅŸtÄ±rÄ±n.")
            generate_training_data()
        train_embedding_model(epochs=args.epochs, batch_size=args.batch_size, 
                            output_model=args.model_path)
    elif args.evaluate:
        evaluate_model(args.model_path)
    else:
        parser.print_help()
